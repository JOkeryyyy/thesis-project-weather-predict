=========== SHAPE INPUT-OUTPUT MATRIXES =====================
shape x_training : (227247, 189)  y_training: :  (227247, 24)
shape x_test     :  (75740, 189)  y_test      :  (75740, 24)
shape x_val      :  (75741, 189)  y_val       :  (75741, 24)
=============================================================
(189,)
Model: "MLP"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_1 (InputLayer)        [(None, 189)]             0         
                                                                 
 dense (Dense)               (None, 1024)              194560    
                                                                 
 elu (ELU)                   (None, 1024)              0         
                                                                 
 dropout (Dropout)           (None, 1024)              0         
                                                                 
 dense_1 (Dense)             (None, 512)               524800    
                                                                 
 dropout_1 (Dropout)         (None, 512)               0         
                                                                 
 dense_2 (Dense)             (None, 512)               262656    
                                                                 
 dropout_2 (Dropout)         (None, 512)               0         
                                                                 
 dense_3 (Dense)             (None, 24)                12312     
                                                                 
=================================================================
Total params: 994,328
Trainable params: 994,328
Non-trainable params: 0
_________________________________________________________________
Epoch 1/200
222/222 [==============================] - 3s 6ms/step - loss: 0.3687 - val_loss: 0.1103
Epoch 2/200
222/222 [==============================] - 1s 5ms/step - loss: 0.2112 - val_loss: 0.1024
Epoch 3/200
222/222 [==============================] - 1s 5ms/step - loss: 0.1839 - val_loss: 0.1011
Epoch 4/200
222/222 [==============================] - 1s 5ms/step - loss: 0.1707 - val_loss: 0.0982
Epoch 5/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1631 - val_loss: 0.0970
Epoch 6/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1573 - val_loss: 0.0956
Epoch 7/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1530 - val_loss: 0.0956
Epoch 8/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1499 - val_loss: 0.0943
Epoch 9/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1471 - val_loss: 0.0952
Epoch 10/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1448 - val_loss: 0.0969
Epoch 11/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1430 - val_loss: 0.0937
Epoch 12/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1410 - val_loss: 0.0931
Epoch 13/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1392 - val_loss: 0.0924
Epoch 14/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1379 - val_loss: 0.0945
Epoch 15/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1365 - val_loss: 0.0920
Epoch 16/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1355 - val_loss: 0.0931
Epoch 17/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1345 - val_loss: 0.0917
Epoch 18/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1335 - val_loss: 0.0929
Epoch 19/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1326 - val_loss: 0.0925
Epoch 20/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1318 - val_loss: 0.0916
Epoch 21/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1309 - val_loss: 0.0916
Epoch 22/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1302 - val_loss: 0.0924
Epoch 23/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1296 - val_loss: 0.0927
Epoch 24/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1288 - val_loss: 0.0920
Epoch 25/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1283 - val_loss: 0.0929
Epoch 26/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1276 - val_loss: 0.0911
Epoch 27/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1273 - val_loss: 0.0916
Epoch 28/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1269 - val_loss: 0.0904
Epoch 29/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1263 - val_loss: 0.0911
Epoch 30/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1259 - val_loss: 0.0927
Epoch 31/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1254 - val_loss: 0.0893
Epoch 32/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1250 - val_loss: 0.0906
Epoch 33/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1248 - val_loss: 0.0889
Epoch 34/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1245 - val_loss: 0.0906
Epoch 35/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1243 - val_loss: 0.0907
Epoch 36/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1236 - val_loss: 0.0905
Epoch 37/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1237 - val_loss: 0.0890
Epoch 38/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1232 - val_loss: 0.0914
Epoch 39/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1231 - val_loss: 0.0889
Epoch 40/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1228 - val_loss: 0.0905
Epoch 41/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1225 - val_loss: 0.0885
Epoch 42/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1221 - val_loss: 0.0884
Epoch 43/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1218 - val_loss: 0.0878
Epoch 44/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1217 - val_loss: 0.0894
Epoch 45/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1214 - val_loss: 0.0877
Epoch 46/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1210 - val_loss: 0.0877
Epoch 47/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1207 - val_loss: 0.0885
Epoch 48/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1204 - val_loss: 0.0869
Epoch 49/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1204 - val_loss: 0.0862
Epoch 50/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1200 - val_loss: 0.0868
Epoch 51/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1197 - val_loss: 0.0878
Epoch 52/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1194 - val_loss: 0.0871
Epoch 53/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1192 - val_loss: 0.0878
Epoch 54/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1191 - val_loss: 0.0859
Epoch 55/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1189 - val_loss: 0.0856
Epoch 56/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1187 - val_loss: 0.0848
Epoch 57/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1182 - val_loss: 0.0853
Epoch 58/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1181 - val_loss: 0.0871
Epoch 59/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1179 - val_loss: 0.0864
Epoch 60/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1178 - val_loss: 0.0856
Epoch 61/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1175 - val_loss: 0.0852
Epoch 62/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1174 - val_loss: 0.0845
Epoch 63/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1172 - val_loss: 0.0857
Epoch 64/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1171 - val_loss: 0.0864
Epoch 65/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1168 - val_loss: 0.0852
Epoch 66/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1165 - val_loss: 0.0864
Epoch 67/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1164 - val_loss: 0.0857
Epoch 68/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1161 - val_loss: 0.0837
Epoch 69/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1158 - val_loss: 0.0861
Epoch 70/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1159 - val_loss: 0.0853
Epoch 71/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1157 - val_loss: 0.0868
Epoch 72/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1155 - val_loss: 0.0847
Epoch 73/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1153 - val_loss: 0.0829
Epoch 74/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1151 - val_loss: 0.0834
Epoch 75/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1150 - val_loss: 0.0842
Epoch 76/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1152 - val_loss: 0.0842
Epoch 77/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1147 - val_loss: 0.0833
Epoch 78/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1145 - val_loss: 0.0853
Epoch 79/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1145 - val_loss: 0.0846
Epoch 80/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1143 - val_loss: 0.0834
Epoch 81/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1142 - val_loss: 0.0833
Epoch 82/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1139 - val_loss: 0.0845
Epoch 83/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1138 - val_loss: 0.0848
Val_yp Shape is 
(75741, 24)
Results === Test == Validation ===== 27
=============================
step 1    -  0.992    0.992
step 2    -  0.985    0.984
step 3    -  0.977    0.975
step 4    -  0.967    0.965
step 5    -  0.956    0.954
step 6    -  0.945    0.944
step 7    -  0.935    0.934
step 8    -  0.924    0.924
step 9    -  0.915    0.915
step 10   -  0.905    0.906
step 11   -  0.896    0.898
step 12   -  0.888    0.891
step 13   -  0.881    0.885
step 14   -  0.875    0.879
step 15   -  0.868    0.873
step 16   -  0.861    0.867
step 17   -  0.855    0.862
step 18   -  0.849    0.856
step 19   -  0.845    0.853
step 20   -  0.839    0.848
step 21   -  0.834    0.844
step 22   -  0.829    0.840
step 23   -  0.823    0.835
step 24   -  0.816    0.829
=============================
Summary   -  21.461    21.550
V_y Shape is 
(75741, 1)
=========== SHAPE INPUT-OUTPUT MATRIXES =====================
shape x_training : (227246, 196)  y_training: :  (227246, 24)
shape x_test     :  (75740, 196)  y_test      :  (75740, 24)
shape x_val      :  (75740, 196)  y_val       :  (75740, 24)
=============================================================
(196,)
Model: "MLP"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_2 (InputLayer)        [(None, 196)]             0         
                                                                 
 dense_4 (Dense)             (None, 1024)              201728    
                                                                 
 elu_1 (ELU)                 (None, 1024)              0         
                                                                 
 dropout_3 (Dropout)         (None, 1024)              0         
                                                                 
 dense_5 (Dense)             (None, 512)               524800    
                                                                 
 dropout_4 (Dropout)         (None, 512)               0         
                                                                 
 dense_6 (Dense)             (None, 512)               262656    
                                                                 
 dropout_5 (Dropout)         (None, 512)               0         
                                                                 
 dense_7 (Dense)             (None, 24)                12312     
                                                                 
=================================================================
Total params: 1,001,496
Trainable params: 1,001,496
Non-trainable params: 0
_________________________________________________________________
Epoch 1/200
222/222 [==============================] - 2s 7ms/step - loss: 0.3707 - val_loss: 0.1100
Epoch 2/200
222/222 [==============================] - 1s 6ms/step - loss: 0.2114 - val_loss: 0.1007
Epoch 3/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1839 - val_loss: 0.1002
Epoch 4/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1711 - val_loss: 0.0995
Epoch 5/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1629 - val_loss: 0.0963
Epoch 6/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1571 - val_loss: 0.0957
Epoch 7/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1530 - val_loss: 0.0951
Epoch 8/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1494 - val_loss: 0.0952
Epoch 9/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1465 - val_loss: 0.0958
Epoch 10/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1443 - val_loss: 0.0940
Epoch 11/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1423 - val_loss: 0.0917
Epoch 12/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1404 - val_loss: 0.0931
Epoch 13/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1387 - val_loss: 0.0943
Epoch 14/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1376 - val_loss: 0.0927
Epoch 15/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1361 - val_loss: 0.0905
Epoch 16/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1348 - val_loss: 0.0929
Epoch 17/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1338 - val_loss: 0.0911
Epoch 18/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1331 - val_loss: 0.0914
Epoch 19/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1322 - val_loss: 0.0917
Epoch 20/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1313 - val_loss: 0.0915
Epoch 21/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1303 - val_loss: 0.0920
Epoch 22/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1297 - val_loss: 0.0911
Epoch 23/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1291 - val_loss: 0.0924
Epoch 24/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1283 - val_loss: 0.0910
Epoch 25/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1281 - val_loss: 0.0900
Epoch 26/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1275 - val_loss: 0.0913
Epoch 27/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1268 - val_loss: 0.0902
Epoch 28/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1262 - val_loss: 0.0890
Epoch 29/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1259 - val_loss: 0.0912
Epoch 30/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1255 - val_loss: 0.0891
Epoch 31/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1252 - val_loss: 0.0906
Epoch 32/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1246 - val_loss: 0.0900
Epoch 33/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1245 - val_loss: 0.0894
Epoch 34/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1241 - val_loss: 0.0909
Epoch 35/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1236 - val_loss: 0.0885
Epoch 36/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1234 - val_loss: 0.0912
Epoch 37/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1234 - val_loss: 0.0882
Epoch 38/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1230 - val_loss: 0.0894
Epoch 39/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1225 - val_loss: 0.0888
Epoch 40/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1224 - val_loss: 0.0876
Epoch 41/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1221 - val_loss: 0.0884
Epoch 42/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1220 - val_loss: 0.0883
Epoch 43/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1216 - val_loss: 0.0882
Epoch 44/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1214 - val_loss: 0.0880
Epoch 45/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1211 - val_loss: 0.0884
Epoch 46/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1207 - val_loss: 0.0884
Epoch 47/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1206 - val_loss: 0.0879
Epoch 48/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1204 - val_loss: 0.0869
Epoch 49/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1200 - val_loss: 0.0869
Epoch 50/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1197 - val_loss: 0.0886
Epoch 51/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1193 - val_loss: 0.0869
Epoch 52/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1193 - val_loss: 0.0862
Epoch 53/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1190 - val_loss: 0.0868
Epoch 54/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1187 - val_loss: 0.0879
Epoch 55/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1186 - val_loss: 0.0860
Epoch 56/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1184 - val_loss: 0.0863
Epoch 57/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1180 - val_loss: 0.0864
Epoch 58/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1177 - val_loss: 0.0857
Epoch 59/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1178 - val_loss: 0.0858
Epoch 60/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1173 - val_loss: 0.0844
Epoch 61/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1171 - val_loss: 0.0861
Epoch 62/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1168 - val_loss: 0.0850
Epoch 63/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1168 - val_loss: 0.0847
Epoch 64/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1165 - val_loss: 0.0864
Epoch 65/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1162 - val_loss: 0.0850
Epoch 66/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1161 - val_loss: 0.0848
Epoch 67/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1156 - val_loss: 0.0839
Epoch 68/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1156 - val_loss: 0.0844
Epoch 69/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1156 - val_loss: 0.0835
Epoch 70/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1154 - val_loss: 0.0842
Epoch 71/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1153 - val_loss: 0.0839
Epoch 72/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1149 - val_loss: 0.0833
Epoch 73/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1148 - val_loss: 0.0835
Epoch 74/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1148 - val_loss: 0.0848
Epoch 75/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1145 - val_loss: 0.0854
Epoch 76/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1144 - val_loss: 0.0845
Epoch 77/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1144 - val_loss: 0.0849
Epoch 78/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1141 - val_loss: 0.0837
Epoch 79/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1138 - val_loss: 0.0833
Epoch 80/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1138 - val_loss: 0.0841
Epoch 81/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1137 - val_loss: 0.0842
Epoch 82/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1134 - val_loss: 0.0834
Epoch 83/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1133 - val_loss: 0.0833
Epoch 84/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1132 - val_loss: 0.0836
Epoch 85/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1129 - val_loss: 0.0825
Epoch 86/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1128 - val_loss: 0.0858
Epoch 87/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1127 - val_loss: 0.0835
Epoch 88/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1128 - val_loss: 0.0827
Epoch 89/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1122 - val_loss: 0.0845
Epoch 90/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1122 - val_loss: 0.0821
Epoch 91/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1121 - val_loss: 0.0824
Epoch 92/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1122 - val_loss: 0.0814
Epoch 93/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1122 - val_loss: 0.0834
Epoch 94/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1121 - val_loss: 0.0839
Epoch 95/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1117 - val_loss: 0.0836
Epoch 96/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1115 - val_loss: 0.0824
Epoch 97/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1116 - val_loss: 0.0840
Epoch 98/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1115 - val_loss: 0.0818
Epoch 99/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1112 - val_loss: 0.0820
Epoch 100/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1110 - val_loss: 0.0820
Epoch 101/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1111 - val_loss: 0.0816
Epoch 102/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1110 - val_loss: 0.0812
Epoch 103/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1109 - val_loss: 0.0822
Epoch 104/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1107 - val_loss: 0.0831
Epoch 105/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1106 - val_loss: 0.0817
Epoch 106/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1104 - val_loss: 0.0827
Epoch 107/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1103 - val_loss: 0.0829
Epoch 108/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1103 - val_loss: 0.0821
Epoch 109/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1103 - val_loss: 0.0826
Epoch 110/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1100 - val_loss: 0.0820
Epoch 111/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1100 - val_loss: 0.0811
Epoch 112/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1099 - val_loss: 0.0812
Epoch 113/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1096 - val_loss: 0.0822
Epoch 114/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1096 - val_loss: 0.0820
Epoch 115/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1096 - val_loss: 0.0814
Epoch 116/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1095 - val_loss: 0.0826
Epoch 117/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1093 - val_loss: 0.0812
Epoch 118/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1094 - val_loss: 0.0821
Epoch 119/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1091 - val_loss: 0.0812
Epoch 120/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1091 - val_loss: 0.0812
Epoch 121/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1090 - val_loss: 0.0822
Val_yp Shape is 
(75740, 24)
Results === Test == Validation ===== 28
=============================
step 1    -  0.993    0.992
step 2    -  0.986    0.984
step 3    -  0.977    0.975
step 4    -  0.967    0.965
step 5    -  0.956    0.954
step 6    -  0.946    0.944
step 7    -  0.935    0.934
step 8    -  0.925    0.925
step 9    -  0.915    0.916
step 10   -  0.905    0.907
step 11   -  0.897    0.900
step 12   -  0.890    0.894
step 13   -  0.883    0.887
step 14   -  0.877    0.881
step 15   -  0.871    0.875
step 16   -  0.867    0.871
step 17   -  0.861    0.866
step 18   -  0.855    0.861
step 19   -  0.853    0.859
step 20   -  0.848    0.856
step 21   -  0.844    0.853
step 22   -  0.840    0.849
step 23   -  0.835    0.846
step 24   -  0.830    0.842
=============================
Summary   -  21.557    21.636
V_y Shape is 
(75740, 1)
=========== SHAPE INPUT-OUTPUT MATRIXES =====================
shape x_training : (227245, 203)  y_training: :  (227245, 24)
shape x_test     :  (75739, 203)  y_test      :  (75739, 24)
shape x_val      :  (75740, 203)  y_val       :  (75740, 24)
=============================================================
(203,)
Model: "MLP"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_3 (InputLayer)        [(None, 203)]             0         
                                                                 
 dense_8 (Dense)             (None, 1024)              208896    
                                                                 
 elu_2 (ELU)                 (None, 1024)              0         
                                                                 
 dropout_6 (Dropout)         (None, 1024)              0         
                                                                 
 dense_9 (Dense)             (None, 512)               524800    
                                                                 
 dropout_7 (Dropout)         (None, 512)               0         
                                                                 
 dense_10 (Dense)            (None, 512)               262656    
                                                                 
 dropout_8 (Dropout)         (None, 512)               0         
                                                                 
 dense_11 (Dense)            (None, 24)                12312     
                                                                 
=================================================================
Total params: 1,008,664
Trainable params: 1,008,664
Non-trainable params: 0
_________________________________________________________________
Epoch 1/200
222/222 [==============================] - 2s 7ms/step - loss: 0.3747 - val_loss: 0.1098
Epoch 2/200
222/222 [==============================] - 1s 5ms/step - loss: 0.2123 - val_loss: 0.1017
Epoch 3/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1841 - val_loss: 0.0999
Epoch 4/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1710 - val_loss: 0.0985
Epoch 5/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1630 - val_loss: 0.0966
Epoch 6/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1574 - val_loss: 0.0974
Epoch 7/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1527 - val_loss: 0.0960
Epoch 8/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1492 - val_loss: 0.0957
Epoch 9/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1464 - val_loss: 0.0949
Epoch 10/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1443 - val_loss: 0.0940
Epoch 11/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1420 - val_loss: 0.0939
Epoch 12/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1402 - val_loss: 0.0931
Epoch 13/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1387 - val_loss: 0.0928
Epoch 14/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1373 - val_loss: 0.0921
Epoch 15/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1361 - val_loss: 0.0932
Epoch 16/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1347 - val_loss: 0.0925
Epoch 17/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1338 - val_loss: 0.0918
Epoch 18/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1326 - val_loss: 0.0923
Epoch 19/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1317 - val_loss: 0.0917
Epoch 20/200
222/222 [==============================] - 1s 5ms/step - loss: 0.1308 - val_loss: 0.0906
Epoch 21/200
222/222 [==============================] - 1s 5ms/step - loss: 0.1299 - val_loss: 0.0911
Epoch 22/200
222/222 [==============================] - 1s 5ms/step - loss: 0.1294 - val_loss: 0.0929
Epoch 23/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1287 - val_loss: 0.0915
Epoch 24/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1282 - val_loss: 0.0908
Epoch 25/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1275 - val_loss: 0.0916
Epoch 26/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1269 - val_loss: 0.0911
Epoch 27/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1265 - val_loss: 0.0894
Epoch 28/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1258 - val_loss: 0.0905
Epoch 29/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1254 - val_loss: 0.0900
Epoch 30/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1251 - val_loss: 0.0888
Epoch 31/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1246 - val_loss: 0.0921
Epoch 32/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1241 - val_loss: 0.0891
Epoch 33/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1240 - val_loss: 0.0884
Epoch 34/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1239 - val_loss: 0.0887
Epoch 35/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1234 - val_loss: 0.0900
Epoch 36/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1228 - val_loss: 0.0900
Epoch 37/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1228 - val_loss: 0.0894
Epoch 38/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1223 - val_loss: 0.0879
Epoch 39/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1221 - val_loss: 0.0889
Epoch 40/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1218 - val_loss: 0.0893
Epoch 41/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1216 - val_loss: 0.0869
Epoch 42/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1212 - val_loss: 0.0884
Epoch 43/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1211 - val_loss: 0.0875
Epoch 44/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1207 - val_loss: 0.0890
Epoch 45/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1205 - val_loss: 0.0881
Epoch 46/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1202 - val_loss: 0.0872
Epoch 47/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1200 - val_loss: 0.0869
Epoch 48/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1196 - val_loss: 0.0876
Epoch 49/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1195 - val_loss: 0.0863
Epoch 50/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1191 - val_loss: 0.0878
Epoch 51/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1190 - val_loss: 0.0869
Epoch 52/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1186 - val_loss: 0.0859
Epoch 53/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1186 - val_loss: 0.0862
Epoch 54/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1179 - val_loss: 0.0847
Epoch 55/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1178 - val_loss: 0.0849
Epoch 56/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1176 - val_loss: 0.0864
Epoch 57/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1172 - val_loss: 0.0859
Epoch 58/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1172 - val_loss: 0.0852
Epoch 59/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1169 - val_loss: 0.0872
Epoch 60/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1169 - val_loss: 0.0852
Epoch 61/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1165 - val_loss: 0.0849
Epoch 62/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1163 - val_loss: 0.0831
Epoch 63/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1161 - val_loss: 0.0873
Epoch 64/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1158 - val_loss: 0.0841
Epoch 65/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1155 - val_loss: 0.0845
Epoch 66/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1155 - val_loss: 0.0853
Epoch 67/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1152 - val_loss: 0.0837
Epoch 68/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1151 - val_loss: 0.0828
Epoch 69/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1148 - val_loss: 0.0829
Epoch 70/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1146 - val_loss: 0.0832
Epoch 71/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1144 - val_loss: 0.0841
Epoch 72/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1145 - val_loss: 0.0833
Epoch 73/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1141 - val_loss: 0.0835
Epoch 74/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1142 - val_loss: 0.0833
Epoch 75/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1139 - val_loss: 0.0838
Epoch 76/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1137 - val_loss: 0.0841
Epoch 77/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1138 - val_loss: 0.0845
Epoch 78/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1132 - val_loss: 0.0824
Epoch 79/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1132 - val_loss: 0.0820
Epoch 80/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1131 - val_loss: 0.0825
Epoch 81/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1131 - val_loss: 0.0823
Epoch 82/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1129 - val_loss: 0.0835
Epoch 83/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1126 - val_loss: 0.0824
Epoch 84/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1124 - val_loss: 0.0824
Epoch 85/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1123 - val_loss: 0.0829
Epoch 86/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1123 - val_loss: 0.0821
Epoch 87/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1121 - val_loss: 0.0829
Epoch 88/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1120 - val_loss: 0.0824
Epoch 89/200
222/222 [==============================] - 1s 5ms/step - loss: 0.1120 - val_loss: 0.0834
Val_yp Shape is 
(75740, 24)
Results === Test == Validation ===== 29
=============================
step 1    -  0.993    0.992
step 2    -  0.986    0.984
step 3    -  0.977    0.974
step 4    -  0.967    0.964
step 5    -  0.956    0.953
step 6    -  0.946    0.944
step 7    -  0.936    0.935
step 8    -  0.927    0.926
step 9    -  0.917    0.918
step 10   -  0.909    0.910
step 11   -  0.900    0.902
step 12   -  0.893    0.895
step 13   -  0.886    0.889
step 14   -  0.879    0.882
step 15   -  0.872    0.877
step 16   -  0.865    0.870
step 17   -  0.860    0.866
step 18   -  0.855    0.862
step 19   -  0.851    0.858
step 20   -  0.847    0.855
step 21   -  0.843    0.852
step 22   -  0.838    0.848
step 23   -  0.832    0.843
step 24   -  0.826    0.838
=============================
Summary   -  21.558    21.637
V_y Shape is 
(75740, 1)
=========== SHAPE INPUT-OUTPUT MATRIXES =====================
shape x_training : (227244, 210)  y_training: :  (227244, 24)
shape x_test     :  (75739, 210)  y_test      :  (75739, 24)
shape x_val      :  (75739, 210)  y_val       :  (75739, 24)
=============================================================
(210,)
Model: "MLP"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_4 (InputLayer)        [(None, 210)]             0         
                                                                 
 dense_12 (Dense)            (None, 1024)              216064    
                                                                 
 elu_3 (ELU)                 (None, 1024)              0         
                                                                 
 dropout_9 (Dropout)         (None, 1024)              0         
                                                                 
 dense_13 (Dense)            (None, 512)               524800    
                                                                 
 dropout_10 (Dropout)        (None, 512)               0         
                                                                 
 dense_14 (Dense)            (None, 512)               262656    
                                                                 
 dropout_11 (Dropout)        (None, 512)               0         
                                                                 
 dense_15 (Dense)            (None, 24)                12312     
                                                                 
=================================================================
Total params: 1,015,832
Trainable params: 1,015,832
Non-trainable params: 0
_________________________________________________________________
Epoch 1/200
222/222 [==============================] - 2s 7ms/step - loss: 0.3854 - val_loss: 0.1110
Epoch 2/200
222/222 [==============================] - 1s 6ms/step - loss: 0.2145 - val_loss: 0.1037
Epoch 3/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1855 - val_loss: 0.1011
Epoch 4/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1719 - val_loss: 0.0981
Epoch 5/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1639 - val_loss: 0.0981
Epoch 6/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1577 - val_loss: 0.0978
Epoch 7/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1533 - val_loss: 0.0975
Epoch 8/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1496 - val_loss: 0.0963
Epoch 9/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1469 - val_loss: 0.0943
Epoch 10/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1443 - val_loss: 0.0958
Epoch 11/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1423 - val_loss: 0.0947
Epoch 12/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1406 - val_loss: 0.0933
Epoch 13/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1389 - val_loss: 0.0930
Epoch 14/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1377 - val_loss: 0.0929
Epoch 15/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1362 - val_loss: 0.0924
Epoch 16/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1347 - val_loss: 0.0913
Epoch 17/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1337 - val_loss: 0.0920
Epoch 18/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1327 - val_loss: 0.0909
Epoch 19/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1320 - val_loss: 0.0918
Epoch 20/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1308 - val_loss: 0.0912
Epoch 21/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1301 - val_loss: 0.0910
Epoch 22/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1293 - val_loss: 0.0912
Epoch 23/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1285 - val_loss: 0.0901
Epoch 24/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1281 - val_loss: 0.0902
Epoch 25/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1274 - val_loss: 0.0900
Epoch 26/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1268 - val_loss: 0.0903
Epoch 27/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1263 - val_loss: 0.0894
Epoch 28/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1256 - val_loss: 0.0899
Epoch 29/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1253 - val_loss: 0.0888
Epoch 30/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1249 - val_loss: 0.0888
Epoch 31/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1246 - val_loss: 0.0909
Epoch 32/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1242 - val_loss: 0.0911
Epoch 33/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1238 - val_loss: 0.0903
Epoch 34/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1234 - val_loss: 0.0899
Epoch 35/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1231 - val_loss: 0.0906
Epoch 36/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1230 - val_loss: 0.0906
Epoch 37/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1226 - val_loss: 0.0877
Epoch 38/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1222 - val_loss: 0.0887
Epoch 39/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1219 - val_loss: 0.0876
Epoch 40/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1216 - val_loss: 0.0897
Epoch 41/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1215 - val_loss: 0.0888
Epoch 42/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1210 - val_loss: 0.0894
Epoch 43/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1209 - val_loss: 0.0869
Epoch 44/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1206 - val_loss: 0.0871
Epoch 45/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1201 - val_loss: 0.0885
Epoch 46/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1199 - val_loss: 0.0872
Epoch 47/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1195 - val_loss: 0.0866
Epoch 48/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1195 - val_loss: 0.0871
Epoch 49/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1188 - val_loss: 0.0851
Epoch 50/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1186 - val_loss: 0.0871
Epoch 51/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1184 - val_loss: 0.0874
Epoch 52/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1183 - val_loss: 0.0847
Epoch 53/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1180 - val_loss: 0.0864
Epoch 54/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1176 - val_loss: 0.0863
Epoch 55/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1174 - val_loss: 0.0851
Epoch 56/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1171 - val_loss: 0.0860
Epoch 57/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1171 - val_loss: 0.0848
Epoch 58/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1166 - val_loss: 0.0857
Epoch 59/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1166 - val_loss: 0.0844
Epoch 60/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1164 - val_loss: 0.0846
Epoch 61/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1160 - val_loss: 0.0852
Epoch 62/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1158 - val_loss: 0.0845
Epoch 63/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1156 - val_loss: 0.0854
Epoch 64/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1154 - val_loss: 0.0849
Epoch 65/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1152 - val_loss: 0.0841
Epoch 66/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1150 - val_loss: 0.0859
Epoch 67/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1150 - val_loss: 0.0850
Epoch 68/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1149 - val_loss: 0.0829
Epoch 69/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1145 - val_loss: 0.0847
Epoch 70/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1143 - val_loss: 0.0844
Epoch 71/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1141 - val_loss: 0.0839
Epoch 72/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1140 - val_loss: 0.0836
Epoch 73/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1138 - val_loss: 0.0847
Epoch 74/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1137 - val_loss: 0.0838
Epoch 75/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1135 - val_loss: 0.0842
Epoch 76/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1132 - val_loss: 0.0837
Epoch 77/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1131 - val_loss: 0.0833
Epoch 78/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1130 - val_loss: 0.0838
Val_yp Shape is 
(75739, 24)
Results === Test == Validation ===== 30
=============================
step 1    -  0.992    0.991
step 2    -  0.984    0.982
step 3    -  0.974    0.972
step 4    -  0.963    0.961
step 5    -  0.951    0.949
step 6    -  0.941    0.939
step 7    -  0.930    0.929
step 8    -  0.921    0.921
step 9    -  0.912    0.913
step 10   -  0.905    0.907
step 11   -  0.898    0.900
step 12   -  0.892    0.894
step 13   -  0.886    0.888
step 14   -  0.881    0.884
step 15   -  0.875    0.879
step 16   -  0.869    0.873
step 17   -  0.865    0.869
step 18   -  0.859    0.864
step 19   -  0.854    0.860
step 20   -  0.849    0.856
step 21   -  0.843    0.851
step 22   -  0.836    0.845
step 23   -  0.830    0.841
step 24   -  0.823    0.835
=============================
Summary   -  21.533    21.602
V_y Shape is 
(75739, 1)
=========== SHAPE INPUT-OUTPUT MATRIXES =====================
shape x_training : (227243, 217)  y_training: :  (227243, 24)
shape x_test     :  (75738, 217)  y_test      :  (75738, 24)
shape x_val      :  (75739, 217)  y_val       :  (75739, 24)
=============================================================
(217,)
Model: "MLP"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_5 (InputLayer)        [(None, 217)]             0         
                                                                 
 dense_16 (Dense)            (None, 1024)              223232    
                                                                 
 elu_4 (ELU)                 (None, 1024)              0         
                                                                 
 dropout_12 (Dropout)        (None, 1024)              0         
                                                                 
 dense_17 (Dense)            (None, 512)               524800    
                                                                 
 dropout_13 (Dropout)        (None, 512)               0         
                                                                 
 dense_18 (Dense)            (None, 512)               262656    
                                                                 
 dropout_14 (Dropout)        (None, 512)               0         
                                                                 
 dense_19 (Dense)            (None, 24)                12312     
                                                                 
=================================================================
Total params: 1,023,000
Trainable params: 1,023,000
Non-trainable params: 0
_________________________________________________________________
Epoch 1/200
222/222 [==============================] - 2s 8ms/step - loss: 0.3778 - val_loss: 0.1099
Epoch 2/200
222/222 [==============================] - 1s 6ms/step - loss: 0.2134 - val_loss: 0.1018
Epoch 3/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1853 - val_loss: 0.1002
Epoch 4/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1715 - val_loss: 0.0978
Epoch 5/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1634 - val_loss: 0.0965
Epoch 6/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1575 - val_loss: 0.0972
Epoch 7/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1532 - val_loss: 0.0964
Epoch 8/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1496 - val_loss: 0.0946
Epoch 9/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1466 - val_loss: 0.0955
Epoch 10/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1439 - val_loss: 0.0957
Epoch 11/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1421 - val_loss: 0.0931
Epoch 12/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1403 - val_loss: 0.0974
Epoch 13/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1385 - val_loss: 0.0938
Epoch 14/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1371 - val_loss: 0.0930
Epoch 15/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1355 - val_loss: 0.0920
Epoch 16/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1343 - val_loss: 0.0929
Epoch 17/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1332 - val_loss: 0.0918
Epoch 18/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1321 - val_loss: 0.0902
Epoch 19/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1312 - val_loss: 0.0908
Epoch 20/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1303 - val_loss: 0.0899
Epoch 21/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1296 - val_loss: 0.0915
Epoch 22/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1288 - val_loss: 0.0897
Epoch 23/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1278 - val_loss: 0.0902
Epoch 24/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1272 - val_loss: 0.0892
Epoch 25/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1267 - val_loss: 0.0898
Epoch 26/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1264 - val_loss: 0.0902
Epoch 27/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1256 - val_loss: 0.0909
Epoch 28/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1251 - val_loss: 0.0903
Epoch 29/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1247 - val_loss: 0.0904
Epoch 30/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1243 - val_loss: 0.0902
Epoch 31/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1242 - val_loss: 0.0887
Epoch 32/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1235 - val_loss: 0.0900
Epoch 33/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1234 - val_loss: 0.0887
Epoch 34/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1230 - val_loss: 0.0892
Epoch 35/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1228 - val_loss: 0.0884
Epoch 36/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1225 - val_loss: 0.0901
Epoch 37/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1218 - val_loss: 0.0883
Epoch 38/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1216 - val_loss: 0.0885
Epoch 39/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1214 - val_loss: 0.0873
Epoch 40/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1211 - val_loss: 0.0885
Epoch 41/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1208 - val_loss: 0.0872
Epoch 42/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1206 - val_loss: 0.0893
Epoch 43/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1204 - val_loss: 0.0873
Epoch 44/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1199 - val_loss: 0.0864
Epoch 45/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1197 - val_loss: 0.0863
Epoch 46/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1193 - val_loss: 0.0883
Epoch 47/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1193 - val_loss: 0.0856
Epoch 48/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1186 - val_loss: 0.0877
Epoch 49/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1187 - val_loss: 0.0864
Epoch 50/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1184 - val_loss: 0.0870
Epoch 51/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1179 - val_loss: 0.0863
Epoch 52/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1177 - val_loss: 0.0856
Epoch 53/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1175 - val_loss: 0.0860
Epoch 54/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1171 - val_loss: 0.0849
Epoch 55/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1169 - val_loss: 0.0868
Epoch 56/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1168 - val_loss: 0.0860
Epoch 57/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1164 - val_loss: 0.0837
Epoch 58/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1162 - val_loss: 0.0847
Epoch 59/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1158 - val_loss: 0.0863
Epoch 60/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1157 - val_loss: 0.0850
Epoch 61/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1156 - val_loss: 0.0848
Epoch 62/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1154 - val_loss: 0.0838
Epoch 63/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1152 - val_loss: 0.0843
Epoch 64/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1149 - val_loss: 0.0848
Epoch 65/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1149 - val_loss: 0.0839
Epoch 66/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1143 - val_loss: 0.0829
Epoch 67/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1143 - val_loss: 0.0862
Epoch 68/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1142 - val_loss: 0.0842
Epoch 69/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1138 - val_loss: 0.0842
Epoch 70/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1137 - val_loss: 0.0836
Epoch 71/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1136 - val_loss: 0.0829
Epoch 72/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1136 - val_loss: 0.0824
Epoch 73/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1133 - val_loss: 0.0822
Epoch 74/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1130 - val_loss: 0.0818
Epoch 75/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1127 - val_loss: 0.0837
Epoch 76/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1127 - val_loss: 0.0827
Epoch 77/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1126 - val_loss: 0.0831
Epoch 78/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1124 - val_loss: 0.0816
Epoch 79/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1122 - val_loss: 0.0827
Epoch 80/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1121 - val_loss: 0.0827
Epoch 81/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1119 - val_loss: 0.0831
Epoch 82/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1118 - val_loss: 0.0822
Epoch 83/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1117 - val_loss: 0.0830
Epoch 84/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1114 - val_loss: 0.0820
Epoch 85/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1114 - val_loss: 0.0829
Epoch 86/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1113 - val_loss: 0.0816
Epoch 87/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1109 - val_loss: 0.0805
Epoch 88/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1109 - val_loss: 0.0825
Epoch 89/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1109 - val_loss: 0.0810
Epoch 90/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1105 - val_loss: 0.0818
Epoch 91/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1104 - val_loss: 0.0825
Epoch 92/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1103 - val_loss: 0.0813
Epoch 93/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1102 - val_loss: 0.0812
Epoch 94/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1100 - val_loss: 0.0823
Epoch 95/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1100 - val_loss: 0.0821
Epoch 96/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1100 - val_loss: 0.0828
Epoch 97/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1099 - val_loss: 0.0824
Val_yp Shape is 
(75739, 24)
Results === Test == Validation ===== 31
=============================
step 1    -  0.992    0.992
step 2    -  0.985    0.984
step 3    -  0.976    0.974
step 4    -  0.966    0.964
step 5    -  0.955    0.953
step 6    -  0.945    0.944
step 7    -  0.935    0.934
step 8    -  0.926    0.926
step 9    -  0.916    0.917
step 10   -  0.907    0.909
step 11   -  0.899    0.902
step 12   -  0.893    0.896
step 13   -  0.887    0.890
step 14   -  0.881    0.884
step 15   -  0.875    0.879
step 16   -  0.869    0.874
step 17   -  0.864    0.870
step 18   -  0.859    0.866
step 19   -  0.855    0.862
step 20   -  0.849    0.858
step 21   -  0.846    0.856
step 22   -  0.840    0.852
step 23   -  0.834    0.847
step 24   -  0.828    0.841
=============================
Summary   -  21.584    21.671
V_y Shape is 
(75739, 1)
=========== SHAPE INPUT-OUTPUT MATRIXES =====================
shape x_training : (227242, 224)  y_training: :  (227242, 24)
shape x_test     :  (75738, 224)  y_test      :  (75738, 24)
shape x_val      :  (75738, 224)  y_val       :  (75738, 24)
=============================================================
(224,)
Model: "MLP"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_6 (InputLayer)        [(None, 224)]             0         
                                                                 
 dense_20 (Dense)            (None, 1024)              230400    
                                                                 
 elu_5 (ELU)                 (None, 1024)              0         
                                                                 
 dropout_15 (Dropout)        (None, 1024)              0         
                                                                 
 dense_21 (Dense)            (None, 512)               524800    
                                                                 
 dropout_16 (Dropout)        (None, 512)               0         
                                                                 
 dense_22 (Dense)            (None, 512)               262656    
                                                                 
 dropout_17 (Dropout)        (None, 512)               0         
                                                                 
 dense_23 (Dense)            (None, 24)                12312     
                                                                 
=================================================================
Total params: 1,030,168
Trainable params: 1,030,168
Non-trainable params: 0
_________________________________________________________________
Epoch 1/200
222/222 [==============================] - 2s 8ms/step - loss: 0.3849 - val_loss: 0.1092
Epoch 2/200
222/222 [==============================] - 1s 6ms/step - loss: 0.2152 - val_loss: 0.1014
Epoch 3/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1858 - val_loss: 0.1004
Epoch 4/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1720 - val_loss: 0.0975
Epoch 5/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1635 - val_loss: 0.0965
Epoch 6/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1576 - val_loss: 0.0947
Epoch 7/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1532 - val_loss: 0.0957
Epoch 8/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1494 - val_loss: 0.0926
Epoch 9/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1466 - val_loss: 0.0944
Epoch 10/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1438 - val_loss: 0.0923
Epoch 11/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1419 - val_loss: 0.0934
Epoch 12/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1397 - val_loss: 0.0934
Epoch 13/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1383 - val_loss: 0.0929
Epoch 14/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1365 - val_loss: 0.0906
Epoch 15/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1353 - val_loss: 0.0928
Epoch 16/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1341 - val_loss: 0.0920
Epoch 17/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1327 - val_loss: 0.0917
Epoch 18/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1318 - val_loss: 0.0907
Epoch 19/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1309 - val_loss: 0.0900
Epoch 20/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1300 - val_loss: 0.0907
Epoch 21/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1292 - val_loss: 0.0899
Epoch 22/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1285 - val_loss: 0.0892
Epoch 23/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1276 - val_loss: 0.0893
Epoch 24/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1269 - val_loss: 0.0904
Epoch 25/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1263 - val_loss: 0.0890
Epoch 26/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1257 - val_loss: 0.0905
Epoch 27/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1253 - val_loss: 0.0888
Epoch 28/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1250 - val_loss: 0.0886
Epoch 29/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1242 - val_loss: 0.0907
Epoch 30/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1240 - val_loss: 0.0895
Epoch 31/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1235 - val_loss: 0.0898
Epoch 32/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1231 - val_loss: 0.0885
Epoch 33/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1226 - val_loss: 0.0903
Epoch 34/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1225 - val_loss: 0.0887
Epoch 35/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1222 - val_loss: 0.0892
Epoch 36/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1218 - val_loss: 0.0883
Epoch 37/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1215 - val_loss: 0.0874
Epoch 38/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1211 - val_loss: 0.0876
Epoch 39/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1210 - val_loss: 0.0914
Epoch 40/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1209 - val_loss: 0.0878
Epoch 41/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1204 - val_loss: 0.0876
Epoch 42/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1200 - val_loss: 0.0873
Epoch 43/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1197 - val_loss: 0.0877
Epoch 44/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1194 - val_loss: 0.0874
Epoch 45/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1192 - val_loss: 0.0857
Epoch 46/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1188 - val_loss: 0.0849
Epoch 47/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1186 - val_loss: 0.0863
Epoch 48/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1183 - val_loss: 0.0859
Epoch 49/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1178 - val_loss: 0.0875
Epoch 50/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1178 - val_loss: 0.0873
Epoch 51/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1176 - val_loss: 0.0858
Epoch 52/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1173 - val_loss: 0.0857
Epoch 53/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1170 - val_loss: 0.0850
Epoch 54/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1166 - val_loss: 0.0848
Epoch 55/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1164 - val_loss: 0.0840
Epoch 56/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1161 - val_loss: 0.0856
Epoch 57/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1161 - val_loss: 0.0841
Epoch 58/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1156 - val_loss: 0.0853
Epoch 59/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1155 - val_loss: 0.0836
Epoch 60/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1152 - val_loss: 0.0835
Epoch 61/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1150 - val_loss: 0.0840
Epoch 62/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1149 - val_loss: 0.0836
Epoch 63/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1145 - val_loss: 0.0835
Epoch 64/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1144 - val_loss: 0.0848
Epoch 65/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1140 - val_loss: 0.0842
Epoch 66/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1141 - val_loss: 0.0837
Epoch 67/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1139 - val_loss: 0.0830
Epoch 68/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1136 - val_loss: 0.0841
Epoch 69/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1135 - val_loss: 0.0845
Epoch 70/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1132 - val_loss: 0.0835
Epoch 71/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1130 - val_loss: 0.0835
Epoch 72/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1130 - val_loss: 0.0835
Epoch 73/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1127 - val_loss: 0.0844
Epoch 74/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1126 - val_loss: 0.0830
Epoch 75/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1124 - val_loss: 0.0824
Epoch 76/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1121 - val_loss: 0.0817
Epoch 77/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1119 - val_loss: 0.0842
Epoch 78/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1120 - val_loss: 0.0827
Epoch 79/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1119 - val_loss: 0.0816
Epoch 80/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1116 - val_loss: 0.0827
Epoch 81/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1114 - val_loss: 0.0816
Epoch 82/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1114 - val_loss: 0.0818
Epoch 83/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1111 - val_loss: 0.0818
Epoch 84/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1108 - val_loss: 0.0810
Epoch 85/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1108 - val_loss: 0.0812
Epoch 86/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1108 - val_loss: 0.0817
Epoch 87/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1105 - val_loss: 0.0818
Epoch 88/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1105 - val_loss: 0.0837
Epoch 89/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1103 - val_loss: 0.0823
Epoch 90/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1101 - val_loss: 0.0824
Epoch 91/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1100 - val_loss: 0.0816
Epoch 92/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1097 - val_loss: 0.0826
Epoch 93/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1098 - val_loss: 0.0810
Epoch 94/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1095 - val_loss: 0.0811
Val_yp Shape is 
(75738, 24)
Results === Test == Validation ===== 32
=============================
step 1    -  0.993    0.992
step 2    -  0.986    0.984
step 3    -  0.977    0.975
step 4    -  0.967    0.965
step 5    -  0.957    0.955
step 6    -  0.947    0.945
step 7    -  0.936    0.936
step 8    -  0.927    0.927
step 9    -  0.917    0.918
step 10   -  0.908    0.910
step 11   -  0.899    0.902
step 12   -  0.893    0.896
step 13   -  0.887    0.890
step 14   -  0.881    0.885
step 15   -  0.876    0.881
step 16   -  0.871    0.876
step 17   -  0.866    0.872
step 18   -  0.860    0.867
step 19   -  0.853    0.861
step 20   -  0.847    0.856
step 21   -  0.842    0.852
step 22   -  0.837    0.848
step 23   -  0.831    0.843
step 24   -  0.823    0.837
=============================
Summary   -  21.580    21.674
V_y Shape is 
(75738, 1)
=========== SHAPE INPUT-OUTPUT MATRIXES =====================
shape x_training : (227241, 231)  y_training: :  (227241, 24)
shape x_test     :  (75737, 231)  y_test      :  (75737, 24)
shape x_val      :  (75738, 231)  y_val       :  (75738, 24)
=============================================================
(231,)
Model: "MLP"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_7 (InputLayer)        [(None, 231)]             0         
                                                                 
 dense_24 (Dense)            (None, 1024)              237568    
                                                                 
 elu_6 (ELU)                 (None, 1024)              0         
                                                                 
 dropout_18 (Dropout)        (None, 1024)              0         
                                                                 
 dense_25 (Dense)            (None, 512)               524800    
                                                                 
 dropout_19 (Dropout)        (None, 512)               0         
                                                                 
 dense_26 (Dense)            (None, 512)               262656    
                                                                 
 dropout_20 (Dropout)        (None, 512)               0         
                                                                 
 dense_27 (Dense)            (None, 24)                12312     
                                                                 
=================================================================
Total params: 1,037,336
Trainable params: 1,037,336
Non-trainable params: 0
_________________________________________________________________
Epoch 1/200
222/222 [==============================] - 2s 7ms/step - loss: 0.3811 - val_loss: 0.1079
Epoch 2/200
222/222 [==============================] - 1s 6ms/step - loss: 0.2151 - val_loss: 0.1012
Epoch 3/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1859 - val_loss: 0.0975
Epoch 4/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1720 - val_loss: 0.0961
Epoch 5/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1635 - val_loss: 0.0959
Epoch 6/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1572 - val_loss: 0.0937
Epoch 7/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1528 - val_loss: 0.0932
Epoch 8/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1489 - val_loss: 0.0930
Epoch 9/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1462 - val_loss: 0.0940
Epoch 10/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1437 - val_loss: 0.0943
Epoch 11/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1412 - val_loss: 0.0931
Epoch 12/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1394 - val_loss: 0.0920
Epoch 13/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1377 - val_loss: 0.0945
Epoch 14/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1360 - val_loss: 0.0927
Epoch 15/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1345 - val_loss: 0.0925
Epoch 16/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1334 - val_loss: 0.0939
Epoch 17/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1324 - val_loss: 0.0918
Epoch 18/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1313 - val_loss: 0.0905
Epoch 19/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1304 - val_loss: 0.0910
Epoch 20/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1294 - val_loss: 0.0897
Epoch 21/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1285 - val_loss: 0.0906
Epoch 22/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1278 - val_loss: 0.0895
Epoch 23/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1273 - val_loss: 0.0891
Epoch 24/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1266 - val_loss: 0.0893
Epoch 25/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1258 - val_loss: 0.0901
Epoch 26/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1255 - val_loss: 0.0907
Epoch 27/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1248 - val_loss: 0.0891
Epoch 28/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1242 - val_loss: 0.0903
Epoch 29/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1239 - val_loss: 0.0890
Epoch 30/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1235 - val_loss: 0.0891
Epoch 31/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1231 - val_loss: 0.0888
Epoch 32/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1226 - val_loss: 0.0874
Epoch 33/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1224 - val_loss: 0.0881
Epoch 34/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1220 - val_loss: 0.0898
Epoch 35/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1217 - val_loss: 0.0876
Epoch 36/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1215 - val_loss: 0.0887
Epoch 37/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1212 - val_loss: 0.0889
Epoch 38/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1207 - val_loss: 0.0880
Epoch 39/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1205 - val_loss: 0.0884
Epoch 40/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1201 - val_loss: 0.0892
Epoch 41/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1200 - val_loss: 0.0873
Epoch 42/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1198 - val_loss: 0.0893
Epoch 43/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1194 - val_loss: 0.0871
Epoch 44/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1191 - val_loss: 0.0863
Epoch 45/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1189 - val_loss: 0.0861
Epoch 46/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1185 - val_loss: 0.0867
Epoch 47/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1183 - val_loss: 0.0865
Epoch 48/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1179 - val_loss: 0.0862
Epoch 49/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1177 - val_loss: 0.0878
Epoch 50/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1175 - val_loss: 0.0864
Epoch 51/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1172 - val_loss: 0.0869
Epoch 52/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1167 - val_loss: 0.0845
Epoch 53/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1167 - val_loss: 0.0859
Epoch 54/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1163 - val_loss: 0.0865
Epoch 55/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1161 - val_loss: 0.0840
Epoch 56/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1158 - val_loss: 0.0845
Epoch 57/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1157 - val_loss: 0.0859
Epoch 58/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1153 - val_loss: 0.0843
Epoch 59/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1151 - val_loss: 0.0859
Epoch 60/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1147 - val_loss: 0.0841
Epoch 61/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1147 - val_loss: 0.0850
Epoch 62/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1144 - val_loss: 0.0840
Epoch 63/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1141 - val_loss: 0.0839
Epoch 64/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1139 - val_loss: 0.0837
Epoch 65/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1137 - val_loss: 0.0836
Epoch 66/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1137 - val_loss: 0.0845
Epoch 67/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1135 - val_loss: 0.0821
Epoch 68/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1132 - val_loss: 0.0835
Epoch 69/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1129 - val_loss: 0.0828
Epoch 70/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1128 - val_loss: 0.0831
Epoch 71/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1126 - val_loss: 0.0842
Epoch 72/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1124 - val_loss: 0.0835
Epoch 73/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1122 - val_loss: 0.0825
Epoch 74/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1120 - val_loss: 0.0825
Epoch 75/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1120 - val_loss: 0.0834
Epoch 76/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1117 - val_loss: 0.0838
Epoch 77/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1116 - val_loss: 0.0826
Val_yp Shape is 
(75738, 24)
Results === Test == Validation ===== 33
=============================
step 1    -  0.992    0.991
step 2    -  0.985    0.983
step 3    -  0.976    0.974
step 4    -  0.966    0.964
step 5    -  0.955    0.953
step 6    -  0.946    0.944
step 7    -  0.936    0.935
step 8    -  0.926    0.926
step 9    -  0.919    0.920
step 10   -  0.911    0.913
step 11   -  0.905    0.908
step 12   -  0.900    0.903
step 13   -  0.895    0.898
step 14   -  0.890    0.894
step 15   -  0.885    0.890
step 16   -  0.880    0.885
step 17   -  0.874    0.880
step 18   -  0.868    0.875
step 19   -  0.862    0.869
step 20   -  0.854    0.863
step 21   -  0.848    0.858
step 22   -  0.842    0.853
step 23   -  0.836    0.847
step 24   -  0.829    0.842
=============================
Summary   -  21.682    21.767
V_y Shape is 
(75738, 1)
=========== SHAPE INPUT-OUTPUT MATRIXES =====================
shape x_training : (227240, 238)  y_training: :  (227240, 24)
shape x_test     :  (75737, 238)  y_test      :  (75737, 24)
shape x_val      :  (75737, 238)  y_val       :  (75737, 24)
=============================================================
(238,)
Model: "MLP"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_8 (InputLayer)        [(None, 238)]             0         
                                                                 
 dense_28 (Dense)            (None, 1024)              244736    
                                                                 
 elu_7 (ELU)                 (None, 1024)              0         
                                                                 
 dropout_21 (Dropout)        (None, 1024)              0         
                                                                 
 dense_29 (Dense)            (None, 512)               524800    
                                                                 
 dropout_22 (Dropout)        (None, 512)               0         
                                                                 
 dense_30 (Dense)            (None, 512)               262656    
                                                                 
 dropout_23 (Dropout)        (None, 512)               0         
                                                                 
 dense_31 (Dense)            (None, 24)                12312     
                                                                 
=================================================================
Total params: 1,044,504
Trainable params: 1,044,504
Non-trainable params: 0
_________________________________________________________________
Epoch 1/200
222/222 [==============================] - 2s 7ms/step - loss: 0.3851 - val_loss: 0.1090
Epoch 2/200
222/222 [==============================] - 1s 6ms/step - loss: 0.2165 - val_loss: 0.1018
Epoch 3/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1868 - val_loss: 0.0987
Epoch 4/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1727 - val_loss: 0.0985
Epoch 5/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1639 - val_loss: 0.0972
Epoch 6/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1574 - val_loss: 0.0959
Epoch 7/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1534 - val_loss: 0.0948
Epoch 8/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1491 - val_loss: 0.0945
Epoch 9/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1461 - val_loss: 0.0922
Epoch 10/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1432 - val_loss: 0.0924
Epoch 11/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1412 - val_loss: 0.0917
Epoch 12/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1390 - val_loss: 0.0937
Epoch 13/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1374 - val_loss: 0.0929
Epoch 14/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1358 - val_loss: 0.0940
Epoch 15/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1345 - val_loss: 0.0922
Epoch 16/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1331 - val_loss: 0.0907
Epoch 17/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1322 - val_loss: 0.0923
Epoch 18/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1309 - val_loss: 0.0928
Epoch 19/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1299 - val_loss: 0.0923
Epoch 20/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1291 - val_loss: 0.0909
Epoch 21/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1282 - val_loss: 0.0888
Epoch 22/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1275 - val_loss: 0.0884
Epoch 23/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1266 - val_loss: 0.0901
Epoch 24/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1261 - val_loss: 0.0920
Epoch 25/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1256 - val_loss: 0.0889
Epoch 26/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1247 - val_loss: 0.0896
Epoch 27/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1245 - val_loss: 0.0894
Epoch 28/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1242 - val_loss: 0.0883
Epoch 29/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1236 - val_loss: 0.0889
Epoch 30/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1230 - val_loss: 0.0886
Epoch 31/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1229 - val_loss: 0.0900
Epoch 32/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1223 - val_loss: 0.0886
Epoch 33/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1219 - val_loss: 0.0890
Epoch 34/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1215 - val_loss: 0.0891
Epoch 35/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1211 - val_loss: 0.0889
Epoch 36/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1211 - val_loss: 0.0878
Epoch 37/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1206 - val_loss: 0.0872
Epoch 38/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1204 - val_loss: 0.0880
Epoch 39/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1201 - val_loss: 0.0877
Epoch 40/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1197 - val_loss: 0.0880
Epoch 41/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1196 - val_loss: 0.0864
Epoch 42/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1191 - val_loss: 0.0869
Epoch 43/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1187 - val_loss: 0.0872
Epoch 44/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1186 - val_loss: 0.0878
Epoch 45/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1183 - val_loss: 0.0864
Epoch 46/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1179 - val_loss: 0.0858
Epoch 47/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1175 - val_loss: 0.0861
Epoch 48/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1174 - val_loss: 0.0849
Epoch 49/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1170 - val_loss: 0.0859
Epoch 50/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1168 - val_loss: 0.0851
Epoch 51/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1165 - val_loss: 0.0851
Epoch 52/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1161 - val_loss: 0.0848
Epoch 53/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1160 - val_loss: 0.0842
Epoch 54/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1157 - val_loss: 0.0849
Epoch 55/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1153 - val_loss: 0.0847
Epoch 56/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1150 - val_loss: 0.0844
Epoch 57/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1151 - val_loss: 0.0834
Epoch 58/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1146 - val_loss: 0.0843
Epoch 59/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1146 - val_loss: 0.0848
Epoch 60/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1141 - val_loss: 0.0838
Epoch 61/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1140 - val_loss: 0.0846
Epoch 62/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1137 - val_loss: 0.0827
Epoch 63/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1137 - val_loss: 0.0834
Epoch 64/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1133 - val_loss: 0.0844
Epoch 65/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1132 - val_loss: 0.0824
Epoch 66/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1129 - val_loss: 0.0833
Epoch 67/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1129 - val_loss: 0.0828
Epoch 68/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1126 - val_loss: 0.0844
Epoch 69/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1125 - val_loss: 0.0847
Epoch 70/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1123 - val_loss: 0.0835
Epoch 71/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1121 - val_loss: 0.0832
Epoch 72/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1119 - val_loss: 0.0826
Epoch 73/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1118 - val_loss: 0.0817
Epoch 74/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1115 - val_loss: 0.0823
Epoch 75/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1113 - val_loss: 0.0824
Epoch 76/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1111 - val_loss: 0.0814
Epoch 77/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1109 - val_loss: 0.0828
Epoch 78/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1108 - val_loss: 0.0828
Epoch 79/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1107 - val_loss: 0.0818
Epoch 80/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1105 - val_loss: 0.0821
Epoch 81/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1103 - val_loss: 0.0803
Epoch 82/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1102 - val_loss: 0.0812
Epoch 83/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1100 - val_loss: 0.0828
Epoch 84/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1097 - val_loss: 0.0825
Epoch 85/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1098 - val_loss: 0.0804
Epoch 86/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1094 - val_loss: 0.0813
Epoch 87/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1094 - val_loss: 0.0833
Epoch 88/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1093 - val_loss: 0.0815
Epoch 89/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1094 - val_loss: 0.0807
Epoch 90/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1088 - val_loss: 0.0813
Epoch 91/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1090 - val_loss: 0.0813
Val_yp Shape is 
(75737, 24)
Results === Test == Validation ===== 34
=============================
step 1    -  0.993    0.991
step 2    -  0.986    0.984
step 3    -  0.977    0.974
step 4    -  0.967    0.964
step 5    -  0.956    0.954
step 6    -  0.946    0.945
step 7    -  0.937    0.936
step 8    -  0.928    0.928
step 9    -  0.920    0.920
step 10   -  0.912    0.914
step 11   -  0.905    0.907
step 12   -  0.899    0.901
step 13   -  0.893    0.895
step 14   -  0.888    0.891
step 15   -  0.882    0.885
step 16   -  0.875    0.879
step 17   -  0.870    0.874
step 18   -  0.864    0.870
step 19   -  0.859    0.865
step 20   -  0.853    0.860
step 21   -  0.848    0.856
step 22   -  0.842    0.852
step 23   -  0.836    0.847
step 24   -  0.829    0.841
=============================
Summary   -  21.663    21.734
V_y Shape is 
(75737, 1)
=========== SHAPE INPUT-OUTPUT MATRIXES =====================
shape x_training : (227239, 245)  y_training: :  (227239, 24)
shape x_test     :  (75736, 245)  y_test      :  (75736, 24)
shape x_val      :  (75737, 245)  y_val       :  (75737, 24)
=============================================================
(245,)
Model: "MLP"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_9 (InputLayer)        [(None, 245)]             0         
                                                                 
 dense_32 (Dense)            (None, 1024)              251904    
                                                                 
 elu_8 (ELU)                 (None, 1024)              0         
                                                                 
 dropout_24 (Dropout)        (None, 1024)              0         
                                                                 
 dense_33 (Dense)            (None, 512)               524800    
                                                                 
 dropout_25 (Dropout)        (None, 512)               0         
                                                                 
 dense_34 (Dense)            (None, 512)               262656    
                                                                 
 dropout_26 (Dropout)        (None, 512)               0         
                                                                 
 dense_35 (Dense)            (None, 24)                12312     
                                                                 
=================================================================
Total params: 1,051,672
Trainable params: 1,051,672
Non-trainable params: 0
_________________________________________________________________
Epoch 1/200
222/222 [==============================] - 2s 7ms/step - loss: 0.3914 - val_loss: 0.1106
Epoch 2/200
222/222 [==============================] - 1s 6ms/step - loss: 0.2178 - val_loss: 0.1016
Epoch 3/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1878 - val_loss: 0.0996
Epoch 4/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1735 - val_loss: 0.0984
Epoch 5/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1642 - val_loss: 0.0966
Epoch 6/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1580 - val_loss: 0.0952
Epoch 7/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1531 - val_loss: 0.0951
Epoch 8/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1493 - val_loss: 0.0940
Epoch 9/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1463 - val_loss: 0.0928
Epoch 10/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1436 - val_loss: 0.0950
Epoch 11/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1417 - val_loss: 0.0913
Epoch 12/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1397 - val_loss: 0.0937
Epoch 13/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1377 - val_loss: 0.0916
Epoch 14/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1360 - val_loss: 0.0919
Epoch 15/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1349 - val_loss: 0.0900
Epoch 16/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1333 - val_loss: 0.0919
Epoch 17/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1323 - val_loss: 0.0901
Epoch 18/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1311 - val_loss: 0.0910
Epoch 19/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1302 - val_loss: 0.0910
Epoch 20/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1291 - val_loss: 0.0918
Epoch 21/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1284 - val_loss: 0.0911
Epoch 22/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1277 - val_loss: 0.0885
Epoch 23/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1269 - val_loss: 0.0898
Epoch 24/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1262 - val_loss: 0.0925
Epoch 25/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1254 - val_loss: 0.0923
Epoch 26/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1249 - val_loss: 0.0889
Epoch 27/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1244 - val_loss: 0.0911
Epoch 28/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1238 - val_loss: 0.0892
Epoch 29/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1234 - val_loss: 0.0894
Epoch 30/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1231 - val_loss: 0.0891
Epoch 31/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1227 - val_loss: 0.0890
Epoch 32/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1221 - val_loss: 0.0895
Val_yp Shape is 
(75737, 24)
Results === Test == Validation ===== 35
=============================
step 1    -  0.991    0.990
step 2    -  0.983    0.982
step 3    -  0.974    0.972
step 4    -  0.963    0.961
step 5    -  0.952    0.950
step 6    -  0.941    0.939
step 7    -  0.930    0.929
step 8    -  0.921    0.920
step 9    -  0.912    0.912
step 10   -  0.904    0.905
step 11   -  0.897    0.899
step 12   -  0.890    0.892
step 13   -  0.885    0.888
step 14   -  0.879    0.883
step 15   -  0.874    0.878
step 16   -  0.869    0.874
step 17   -  0.863    0.869
step 18   -  0.856    0.864
step 19   -  0.852    0.860
step 20   -  0.846    0.855
step 21   -  0.840    0.850
step 22   -  0.833    0.845
step 23   -  0.827    0.839
step 24   -  0.821    0.834
=============================
Summary   -  21.503    21.589
V_y Shape is 
(75737, 1)
=========== SHAPE INPUT-OUTPUT MATRIXES =====================
shape x_training : (227238, 252)  y_training: :  (227238, 24)
shape x_test     :  (75736, 252)  y_test      :  (75736, 24)
shape x_val      :  (75736, 252)  y_val       :  (75736, 24)
=============================================================
(252,)
Model: "MLP"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_10 (InputLayer)       [(None, 252)]             0         
                                                                 
 dense_36 (Dense)            (None, 1024)              259072    
                                                                 
 elu_9 (ELU)                 (None, 1024)              0         
                                                                 
 dropout_27 (Dropout)        (None, 1024)              0         
                                                                 
 dense_37 (Dense)            (None, 512)               524800    
                                                                 
 dropout_28 (Dropout)        (None, 512)               0         
                                                                 
 dense_38 (Dense)            (None, 512)               262656    
                                                                 
 dropout_29 (Dropout)        (None, 512)               0         
                                                                 
 dense_39 (Dense)            (None, 24)                12312     
                                                                 
=================================================================
Total params: 1,058,840
Trainable params: 1,058,840
Non-trainable params: 0
_________________________________________________________________
Epoch 1/200
222/222 [==============================] - 2s 8ms/step - loss: 0.4028 - val_loss: 0.1096
Epoch 2/200
222/222 [==============================] - 1s 6ms/step - loss: 0.2196 - val_loss: 0.1026
Epoch 3/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1890 - val_loss: 0.0999
Epoch 4/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1743 - val_loss: 0.0974
Epoch 5/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1653 - val_loss: 0.0962
Epoch 6/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1585 - val_loss: 0.0954
Epoch 7/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1539 - val_loss: 0.0962
Epoch 8/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1497 - val_loss: 0.0950
Epoch 9/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1467 - val_loss: 0.0927
Epoch 10/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1438 - val_loss: 0.0934
Epoch 11/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1416 - val_loss: 0.0929
Epoch 12/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1394 - val_loss: 0.0928
Epoch 13/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1380 - val_loss: 0.0929
Epoch 14/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1361 - val_loss: 0.0909
Epoch 15/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1347 - val_loss: 0.0929
Epoch 16/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1335 - val_loss: 0.0917
Epoch 17/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1320 - val_loss: 0.0918
Epoch 18/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1309 - val_loss: 0.0900
Epoch 19/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1298 - val_loss: 0.0899
Epoch 20/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1292 - val_loss: 0.0901
Epoch 21/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1280 - val_loss: 0.0892
Epoch 22/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1273 - val_loss: 0.0883
Epoch 23/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1267 - val_loss: 0.0896
Epoch 24/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1260 - val_loss: 0.0888
Epoch 25/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1254 - val_loss: 0.0889
Epoch 26/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1248 - val_loss: 0.0897
Epoch 27/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1243 - val_loss: 0.0895
Epoch 28/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1235 - val_loss: 0.0889
Epoch 29/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1232 - val_loss: 0.0894
Epoch 30/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1228 - val_loss: 0.0898
Epoch 31/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1223 - val_loss: 0.0887
Epoch 32/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1220 - val_loss: 0.0874
Epoch 33/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1216 - val_loss: 0.0895
Epoch 34/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1213 - val_loss: 0.0884
Epoch 35/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1209 - val_loss: 0.0896
Epoch 36/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1206 - val_loss: 0.0882
Epoch 37/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1202 - val_loss: 0.0888
Epoch 38/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1199 - val_loss: 0.0876
Epoch 39/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1197 - val_loss: 0.0871
Epoch 40/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1194 - val_loss: 0.0883
Epoch 41/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1187 - val_loss: 0.0900
Epoch 42/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1187 - val_loss: 0.0885
Epoch 43/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1184 - val_loss: 0.0870
Epoch 44/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1182 - val_loss: 0.0861
Epoch 45/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1178 - val_loss: 0.0859
Epoch 46/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1174 - val_loss: 0.0871
Epoch 47/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1173 - val_loss: 0.0851
Epoch 48/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1170 - val_loss: 0.0853
Epoch 49/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1165 - val_loss: 0.0860
Epoch 50/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1164 - val_loss: 0.0846
Epoch 51/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1159 - val_loss: 0.0846
Epoch 52/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1155 - val_loss: 0.0861
Epoch 53/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1154 - val_loss: 0.0856
Epoch 54/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1151 - val_loss: 0.0844
Epoch 55/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1150 - val_loss: 0.0841
Epoch 56/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1148 - val_loss: 0.0859
Epoch 57/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1145 - val_loss: 0.0835
Epoch 58/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1141 - val_loss: 0.0838
Epoch 59/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1139 - val_loss: 0.0842
Epoch 60/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1138 - val_loss: 0.0850
Epoch 61/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1136 - val_loss: 0.0843
Epoch 62/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1131 - val_loss: 0.0842
Epoch 63/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1130 - val_loss: 0.0820
Epoch 64/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1128 - val_loss: 0.0822
Epoch 65/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1125 - val_loss: 0.0826
Epoch 66/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1125 - val_loss: 0.0827
Epoch 67/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1121 - val_loss: 0.0823
Epoch 68/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1119 - val_loss: 0.0821
Epoch 69/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1117 - val_loss: 0.0826
Epoch 70/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1116 - val_loss: 0.0838
Epoch 71/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1115 - val_loss: 0.0823
Epoch 72/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1113 - val_loss: 0.0828
Epoch 73/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1111 - val_loss: 0.0826
Val_yp Shape is 
(75736, 24)
Results === Test == Validation ===== 36
=============================
step 1    -  0.991    0.990
step 2    -  0.983    0.982
step 3    -  0.973    0.971
step 4    -  0.962    0.959
step 5    -  0.951    0.949
step 6    -  0.941    0.939
step 7    -  0.931    0.929
step 8    -  0.922    0.921
step 9    -  0.915    0.914
step 10   -  0.908    0.908
step 11   -  0.901    0.902
step 12   -  0.896    0.897
step 13   -  0.890    0.892
step 14   -  0.885    0.888
step 15   -  0.879    0.882
step 16   -  0.874    0.877
step 17   -  0.867    0.872
step 18   -  0.862    0.867
step 19   -  0.856    0.862
step 20   -  0.849    0.856
step 21   -  0.842    0.850
step 22   -  0.835    0.844
step 23   -  0.827    0.837
step 24   -  0.817    0.828
=============================
Summary   -  21.560    21.617
V_y Shape is 
(75736, 1)
=========== SHAPE INPUT-OUTPUT MATRIXES =====================
shape x_training : (227237, 259)  y_training: :  (227237, 24)
shape x_test     :  (75735, 259)  y_test      :  (75735, 24)
shape x_val      :  (75736, 259)  y_val       :  (75736, 24)
=============================================================
(259,)
Model: "MLP"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_11 (InputLayer)       [(None, 259)]             0         
                                                                 
 dense_40 (Dense)            (None, 1024)              266240    
                                                                 
 elu_10 (ELU)                (None, 1024)              0         
                                                                 
 dropout_30 (Dropout)        (None, 1024)              0         
                                                                 
 dense_41 (Dense)            (None, 512)               524800    
                                                                 
 dropout_31 (Dropout)        (None, 512)               0         
                                                                 
 dense_42 (Dense)            (None, 512)               262656    
                                                                 
 dropout_32 (Dropout)        (None, 512)               0         
                                                                 
 dense_43 (Dense)            (None, 24)                12312     
                                                                 
=================================================================
Total params: 1,066,008
Trainable params: 1,066,008
Non-trainable params: 0
_________________________________________________________________
Epoch 1/200
222/222 [==============================] - 2s 7ms/step - loss: 0.4066 - val_loss: 0.1103
Epoch 2/200
222/222 [==============================] - 1s 6ms/step - loss: 0.2208 - val_loss: 0.1042
Epoch 3/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1899 - val_loss: 0.0993
Epoch 4/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1749 - val_loss: 0.0975
Epoch 5/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1653 - val_loss: 0.0961
Epoch 6/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1586 - val_loss: 0.0941
Epoch 7/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1536 - val_loss: 0.0951
Epoch 8/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1496 - val_loss: 0.0941
Epoch 9/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1466 - val_loss: 0.0938
Epoch 10/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1436 - val_loss: 0.0921
Epoch 11/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1414 - val_loss: 0.0925
Epoch 12/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1393 - val_loss: 0.0914
Epoch 13/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1376 - val_loss: 0.0910
Epoch 14/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1360 - val_loss: 0.0902
Epoch 15/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1345 - val_loss: 0.0913
Epoch 16/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1330 - val_loss: 0.0918
Epoch 17/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1317 - val_loss: 0.0914
Epoch 18/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1306 - val_loss: 0.0916
Epoch 19/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1297 - val_loss: 0.0915
Epoch 20/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1286 - val_loss: 0.0899
Epoch 21/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1279 - val_loss: 0.0895
Epoch 22/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1270 - val_loss: 0.0897
Epoch 23/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1263 - val_loss: 0.0903
Epoch 24/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1257 - val_loss: 0.0905
Epoch 25/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1250 - val_loss: 0.0912
Epoch 26/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1246 - val_loss: 0.0881
Epoch 27/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1238 - val_loss: 0.0905
Epoch 28/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1234 - val_loss: 0.0871
Epoch 29/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1229 - val_loss: 0.0885
Epoch 30/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1226 - val_loss: 0.0879
Epoch 31/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1220 - val_loss: 0.0877
Epoch 32/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1217 - val_loss: 0.0876
Epoch 33/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1211 - val_loss: 0.0889
Epoch 34/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1209 - val_loss: 0.0892
Epoch 35/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1206 - val_loss: 0.0870
Epoch 36/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1203 - val_loss: 0.0873
Epoch 37/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1199 - val_loss: 0.0891
Epoch 38/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1196 - val_loss: 0.0874
Epoch 39/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1194 - val_loss: 0.0882
Epoch 40/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1189 - val_loss: 0.0889
Epoch 41/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1187 - val_loss: 0.0882
Epoch 42/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1184 - val_loss: 0.0859
Epoch 43/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1180 - val_loss: 0.0867
Epoch 44/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1177 - val_loss: 0.0854
Epoch 45/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1176 - val_loss: 0.0861
Epoch 46/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1170 - val_loss: 0.0847
Epoch 47/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1166 - val_loss: 0.0852
Epoch 48/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1164 - val_loss: 0.0846
Epoch 49/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1162 - val_loss: 0.0844
Epoch 50/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1159 - val_loss: 0.0855
Epoch 51/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1157 - val_loss: 0.0865
Epoch 52/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1150 - val_loss: 0.0839
Epoch 53/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1149 - val_loss: 0.0863
Epoch 54/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1146 - val_loss: 0.0845
Epoch 55/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1145 - val_loss: 0.0875
Epoch 56/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1142 - val_loss: 0.0843
Epoch 57/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1138 - val_loss: 0.0837
Epoch 58/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1134 - val_loss: 0.0836
Epoch 59/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1135 - val_loss: 0.0839
Epoch 60/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1130 - val_loss: 0.0837
Epoch 61/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1129 - val_loss: 0.0830
Epoch 62/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1127 - val_loss: 0.0823
Epoch 63/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1126 - val_loss: 0.0825
Epoch 64/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1120 - val_loss: 0.0837
Epoch 65/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1123 - val_loss: 0.0822
Epoch 66/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1118 - val_loss: 0.0829
Epoch 67/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1117 - val_loss: 0.0839
Epoch 68/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1113 - val_loss: 0.0817
Epoch 69/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1112 - val_loss: 0.0822
Epoch 70/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1110 - val_loss: 0.0833
Epoch 71/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1108 - val_loss: 0.0819
Epoch 72/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1105 - val_loss: 0.0812
Epoch 73/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1103 - val_loss: 0.0826
Epoch 74/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1102 - val_loss: 0.0807
Epoch 75/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1101 - val_loss: 0.0810
Epoch 76/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1097 - val_loss: 0.0811
Epoch 77/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1096 - val_loss: 0.0815
Epoch 78/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1094 - val_loss: 0.0817
Epoch 79/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1096 - val_loss: 0.0821
Epoch 80/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1093 - val_loss: 0.0821
Epoch 81/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1091 - val_loss: 0.0805
Epoch 82/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1089 - val_loss: 0.0826
Epoch 83/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1089 - val_loss: 0.0805
Epoch 84/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1085 - val_loss: 0.0810
Epoch 85/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1083 - val_loss: 0.0796
Epoch 86/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1084 - val_loss: 0.0810
Epoch 87/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1082 - val_loss: 0.0817
Epoch 88/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1079 - val_loss: 0.0812
Epoch 89/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1078 - val_loss: 0.0811
Epoch 90/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1078 - val_loss: 0.0813
Epoch 91/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1075 - val_loss: 0.0818
Epoch 92/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1076 - val_loss: 0.0822
Epoch 93/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1072 - val_loss: 0.0812
Epoch 94/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1072 - val_loss: 0.0804
Epoch 95/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1069 - val_loss: 0.0825
Val_yp Shape is 
(75736, 24)
Results === Test == Validation ===== 37
=============================
step 1    -  0.992    0.991
step 2    -  0.984    0.983
step 3    -  0.975    0.973
step 4    -  0.965    0.963
step 5    -  0.955    0.953
step 6    -  0.945    0.944
step 7    -  0.936    0.935
step 8    -  0.927    0.927
step 9    -  0.919    0.920
step 10   -  0.912    0.913
step 11   -  0.906    0.907
step 12   -  0.899    0.901
step 13   -  0.893    0.895
step 14   -  0.887    0.889
step 15   -  0.881    0.883
step 16   -  0.875    0.878
step 17   -  0.869    0.873
step 18   -  0.865    0.869
step 19   -  0.860    0.865
step 20   -  0.855    0.860
step 21   -  0.849    0.856
step 22   -  0.845    0.852
step 23   -  0.839    0.847
step 24   -  0.831    0.840
=============================
Summary   -  21.666    21.717
V_y Shape is 
(75736, 1)
=========== SHAPE INPUT-OUTPUT MATRIXES =====================
shape x_training : (227236, 266)  y_training: :  (227236, 24)
shape x_test     :  (75735, 266)  y_test      :  (75735, 24)
shape x_val      :  (75735, 266)  y_val       :  (75735, 24)
=============================================================
(266,)
Model: "MLP"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_12 (InputLayer)       [(None, 266)]             0         
                                                                 
 dense_44 (Dense)            (None, 1024)              273408    
                                                                 
 elu_11 (ELU)                (None, 1024)              0         
                                                                 
 dropout_33 (Dropout)        (None, 1024)              0         
                                                                 
 dense_45 (Dense)            (None, 512)               524800    
                                                                 
 dropout_34 (Dropout)        (None, 512)               0         
                                                                 
 dense_46 (Dense)            (None, 512)               262656    
                                                                 
 dropout_35 (Dropout)        (None, 512)               0         
                                                                 
 dense_47 (Dense)            (None, 24)                12312     
                                                                 
=================================================================
Total params: 1,073,176
Trainable params: 1,073,176
Non-trainable params: 0
_________________________________________________________________
Epoch 1/200
222/222 [==============================] - 2s 7ms/step - loss: 0.3979 - val_loss: 0.1100
Epoch 2/200
222/222 [==============================] - 1s 6ms/step - loss: 0.2185 - val_loss: 0.1025
Epoch 3/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1881 - val_loss: 0.0991
Epoch 4/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1735 - val_loss: 0.0976
Epoch 5/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1641 - val_loss: 0.0959
Epoch 6/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1578 - val_loss: 0.0958
Epoch 7/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1527 - val_loss: 0.0957
Epoch 8/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1489 - val_loss: 0.0944
Epoch 9/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1459 - val_loss: 0.0949
Epoch 10/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1431 - val_loss: 0.0931
Epoch 11/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1407 - val_loss: 0.0929
Epoch 12/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1384 - val_loss: 0.0944
Epoch 13/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1369 - val_loss: 0.0926
Epoch 14/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1352 - val_loss: 0.0938
Epoch 15/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1336 - val_loss: 0.0905
Epoch 16/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1326 - val_loss: 0.0896
Epoch 17/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1312 - val_loss: 0.0903
Epoch 18/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1302 - val_loss: 0.0904
Epoch 19/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1290 - val_loss: 0.0915
Epoch 20/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1282 - val_loss: 0.0919
Epoch 21/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1273 - val_loss: 0.0913
Epoch 22/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1265 - val_loss: 0.0891
Epoch 23/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1258 - val_loss: 0.0886
Epoch 24/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1253 - val_loss: 0.0905
Epoch 25/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1247 - val_loss: 0.0886
Epoch 26/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1239 - val_loss: 0.0891
Epoch 27/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1234 - val_loss: 0.0887
Epoch 28/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1228 - val_loss: 0.0884
Epoch 29/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1226 - val_loss: 0.0869
Epoch 30/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1218 - val_loss: 0.0887
Epoch 31/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1216 - val_loss: 0.0884
Epoch 32/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1214 - val_loss: 0.0882
Epoch 33/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1210 - val_loss: 0.0878
Epoch 34/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1204 - val_loss: 0.0878
Epoch 35/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1202 - val_loss: 0.0868
Epoch 36/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1200 - val_loss: 0.0879
Epoch 37/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1194 - val_loss: 0.0880
Epoch 38/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1192 - val_loss: 0.0869
Epoch 39/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1187 - val_loss: 0.0860
Epoch 40/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1185 - val_loss: 0.0875
Epoch 41/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1183 - val_loss: 0.0861
Epoch 42/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1177 - val_loss: 0.0866
Epoch 43/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1176 - val_loss: 0.0845
Epoch 44/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1172 - val_loss: 0.0861
Epoch 45/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1171 - val_loss: 0.0857
Epoch 46/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1167 - val_loss: 0.0867
Epoch 47/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1165 - val_loss: 0.0862
Epoch 48/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1160 - val_loss: 0.0842
Epoch 49/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1158 - val_loss: 0.0849
Epoch 50/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1155 - val_loss: 0.0850
Epoch 51/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1151 - val_loss: 0.0855
Epoch 52/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1148 - val_loss: 0.0840
Epoch 53/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1147 - val_loss: 0.0841
Epoch 54/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1141 - val_loss: 0.0841
Epoch 55/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1140 - val_loss: 0.0830
Epoch 56/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1139 - val_loss: 0.0836
Epoch 57/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1134 - val_loss: 0.0832
Epoch 58/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1133 - val_loss: 0.0824
Epoch 59/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1128 - val_loss: 0.0827
Epoch 60/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1127 - val_loss: 0.0838
Epoch 61/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1128 - val_loss: 0.0832
Epoch 62/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1121 - val_loss: 0.0837
Epoch 63/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1121 - val_loss: 0.0842
Epoch 64/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1120 - val_loss: 0.0822
Epoch 65/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1114 - val_loss: 0.0833
Epoch 66/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1113 - val_loss: 0.0819
Epoch 67/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1111 - val_loss: 0.0829
Epoch 68/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1111 - val_loss: 0.0821
Epoch 69/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1109 - val_loss: 0.0838
Epoch 70/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1104 - val_loss: 0.0816
Epoch 71/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1103 - val_loss: 0.0816
Epoch 72/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1103 - val_loss: 0.0825
Epoch 73/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1100 - val_loss: 0.0810
Epoch 74/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1098 - val_loss: 0.0815
Epoch 75/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1096 - val_loss: 0.0813
Epoch 76/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1096 - val_loss: 0.0817
Epoch 77/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1094 - val_loss: 0.0814
Epoch 78/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1091 - val_loss: 0.0803
Epoch 79/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1091 - val_loss: 0.0813
Epoch 80/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1088 - val_loss: 0.0813
Epoch 81/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1087 - val_loss: 0.0807
Epoch 82/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1086 - val_loss: 0.0811
Epoch 83/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1082 - val_loss: 0.0813
Epoch 84/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1082 - val_loss: 0.0801
Epoch 85/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1080 - val_loss: 0.0811
Epoch 86/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1076 - val_loss: 0.0821
Epoch 87/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1078 - val_loss: 0.0817
Epoch 88/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1075 - val_loss: 0.0797
Epoch 89/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1076 - val_loss: 0.0814
Epoch 90/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1074 - val_loss: 0.0808
Epoch 91/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1071 - val_loss: 0.0809
Epoch 92/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1070 - val_loss: 0.0800
Epoch 93/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1067 - val_loss: 0.0807
Epoch 94/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1064 - val_loss: 0.0808
Epoch 95/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1065 - val_loss: 0.0798
Epoch 96/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1065 - val_loss: 0.0794
Epoch 97/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1063 - val_loss: 0.0805
Epoch 98/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1061 - val_loss: 0.0797
Epoch 99/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1060 - val_loss: 0.0803
Epoch 100/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1059 - val_loss: 0.0804
Epoch 101/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1058 - val_loss: 0.0809
Epoch 102/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1054 - val_loss: 0.0812
Epoch 103/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1053 - val_loss: 0.0806
Epoch 104/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1052 - val_loss: 0.0810
Epoch 105/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1053 - val_loss: 0.0795
Epoch 106/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1051 - val_loss: 0.0801
Val_yp Shape is 
(75735, 24)
Results === Test == Validation ===== 38
=============================
step 1    -  0.992    0.992
step 2    -  0.985    0.984
step 3    -  0.977    0.975
step 4    -  0.967    0.965
step 5    -  0.958    0.956
step 6    -  0.949    0.947
step 7    -  0.940    0.938
step 8    -  0.931    0.931
step 9    -  0.924    0.925
step 10   -  0.917    0.918
step 11   -  0.910    0.911
step 12   -  0.902    0.905
step 13   -  0.895    0.898
step 14   -  0.890    0.893
step 15   -  0.884    0.888
step 16   -  0.879    0.883
step 17   -  0.875    0.879
step 18   -  0.869    0.875
step 19   -  0.865    0.871
step 20   -  0.860    0.867
step 21   -  0.855    0.863
step 22   -  0.849    0.858
step 23   -  0.843    0.853
step 24   -  0.837    0.848
=============================
Summary   -  21.755    21.822
V_y Shape is 
(75735, 1)
=========== SHAPE INPUT-OUTPUT MATRIXES =====================
shape x_training : (227235, 273)  y_training: :  (227235, 24)
shape x_test     :  (75734, 273)  y_test      :  (75734, 24)
shape x_val      :  (75735, 273)  y_val       :  (75735, 24)
=============================================================
(273,)
Model: "MLP"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_13 (InputLayer)       [(None, 273)]             0         
                                                                 
 dense_48 (Dense)            (None, 1024)              280576    
                                                                 
 elu_12 (ELU)                (None, 1024)              0         
                                                                 
 dropout_36 (Dropout)        (None, 1024)              0         
                                                                 
 dense_49 (Dense)            (None, 512)               524800    
                                                                 
 dropout_37 (Dropout)        (None, 512)               0         
                                                                 
 dense_50 (Dense)            (None, 512)               262656    
                                                                 
 dropout_38 (Dropout)        (None, 512)               0         
                                                                 
 dense_51 (Dense)            (None, 24)                12312     
                                                                 
=================================================================
Total params: 1,080,344
Trainable params: 1,080,344
Non-trainable params: 0
_________________________________________________________________
Epoch 1/200
222/222 [==============================] - 2s 8ms/step - loss: 0.4061 - val_loss: 0.1091
Epoch 2/200
222/222 [==============================] - 1s 6ms/step - loss: 0.2206 - val_loss: 0.1026
Epoch 3/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1896 - val_loss: 0.0994
Epoch 4/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1748 - val_loss: 0.0964
Epoch 5/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1649 - val_loss: 0.0965
Epoch 6/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1586 - val_loss: 0.0947
Epoch 7/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1537 - val_loss: 0.0933
Epoch 8/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1494 - val_loss: 0.0948
Epoch 9/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1460 - val_loss: 0.0927
Epoch 10/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1433 - val_loss: 0.0935
Epoch 11/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1410 - val_loss: 0.0922
Epoch 12/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1388 - val_loss: 0.0917
Epoch 13/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1371 - val_loss: 0.0926
Epoch 14/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1354 - val_loss: 0.0899
Epoch 15/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1339 - val_loss: 0.0906
Epoch 16/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1325 - val_loss: 0.0903
Epoch 17/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1312 - val_loss: 0.0905
Epoch 18/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1300 - val_loss: 0.0887
Epoch 19/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1290 - val_loss: 0.0899
Epoch 20/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1280 - val_loss: 0.0903
Epoch 21/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1272 - val_loss: 0.0902
Epoch 22/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1266 - val_loss: 0.0899
Epoch 23/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1256 - val_loss: 0.0895
Epoch 24/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1249 - val_loss: 0.0925
Epoch 25/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1243 - val_loss: 0.0884
Epoch 26/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1239 - val_loss: 0.0880
Epoch 27/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1232 - val_loss: 0.0895
Epoch 28/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1228 - val_loss: 0.0874
Epoch 29/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1224 - val_loss: 0.0903
Epoch 30/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1217 - val_loss: 0.0881
Epoch 31/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1214 - val_loss: 0.0897
Epoch 32/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1210 - val_loss: 0.0880
Epoch 33/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1205 - val_loss: 0.0874
Epoch 34/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1202 - val_loss: 0.0880
Epoch 35/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1199 - val_loss: 0.0879
Epoch 36/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1197 - val_loss: 0.0867
Epoch 37/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1194 - val_loss: 0.0871
Epoch 38/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1188 - val_loss: 0.0874
Epoch 39/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1186 - val_loss: 0.0872
Epoch 40/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1182 - val_loss: 0.0864
Epoch 41/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1179 - val_loss: 0.0871
Epoch 42/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1177 - val_loss: 0.0863
Epoch 43/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1172 - val_loss: 0.0859
Epoch 44/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1168 - val_loss: 0.0852
Epoch 45/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1167 - val_loss: 0.0843
Epoch 46/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1163 - val_loss: 0.0857
Epoch 47/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1160 - val_loss: 0.0852
Epoch 48/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1156 - val_loss: 0.0847
Epoch 49/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1154 - val_loss: 0.0840
Epoch 50/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1152 - val_loss: 0.0852
Epoch 51/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1148 - val_loss: 0.0852
Epoch 52/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1148 - val_loss: 0.0845
Epoch 53/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1143 - val_loss: 0.0853
Epoch 54/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1141 - val_loss: 0.0846
Epoch 55/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1136 - val_loss: 0.0842
Epoch 56/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1133 - val_loss: 0.0834
Epoch 57/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1132 - val_loss: 0.0857
Epoch 58/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1128 - val_loss: 0.0836
Epoch 59/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1125 - val_loss: 0.0823
Epoch 60/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1123 - val_loss: 0.0836
Epoch 61/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1121 - val_loss: 0.0838
Epoch 62/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1123 - val_loss: 0.0828
Epoch 63/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1117 - val_loss: 0.0821
Epoch 64/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1116 - val_loss: 0.0824
Epoch 65/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1111 - val_loss: 0.0812
Epoch 66/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1111 - val_loss: 0.0827
Epoch 67/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1109 - val_loss: 0.0820
Epoch 68/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1106 - val_loss: 0.0817
Epoch 69/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1105 - val_loss: 0.0817
Epoch 70/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1101 - val_loss: 0.0817
Epoch 71/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1101 - val_loss: 0.0821
Epoch 72/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1100 - val_loss: 0.0824
Epoch 73/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1096 - val_loss: 0.0819
Epoch 74/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1094 - val_loss: 0.0811
Epoch 75/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1095 - val_loss: 0.0834
Epoch 76/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1095 - val_loss: 0.0820
Epoch 77/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1091 - val_loss: 0.0815
Epoch 78/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1086 - val_loss: 0.0821
Epoch 79/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1087 - val_loss: 0.0828
Epoch 80/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1084 - val_loss: 0.0807
Epoch 81/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1082 - val_loss: 0.0810
Epoch 82/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1080 - val_loss: 0.0816
Epoch 83/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1079 - val_loss: 0.0818
Epoch 84/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1078 - val_loss: 0.0816
Epoch 85/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1077 - val_loss: 0.0823
Epoch 86/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1076 - val_loss: 0.0811
Epoch 87/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1072 - val_loss: 0.0798
Epoch 88/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1071 - val_loss: 0.0812
Epoch 89/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1071 - val_loss: 0.0800
Epoch 90/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1069 - val_loss: 0.0798
Epoch 91/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1067 - val_loss: 0.0797
Epoch 92/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1066 - val_loss: 0.0798
Epoch 93/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1063 - val_loss: 0.0801
Epoch 94/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1061 - val_loss: 0.0809
Epoch 95/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1062 - val_loss: 0.0803
Epoch 96/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1060 - val_loss: 0.0806
Epoch 97/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1059 - val_loss: 0.0796
Epoch 98/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1057 - val_loss: 0.0801
Epoch 99/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1055 - val_loss: 0.0805
Epoch 100/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1055 - val_loss: 0.0806
Epoch 101/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1052 - val_loss: 0.0797
Epoch 102/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1052 - val_loss: 0.0810
Epoch 103/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1051 - val_loss: 0.0799
Epoch 104/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1050 - val_loss: 0.0792
Epoch 105/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1047 - val_loss: 0.0794
Epoch 106/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1048 - val_loss: 0.0817
Epoch 107/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1044 - val_loss: 0.0803
Epoch 108/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1044 - val_loss: 0.0798
Epoch 109/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1044 - val_loss: 0.0795
Epoch 110/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1043 - val_loss: 0.0793
Epoch 111/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1038 - val_loss: 0.0796
Epoch 112/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1039 - val_loss: 0.0792
Epoch 113/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1038 - val_loss: 0.0789
Epoch 114/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1037 - val_loss: 0.0791
Epoch 115/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1036 - val_loss: 0.0818
Epoch 116/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1035 - val_loss: 0.0803
Epoch 117/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1032 - val_loss: 0.0799
Epoch 118/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1030 - val_loss: 0.0788
Epoch 119/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1030 - val_loss: 0.0784
Epoch 120/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1030 - val_loss: 0.0786
Epoch 121/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1028 - val_loss: 0.0801
Epoch 122/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1026 - val_loss: 0.0789
Epoch 123/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1024 - val_loss: 0.0792
Epoch 124/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1024 - val_loss: 0.0793
Epoch 125/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1024 - val_loss: 0.0796
Epoch 126/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1020 - val_loss: 0.0795
Epoch 127/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1020 - val_loss: 0.0796
Epoch 128/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1020 - val_loss: 0.0781
Epoch 129/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1020 - val_loss: 0.0799
Epoch 130/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1019 - val_loss: 0.0801
Epoch 131/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1018 - val_loss: 0.0794
Epoch 132/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1016 - val_loss: 0.0785
Epoch 133/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1015 - val_loss: 0.0791
Epoch 134/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1016 - val_loss: 0.0784
Epoch 135/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1014 - val_loss: 0.0789
Epoch 136/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1012 - val_loss: 0.0804
Epoch 137/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1009 - val_loss: 0.0803
Epoch 138/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1007 - val_loss: 0.0792
Val_yp Shape is 
(75735, 24)
Results === Test == Validation ===== 39
=============================
step 1    -  0.992    0.991
step 2    -  0.985    0.984
step 3    -  0.977    0.975
step 4    -  0.968    0.965
step 5    -  0.959    0.956
step 6    -  0.949    0.948
step 7    -  0.940    0.939
step 8    -  0.932    0.932
step 9    -  0.924    0.925
step 10   -  0.917    0.918
step 11   -  0.910    0.912
step 12   -  0.903    0.906
step 13   -  0.896    0.899
step 14   -  0.890    0.894
step 15   -  0.883    0.887
step 16   -  0.876    0.881
step 17   -  0.869    0.875
step 18   -  0.864    0.870
step 19   -  0.858    0.866
step 20   -  0.853    0.860
step 21   -  0.848    0.857
step 22   -  0.843    0.853
step 23   -  0.838    0.848
step 24   -  0.831    0.842
=============================
Summary   -  21.704    21.782
V_y Shape is 
(75735, 1)
=========== SHAPE INPUT-OUTPUT MATRIXES =====================
shape x_training : (227234, 280)  y_training: :  (227234, 24)
shape x_test     :  (75734, 280)  y_test      :  (75734, 24)
shape x_val      :  (75734, 280)  y_val       :  (75734, 24)
=============================================================
(280,)
Model: "MLP"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_14 (InputLayer)       [(None, 280)]             0         
                                                                 
 dense_52 (Dense)            (None, 1024)              287744    
                                                                 
 elu_13 (ELU)                (None, 1024)              0         
                                                                 
 dropout_39 (Dropout)        (None, 1024)              0         
                                                                 
 dense_53 (Dense)            (None, 512)               524800    
                                                                 
 dropout_40 (Dropout)        (None, 512)               0         
                                                                 
 dense_54 (Dense)            (None, 512)               262656    
                                                                 
 dropout_41 (Dropout)        (None, 512)               0         
                                                                 
 dense_55 (Dense)            (None, 24)                12312     
                                                                 
=================================================================
Total params: 1,087,512
Trainable params: 1,087,512
Non-trainable params: 0
_________________________________________________________________
Epoch 1/200
222/222 [==============================] - 2s 7ms/step - loss: 0.4066 - val_loss: 0.1082
Epoch 2/200
222/222 [==============================] - 1s 6ms/step - loss: 0.2216 - val_loss: 0.1009
Epoch 3/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1897 - val_loss: 0.0981
Epoch 4/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1744 - val_loss: 0.0959
Epoch 5/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1653 - val_loss: 0.0967
Epoch 6/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1587 - val_loss: 0.0953
Epoch 7/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1536 - val_loss: 0.0923
Epoch 8/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1493 - val_loss: 0.0936
Epoch 9/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1462 - val_loss: 0.0910
Epoch 10/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1436 - val_loss: 0.0920
Epoch 11/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1409 - val_loss: 0.0928
Epoch 12/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1388 - val_loss: 0.0911
Epoch 13/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1368 - val_loss: 0.0910
Epoch 14/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1353 - val_loss: 0.0926
Epoch 15/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1339 - val_loss: 0.0910
Epoch 16/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1323 - val_loss: 0.0911
Epoch 17/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1312 - val_loss: 0.0895
Epoch 18/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1299 - val_loss: 0.0899
Epoch 19/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1287 - val_loss: 0.0910
Epoch 20/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1280 - val_loss: 0.0912
Epoch 21/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1271 - val_loss: 0.0898
Epoch 22/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1262 - val_loss: 0.0878
Epoch 23/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1254 - val_loss: 0.0900
Epoch 24/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1248 - val_loss: 0.0901
Epoch 25/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1242 - val_loss: 0.0900
Epoch 26/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1236 - val_loss: 0.0877
Epoch 27/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1230 - val_loss: 0.0884
Epoch 28/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1226 - val_loss: 0.0867
Epoch 29/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1220 - val_loss: 0.0881
Epoch 30/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1217 - val_loss: 0.0889
Epoch 31/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1214 - val_loss: 0.0907
Epoch 32/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1206 - val_loss: 0.0882
Epoch 33/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1202 - val_loss: 0.0878
Epoch 34/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1200 - val_loss: 0.0863
Epoch 35/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1197 - val_loss: 0.0870
Epoch 36/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1192 - val_loss: 0.0860
Epoch 37/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1189 - val_loss: 0.0867
Epoch 38/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1187 - val_loss: 0.0858
Epoch 39/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1186 - val_loss: 0.0865
Epoch 40/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1180 - val_loss: 0.0870
Epoch 41/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1177 - val_loss: 0.0874
Epoch 42/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1175 - val_loss: 0.0868
Epoch 43/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1171 - val_loss: 0.0850
Epoch 44/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1167 - val_loss: 0.0861
Epoch 45/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1165 - val_loss: 0.0854
Epoch 46/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1162 - val_loss: 0.0837
Epoch 47/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1157 - val_loss: 0.0854
Epoch 48/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1158 - val_loss: 0.0834
Epoch 49/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1149 - val_loss: 0.0866
Epoch 50/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1149 - val_loss: 0.0851
Epoch 51/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1144 - val_loss: 0.0828
Epoch 52/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1143 - val_loss: 0.0841
Epoch 53/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1139 - val_loss: 0.0853
Epoch 54/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1137 - val_loss: 0.0826
Epoch 55/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1133 - val_loss: 0.0830
Epoch 56/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1131 - val_loss: 0.0834
Epoch 57/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1129 - val_loss: 0.0832
Epoch 58/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1124 - val_loss: 0.0834
Epoch 59/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1122 - val_loss: 0.0826
Epoch 60/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1121 - val_loss: 0.0840
Epoch 61/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1117 - val_loss: 0.0818
Epoch 62/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1116 - val_loss: 0.0824
Epoch 63/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1113 - val_loss: 0.0828
Epoch 64/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1111 - val_loss: 0.0819
Epoch 65/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1109 - val_loss: 0.0822
Epoch 66/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1105 - val_loss: 0.0822
Epoch 67/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1104 - val_loss: 0.0821
Epoch 68/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1102 - val_loss: 0.0816
Epoch 69/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1099 - val_loss: 0.0819
Epoch 70/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1097 - val_loss: 0.0828
Epoch 71/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1096 - val_loss: 0.0814
Epoch 72/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1095 - val_loss: 0.0817
Epoch 73/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1093 - val_loss: 0.0812
Epoch 74/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1090 - val_loss: 0.0814
Epoch 75/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1087 - val_loss: 0.0818
Epoch 76/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1086 - val_loss: 0.0810
Epoch 77/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1085 - val_loss: 0.0806
Epoch 78/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1084 - val_loss: 0.0813
Epoch 79/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1080 - val_loss: 0.0821
Epoch 80/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1079 - val_loss: 0.0809
Epoch 81/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1079 - val_loss: 0.0810
Epoch 82/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1074 - val_loss: 0.0833
Epoch 83/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1076 - val_loss: 0.0811
Epoch 84/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1074 - val_loss: 0.0807
Epoch 85/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1071 - val_loss: 0.0808
Epoch 86/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1071 - val_loss: 0.0808
Epoch 87/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1068 - val_loss: 0.0799
Epoch 88/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1068 - val_loss: 0.0806
Epoch 89/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1065 - val_loss: 0.0808
Epoch 90/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1063 - val_loss: 0.0805
Epoch 91/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1061 - val_loss: 0.0790
Epoch 92/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1062 - val_loss: 0.0803
Epoch 93/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1059 - val_loss: 0.0803
Epoch 94/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1058 - val_loss: 0.0809
Epoch 95/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1055 - val_loss: 0.0807
Epoch 96/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1055 - val_loss: 0.0802
Epoch 97/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1054 - val_loss: 0.0793
Epoch 98/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1051 - val_loss: 0.0806
Epoch 99/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1050 - val_loss: 0.0811
Epoch 100/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1050 - val_loss: 0.0815
Epoch 101/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1050 - val_loss: 0.0788
Epoch 102/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1045 - val_loss: 0.0804
Epoch 103/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1046 - val_loss: 0.0809
Epoch 104/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1045 - val_loss: 0.0804
Epoch 105/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1042 - val_loss: 0.0791
Epoch 106/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1041 - val_loss: 0.0795
Epoch 107/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1039 - val_loss: 0.0795
Epoch 108/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1038 - val_loss: 0.0804
Epoch 109/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1037 - val_loss: 0.0798
Epoch 110/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1038 - val_loss: 0.0797
Epoch 111/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1036 - val_loss: 0.0806
Val_yp Shape is 
(75734, 24)
Results === Test == Validation ===== 40
=============================
step 1    -  0.993    0.992
step 2    -  0.986    0.984
step 3    -  0.977    0.975
step 4    -  0.968    0.966
step 5    -  0.958    0.956
step 6    -  0.948    0.947
step 7    -  0.939    0.938
step 8    -  0.930    0.930
step 9    -  0.922    0.922
step 10   -  0.914    0.915
step 11   -  0.906    0.908
step 12   -  0.900    0.902
step 13   -  0.894    0.897
step 14   -  0.888    0.891
step 15   -  0.882    0.886
step 16   -  0.876    0.880
step 17   -  0.870    0.875
step 18   -  0.865    0.870
step 19   -  0.861    0.867
step 20   -  0.856    0.863
step 21   -  0.851    0.859
step 22   -  0.845    0.854
step 23   -  0.840    0.849
step 24   -  0.833    0.843
=============================
Summary   -  21.702    21.767
V_y Shape is 
(75734, 1)
=========== SHAPE INPUT-OUTPUT MATRIXES =====================
shape x_training : (227233, 287)  y_training: :  (227233, 24)
shape x_test     :  (75733, 287)  y_test      :  (75733, 24)
shape x_val      :  (75734, 287)  y_val       :  (75734, 24)
=============================================================
(287,)
Model: "MLP"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_15 (InputLayer)       [(None, 287)]             0         
                                                                 
 dense_56 (Dense)            (None, 1024)              294912    
                                                                 
 elu_14 (ELU)                (None, 1024)              0         
                                                                 
 dropout_42 (Dropout)        (None, 1024)              0         
                                                                 
 dense_57 (Dense)            (None, 512)               524800    
                                                                 
 dropout_43 (Dropout)        (None, 512)               0         
                                                                 
 dense_58 (Dense)            (None, 512)               262656    
                                                                 
 dropout_44 (Dropout)        (None, 512)               0         
                                                                 
 dense_59 (Dense)            (None, 24)                12312     
                                                                 
=================================================================
Total params: 1,094,680
Trainable params: 1,094,680
Non-trainable params: 0
_________________________________________________________________
Epoch 1/200
222/222 [==============================] - 2s 8ms/step - loss: 0.4067 - val_loss: 0.1099
Epoch 2/200
222/222 [==============================] - 1s 6ms/step - loss: 0.2220 - val_loss: 0.1006
Epoch 3/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1901 - val_loss: 0.0987
Epoch 4/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1751 - val_loss: 0.0957
Epoch 5/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1659 - val_loss: 0.0954
Epoch 6/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1588 - val_loss: 0.0953
Epoch 7/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1536 - val_loss: 0.0965
Epoch 8/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1497 - val_loss: 0.0936
Epoch 9/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1459 - val_loss: 0.0913
Epoch 10/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1433 - val_loss: 0.0908
Epoch 11/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1409 - val_loss: 0.0942
Epoch 12/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1387 - val_loss: 0.0927
Epoch 13/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1368 - val_loss: 0.0918
Epoch 14/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1353 - val_loss: 0.0919
Epoch 15/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1335 - val_loss: 0.0909
Epoch 16/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1321 - val_loss: 0.0922
Epoch 17/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1310 - val_loss: 0.0888
Epoch 18/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1299 - val_loss: 0.0882
Epoch 19/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1286 - val_loss: 0.0892
Epoch 20/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1277 - val_loss: 0.0885
Epoch 21/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1268 - val_loss: 0.0894
Epoch 22/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1261 - val_loss: 0.0891
Epoch 23/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1251 - val_loss: 0.0892
Epoch 24/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1245 - val_loss: 0.0887
Epoch 25/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1239 - val_loss: 0.0892
Epoch 26/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1236 - val_loss: 0.0870
Epoch 27/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1228 - val_loss: 0.0878
Epoch 28/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1222 - val_loss: 0.0903
Epoch 29/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1216 - val_loss: 0.0879
Epoch 30/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1212 - val_loss: 0.0863
Epoch 31/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1209 - val_loss: 0.0875
Epoch 32/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1206 - val_loss: 0.0901
Epoch 33/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1203 - val_loss: 0.0876
Epoch 34/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1198 - val_loss: 0.0878
Epoch 35/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1196 - val_loss: 0.0865
Epoch 36/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1191 - val_loss: 0.0880
Epoch 37/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1187 - val_loss: 0.0859
Epoch 38/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1184 - val_loss: 0.0874
Epoch 39/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1183 - val_loss: 0.0865
Epoch 40/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1179 - val_loss: 0.0861
Epoch 41/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1175 - val_loss: 0.0893
Epoch 42/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1175 - val_loss: 0.0869
Epoch 43/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1170 - val_loss: 0.0851
Epoch 44/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1166 - val_loss: 0.0853
Epoch 45/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1164 - val_loss: 0.0865
Epoch 46/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1159 - val_loss: 0.0849
Epoch 47/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1156 - val_loss: 0.0848
Epoch 48/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1154 - val_loss: 0.0850
Epoch 49/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1149 - val_loss: 0.0844
Epoch 50/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1148 - val_loss: 0.0837
Epoch 51/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1144 - val_loss: 0.0860
Epoch 52/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1140 - val_loss: 0.0842
Epoch 53/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1139 - val_loss: 0.0845
Epoch 54/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1137 - val_loss: 0.0862
Epoch 55/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1132 - val_loss: 0.0834
Epoch 56/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1130 - val_loss: 0.0848
Epoch 57/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1126 - val_loss: 0.0838
Epoch 58/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1125 - val_loss: 0.0841
Epoch 59/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1120 - val_loss: 0.0834
Epoch 60/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1118 - val_loss: 0.0836
Epoch 61/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1115 - val_loss: 0.0819
Epoch 62/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1114 - val_loss: 0.0823
Epoch 63/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1111 - val_loss: 0.0820
Epoch 64/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1110 - val_loss: 0.0823
Epoch 65/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1106 - val_loss: 0.0825
Epoch 66/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1106 - val_loss: 0.0811
Epoch 67/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1103 - val_loss: 0.0811
Epoch 68/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1100 - val_loss: 0.0824
Epoch 69/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1097 - val_loss: 0.0828
Epoch 70/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1097 - val_loss: 0.0820
Epoch 71/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1093 - val_loss: 0.0828
Epoch 72/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1092 - val_loss: 0.0810
Epoch 73/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1090 - val_loss: 0.0815
Epoch 74/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1088 - val_loss: 0.0817
Epoch 75/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1088 - val_loss: 0.0814
Epoch 76/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1086 - val_loss: 0.0798
Epoch 77/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1082 - val_loss: 0.0809
Epoch 78/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1081 - val_loss: 0.0820
Epoch 79/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1078 - val_loss: 0.0810
Epoch 80/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1077 - val_loss: 0.0809
Epoch 81/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1075 - val_loss: 0.0805
Epoch 82/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1072 - val_loss: 0.0812
Epoch 83/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1074 - val_loss: 0.0816
Epoch 84/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1068 - val_loss: 0.0812
Epoch 85/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1070 - val_loss: 0.0822
Epoch 86/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1067 - val_loss: 0.0805
Val_yp Shape is 
(75734, 24)
Results === Test == Validation ===== 41
=============================
step 1    -  0.992    0.991
step 2    -  0.985    0.983
step 3    -  0.976    0.974
step 4    -  0.967    0.964
step 5    -  0.956    0.955
step 6    -  0.946    0.945
step 7    -  0.936    0.936
step 8    -  0.927    0.927
step 9    -  0.919    0.920
step 10   -  0.909    0.911
step 11   -  0.901    0.903
step 12   -  0.894    0.896
step 13   -  0.888    0.891
step 14   -  0.882    0.885
step 15   -  0.876    0.880
step 16   -  0.870    0.874
step 17   -  0.865    0.870
step 18   -  0.859    0.865
step 19   -  0.853    0.860
step 20   -  0.846    0.854
step 21   -  0.841    0.849
step 22   -  0.835    0.844
step 23   -  0.831    0.840
step 24   -  0.826    0.836
=============================
Summary   -  21.581    21.656
V_y Shape is 
(75734, 1)
=========== SHAPE INPUT-OUTPUT MATRIXES =====================
shape x_training : (227232, 294)  y_training: :  (227232, 24)
shape x_test     :  (75733, 294)  y_test      :  (75733, 24)
shape x_val      :  (75733, 294)  y_val       :  (75733, 24)
=============================================================
(294,)
Model: "MLP"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_16 (InputLayer)       [(None, 294)]             0         
                                                                 
 dense_60 (Dense)            (None, 1024)              302080    
                                                                 
 elu_15 (ELU)                (None, 1024)              0         
                                                                 
 dropout_45 (Dropout)        (None, 1024)              0         
                                                                 
 dense_61 (Dense)            (None, 512)               524800    
                                                                 
 dropout_46 (Dropout)        (None, 512)               0         
                                                                 
 dense_62 (Dense)            (None, 512)               262656    
                                                                 
 dropout_47 (Dropout)        (None, 512)               0         
                                                                 
 dense_63 (Dense)            (None, 24)                12312     
                                                                 
=================================================================
Total params: 1,101,848
Trainable params: 1,101,848
Non-trainable params: 0
_________________________________________________________________
Epoch 1/200
222/222 [==============================] - 2s 8ms/step - loss: 0.4123 - val_loss: 0.1081
Epoch 2/200
222/222 [==============================] - 1s 6ms/step - loss: 0.2234 - val_loss: 0.1010
Epoch 3/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1907 - val_loss: 0.1014
Epoch 4/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1753 - val_loss: 0.0989
Epoch 5/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1657 - val_loss: 0.0947
Epoch 6/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1585 - val_loss: 0.0948
Epoch 7/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1537 - val_loss: 0.0926
Epoch 8/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1495 - val_loss: 0.0933
Epoch 9/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1462 - val_loss: 0.0914
Epoch 10/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1433 - val_loss: 0.0923
Epoch 11/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1408 - val_loss: 0.0929
Epoch 12/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1387 - val_loss: 0.0921
Epoch 13/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1365 - val_loss: 0.0904
Epoch 14/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1350 - val_loss: 0.0912
Epoch 15/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1334 - val_loss: 0.0912
Epoch 16/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1320 - val_loss: 0.0903
Epoch 17/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1308 - val_loss: 0.0906
Epoch 18/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1296 - val_loss: 0.0916
Epoch 19/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1285 - val_loss: 0.0901
Epoch 20/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1274 - val_loss: 0.0881
Epoch 21/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1267 - val_loss: 0.0909
Epoch 22/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1259 - val_loss: 0.0900
Epoch 23/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1252 - val_loss: 0.0889
Epoch 24/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1246 - val_loss: 0.0885
Epoch 25/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1236 - val_loss: 0.0880
Epoch 26/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1234 - val_loss: 0.0898
Epoch 27/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1226 - val_loss: 0.0875
Epoch 28/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1219 - val_loss: 0.0896
Epoch 29/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1218 - val_loss: 0.0885
Epoch 30/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1212 - val_loss: 0.0885
Epoch 31/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1207 - val_loss: 0.0881
Epoch 32/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1203 - val_loss: 0.0880
Epoch 33/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1198 - val_loss: 0.0865
Epoch 34/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1196 - val_loss: 0.0868
Epoch 35/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1192 - val_loss: 0.0872
Epoch 36/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1189 - val_loss: 0.0867
Epoch 37/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1186 - val_loss: 0.0871
Epoch 38/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1183 - val_loss: 0.0860
Epoch 39/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1179 - val_loss: 0.0866
Epoch 40/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1176 - val_loss: 0.0872
Epoch 41/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1173 - val_loss: 0.0873
Epoch 42/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1169 - val_loss: 0.0862
Epoch 43/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1165 - val_loss: 0.0857
Epoch 44/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1162 - val_loss: 0.0854
Epoch 45/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1158 - val_loss: 0.0843
Epoch 46/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1157 - val_loss: 0.0854
Epoch 47/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1153 - val_loss: 0.0841
Epoch 48/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1150 - val_loss: 0.0850
Epoch 49/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1146 - val_loss: 0.0838
Epoch 50/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1143 - val_loss: 0.0844
Epoch 51/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1141 - val_loss: 0.0834
Epoch 52/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1139 - val_loss: 0.0837
Epoch 53/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1135 - val_loss: 0.0835
Epoch 54/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1130 - val_loss: 0.0836
Epoch 55/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1130 - val_loss: 0.0843
Epoch 56/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1127 - val_loss: 0.0837
Epoch 57/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1123 - val_loss: 0.0832
Epoch 58/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1121 - val_loss: 0.0832
Epoch 59/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1117 - val_loss: 0.0829
Epoch 60/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1116 - val_loss: 0.0838
Epoch 61/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1112 - val_loss: 0.0811
Epoch 62/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1109 - val_loss: 0.0820
Epoch 63/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1109 - val_loss: 0.0816
Epoch 64/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1105 - val_loss: 0.0816
Epoch 65/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1104 - val_loss: 0.0820
Epoch 66/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1102 - val_loss: 0.0819
Epoch 67/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1098 - val_loss: 0.0833
Epoch 68/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1094 - val_loss: 0.0822
Epoch 69/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1095 - val_loss: 0.0817
Epoch 70/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1093 - val_loss: 0.0814
Epoch 71/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1092 - val_loss: 0.0828
Val_yp Shape is 
(75733, 24)
Results === Test == Validation ===== 42
=============================
step 1    -  0.992    0.991
step 2    -  0.985    0.983
step 3    -  0.976    0.974
step 4    -  0.966    0.964
step 5    -  0.956    0.955
step 6    -  0.946    0.945
step 7    -  0.936    0.936
step 8    -  0.928    0.928
step 9    -  0.920    0.921
step 10   -  0.912    0.914
step 11   -  0.905    0.907
step 12   -  0.899    0.902
step 13   -  0.892    0.895
step 14   -  0.887    0.890
step 15   -  0.880    0.884
step 16   -  0.874    0.879
step 17   -  0.869    0.874
step 18   -  0.864    0.870
step 19   -  0.857    0.865
step 20   -  0.852    0.860
step 21   -  0.845    0.854
step 22   -  0.840    0.849
step 23   -  0.835    0.844
step 24   -  0.829    0.840
=============================
Summary   -  21.645    21.727
V_y Shape is 
(75733, 1)
=========== SHAPE INPUT-OUTPUT MATRIXES =====================
shape x_training : (227231, 301)  y_training: :  (227231, 24)
shape x_test     :  (75732, 301)  y_test      :  (75732, 24)
shape x_val      :  (75733, 301)  y_val       :  (75733, 24)
=============================================================
(301,)
Model: "MLP"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_17 (InputLayer)       [(None, 301)]             0         
                                                                 
 dense_64 (Dense)            (None, 1024)              309248    
                                                                 
 elu_16 (ELU)                (None, 1024)              0         
                                                                 
 dropout_48 (Dropout)        (None, 1024)              0         
                                                                 
 dense_65 (Dense)            (None, 512)               524800    
                                                                 
 dropout_49 (Dropout)        (None, 512)               0         
                                                                 
 dense_66 (Dense)            (None, 512)               262656    
                                                                 
 dropout_50 (Dropout)        (None, 512)               0         
                                                                 
 dense_67 (Dense)            (None, 24)                12312     
                                                                 
=================================================================
Total params: 1,109,016
Trainable params: 1,109,016
Non-trainable params: 0
_________________________________________________________________
Epoch 1/200
222/222 [==============================] - 2s 7ms/step - loss: 0.4126 - val_loss: 0.1118
Epoch 2/200
222/222 [==============================] - 1s 6ms/step - loss: 0.2235 - val_loss: 0.1015
Epoch 3/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1916 - val_loss: 0.0999
Epoch 4/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1758 - val_loss: 0.0964
Epoch 5/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1662 - val_loss: 0.0973
Epoch 6/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1588 - val_loss: 0.0946
Epoch 7/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1537 - val_loss: 0.0938
Epoch 8/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1496 - val_loss: 0.0921
Epoch 9/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1461 - val_loss: 0.0937
Epoch 10/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1433 - val_loss: 0.0921
Epoch 11/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1407 - val_loss: 0.0919
Epoch 12/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1383 - val_loss: 0.0942
Epoch 13/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1365 - val_loss: 0.0906
Epoch 14/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1348 - val_loss: 0.0908
Epoch 15/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1334 - val_loss: 0.0900
Epoch 16/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1316 - val_loss: 0.0914
Epoch 17/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1306 - val_loss: 0.0916
Epoch 18/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1291 - val_loss: 0.0905
Epoch 19/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1282 - val_loss: 0.0914
Epoch 20/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1272 - val_loss: 0.0910
Epoch 21/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1266 - val_loss: 0.0896
Epoch 22/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1254 - val_loss: 0.0911
Epoch 23/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1249 - val_loss: 0.0899
Epoch 24/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1242 - val_loss: 0.0890
Epoch 25/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1237 - val_loss: 0.0894
Epoch 26/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1231 - val_loss: 0.0890
Epoch 27/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1224 - val_loss: 0.0900
Epoch 28/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1218 - val_loss: 0.0872
Epoch 29/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1213 - val_loss: 0.0867
Epoch 30/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1208 - val_loss: 0.0880
Epoch 31/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1207 - val_loss: 0.0879
Epoch 32/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1201 - val_loss: 0.0876
Epoch 33/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1196 - val_loss: 0.0872
Epoch 34/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1194 - val_loss: 0.0885
Epoch 35/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1191 - val_loss: 0.0890
Epoch 36/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1185 - val_loss: 0.0870
Epoch 37/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1183 - val_loss: 0.0857
Epoch 38/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1179 - val_loss: 0.0865
Epoch 39/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1177 - val_loss: 0.0864
Epoch 40/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1173 - val_loss: 0.0863
Epoch 41/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1172 - val_loss: 0.0866
Epoch 42/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1166 - val_loss: 0.0866
Epoch 43/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1165 - val_loss: 0.0857
Epoch 44/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1159 - val_loss: 0.0855
Epoch 45/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1158 - val_loss: 0.0849
Epoch 46/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1154 - val_loss: 0.0866
Epoch 47/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1150 - val_loss: 0.0855
Epoch 48/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1146 - val_loss: 0.0845
Epoch 49/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1144 - val_loss: 0.0852
Epoch 50/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1142 - val_loss: 0.0835
Epoch 51/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1138 - val_loss: 0.0845
Epoch 52/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1135 - val_loss: 0.0851
Epoch 53/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1133 - val_loss: 0.0830
Epoch 54/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1130 - val_loss: 0.0844
Epoch 55/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1127 - val_loss: 0.0834
Epoch 56/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1124 - val_loss: 0.0826
Epoch 57/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1119 - val_loss: 0.0826
Epoch 58/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1119 - val_loss: 0.0831
Epoch 59/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1114 - val_loss: 0.0836
Epoch 60/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1113 - val_loss: 0.0820
Epoch 61/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1112 - val_loss: 0.0828
Epoch 62/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1109 - val_loss: 0.0828
Epoch 63/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1108 - val_loss: 0.0835
Epoch 64/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1101 - val_loss: 0.0821
Epoch 65/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1101 - val_loss: 0.0820
Epoch 66/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1097 - val_loss: 0.0824
Epoch 67/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1096 - val_loss: 0.0817
Epoch 68/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1094 - val_loss: 0.0832
Epoch 69/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1091 - val_loss: 0.0817
Epoch 70/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1089 - val_loss: 0.0816
Epoch 71/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1087 - val_loss: 0.0806
Epoch 72/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1083 - val_loss: 0.0827
Epoch 73/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1083 - val_loss: 0.0813
Epoch 74/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1081 - val_loss: 0.0820
Epoch 75/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1080 - val_loss: 0.0821
Epoch 76/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1078 - val_loss: 0.0809
Epoch 77/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1076 - val_loss: 0.0813
Epoch 78/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1073 - val_loss: 0.0807
Epoch 79/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1071 - val_loss: 0.0798
Epoch 80/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1070 - val_loss: 0.0816
Epoch 81/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1068 - val_loss: 0.0800
Epoch 82/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1064 - val_loss: 0.0811
Epoch 83/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1065 - val_loss: 0.0807
Epoch 84/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1065 - val_loss: 0.0808
Epoch 85/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1063 - val_loss: 0.0812
Epoch 86/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1059 - val_loss: 0.0804
Epoch 87/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1058 - val_loss: 0.0813
Epoch 88/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1058 - val_loss: 0.0804
Epoch 89/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1055 - val_loss: 0.0817
Val_yp Shape is 
(75733, 24)
Results === Test == Validation ===== 43
=============================
step 1    -  0.992    0.991
step 2    -  0.985    0.984
step 3    -  0.977    0.975
step 4    -  0.967    0.965
step 5    -  0.958    0.956
step 6    -  0.948    0.947
step 7    -  0.939    0.938
step 8    -  0.929    0.929
step 9    -  0.921    0.921
step 10   -  0.913    0.914
step 11   -  0.905    0.907
step 12   -  0.898    0.900
step 13   -  0.891    0.894
step 14   -  0.885    0.888
step 15   -  0.878    0.882
step 16   -  0.871    0.876
step 17   -  0.865    0.870
step 18   -  0.859    0.865
step 19   -  0.854    0.860
step 20   -  0.849    0.856
step 21   -  0.844    0.852
step 22   -  0.841    0.849
step 23   -  0.838    0.846
step 24   -  0.832    0.841
=============================
Summary   -  21.641    21.706
V_y Shape is 
(75733, 1)
=========== SHAPE INPUT-OUTPUT MATRIXES =====================
shape x_training : (227230, 308)  y_training: :  (227230, 24)
shape x_test     :  (75732, 308)  y_test      :  (75732, 24)
shape x_val      :  (75732, 308)  y_val       :  (75732, 24)
=============================================================
(308,)
Model: "MLP"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_18 (InputLayer)       [(None, 308)]             0         
                                                                 
 dense_68 (Dense)            (None, 1024)              316416    
                                                                 
 elu_17 (ELU)                (None, 1024)              0         
                                                                 
 dropout_51 (Dropout)        (None, 1024)              0         
                                                                 
 dense_69 (Dense)            (None, 512)               524800    
                                                                 
 dropout_52 (Dropout)        (None, 512)               0         
                                                                 
 dense_70 (Dense)            (None, 512)               262656    
                                                                 
 dropout_53 (Dropout)        (None, 512)               0         
                                                                 
 dense_71 (Dense)            (None, 24)                12312     
                                                                 
=================================================================
Total params: 1,116,184
Trainable params: 1,116,184
Non-trainable params: 0
_________________________________________________________________
Epoch 1/200
222/222 [==============================] - 2s 8ms/step - loss: 0.4122 - val_loss: 0.1108
Epoch 2/200
222/222 [==============================] - 1s 6ms/step - loss: 0.2246 - val_loss: 0.1026
Epoch 3/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1921 - val_loss: 0.0988
Epoch 4/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1760 - val_loss: 0.0985
Epoch 5/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1662 - val_loss: 0.0961
Epoch 6/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1591 - val_loss: 0.0932
Epoch 7/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1539 - val_loss: 0.0966
Epoch 8/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1497 - val_loss: 0.0947
Epoch 9/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1463 - val_loss: 0.0924
Epoch 10/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1432 - val_loss: 0.0925
Epoch 11/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1406 - val_loss: 0.0922
Epoch 12/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1383 - val_loss: 0.0914
Epoch 13/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1364 - val_loss: 0.0910
Epoch 14/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1348 - val_loss: 0.0900
Epoch 15/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1330 - val_loss: 0.0929
Epoch 16/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1316 - val_loss: 0.0900
Epoch 17/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1304 - val_loss: 0.0911
Epoch 18/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1292 - val_loss: 0.0902
Epoch 19/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1282 - val_loss: 0.0907
Epoch 20/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1271 - val_loss: 0.0891
Epoch 21/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1262 - val_loss: 0.0889
Epoch 22/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1253 - val_loss: 0.0910
Epoch 23/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1244 - val_loss: 0.0885
Epoch 24/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1240 - val_loss: 0.0894
Epoch 25/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1233 - val_loss: 0.0887
Epoch 26/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1226 - val_loss: 0.0884
Epoch 27/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1223 - val_loss: 0.0882
Epoch 28/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1217 - val_loss: 0.0885
Epoch 29/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1212 - val_loss: 0.0873
Epoch 30/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1207 - val_loss: 0.0876
Epoch 31/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1204 - val_loss: 0.0890
Epoch 32/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1199 - val_loss: 0.0870
Epoch 33/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1192 - val_loss: 0.0869
Epoch 34/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1188 - val_loss: 0.0872
Epoch 35/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1185 - val_loss: 0.0863
Epoch 36/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1182 - val_loss: 0.0865
Epoch 37/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1178 - val_loss: 0.0856
Epoch 38/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1177 - val_loss: 0.0868
Epoch 39/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1175 - val_loss: 0.0857
Epoch 40/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1169 - val_loss: 0.0872
Epoch 41/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1169 - val_loss: 0.0865
Epoch 42/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1162 - val_loss: 0.0865
Epoch 43/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1159 - val_loss: 0.0866
Epoch 44/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1156 - val_loss: 0.0860
Epoch 45/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1152 - val_loss: 0.0848
Epoch 46/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1149 - val_loss: 0.0852
Epoch 47/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1148 - val_loss: 0.0826
Epoch 48/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1143 - val_loss: 0.0845
Epoch 49/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1140 - val_loss: 0.0853
Epoch 50/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1136 - val_loss: 0.0837
Epoch 51/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1130 - val_loss: 0.0848
Epoch 52/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1131 - val_loss: 0.0840
Epoch 53/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1127 - val_loss: 0.0830
Epoch 54/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1122 - val_loss: 0.0822
Epoch 55/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1120 - val_loss: 0.0829
Epoch 56/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1119 - val_loss: 0.0834
Epoch 57/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1116 - val_loss: 0.0835
Epoch 58/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1110 - val_loss: 0.0832
Epoch 59/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1109 - val_loss: 0.0824
Epoch 60/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1106 - val_loss: 0.0822
Epoch 61/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1104 - val_loss: 0.0826
Epoch 62/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1103 - val_loss: 0.0835
Epoch 63/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1098 - val_loss: 0.0817
Epoch 64/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1097 - val_loss: 0.0824
Epoch 65/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1096 - val_loss: 0.0815
Epoch 66/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1093 - val_loss: 0.0819
Epoch 67/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1090 - val_loss: 0.0829
Epoch 68/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1089 - val_loss: 0.0814
Epoch 69/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1086 - val_loss: 0.0826
Epoch 70/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1084 - val_loss: 0.0824
Epoch 71/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1083 - val_loss: 0.0819
Epoch 72/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1080 - val_loss: 0.0804
Epoch 73/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1077 - val_loss: 0.0832
Epoch 74/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1076 - val_loss: 0.0815
Epoch 75/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1073 - val_loss: 0.0808
Epoch 76/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1072 - val_loss: 0.0820
Epoch 77/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1070 - val_loss: 0.0818
Epoch 78/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1068 - val_loss: 0.0831
Epoch 79/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1067 - val_loss: 0.0794
Epoch 80/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1065 - val_loss: 0.0809
Epoch 81/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1064 - val_loss: 0.0794
Epoch 82/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1061 - val_loss: 0.0813
Epoch 83/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1061 - val_loss: 0.0820
Epoch 84/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1056 - val_loss: 0.0807
Epoch 85/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1055 - val_loss: 0.0811
Epoch 86/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1054 - val_loss: 0.0813
Epoch 87/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1053 - val_loss: 0.0803
Epoch 88/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1051 - val_loss: 0.0807
Epoch 89/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1049 - val_loss: 0.0814
Val_yp Shape is 
(75732, 24)
Results === Test == Validation ===== 44
=============================
step 1    -  0.991    0.989
step 2    -  0.983    0.981
step 3    -  0.973    0.971
step 4    -  0.964    0.961
step 5    -  0.954    0.952
step 6    -  0.944    0.942
step 7    -  0.936    0.935
step 8    -  0.927    0.926
step 9    -  0.919    0.919
step 10   -  0.911    0.912
step 11   -  0.905    0.906
step 12   -  0.898    0.899
step 13   -  0.892    0.893
step 14   -  0.885    0.888
step 15   -  0.879    0.882
step 16   -  0.872    0.876
step 17   -  0.866    0.871
step 18   -  0.860    0.865
step 19   -  0.855    0.860
step 20   -  0.849    0.855
step 21   -  0.843    0.850
step 22   -  0.837    0.845
step 23   -  0.831    0.839
step 24   -  0.827    0.835
=============================
Summary   -  21.600    21.651
V_y Shape is 
(75732, 1)
=========== SHAPE INPUT-OUTPUT MATRIXES =====================
shape x_training : (227229, 315)  y_training: :  (227229, 24)
shape x_test     :  (75731, 315)  y_test      :  (75731, 24)
shape x_val      :  (75732, 315)  y_val       :  (75732, 24)
=============================================================
(315,)
Model: "MLP"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_19 (InputLayer)       [(None, 315)]             0         
                                                                 
 dense_72 (Dense)            (None, 1024)              323584    
                                                                 
 elu_18 (ELU)                (None, 1024)              0         
                                                                 
 dropout_54 (Dropout)        (None, 1024)              0         
                                                                 
 dense_73 (Dense)            (None, 512)               524800    
                                                                 
 dropout_55 (Dropout)        (None, 512)               0         
                                                                 
 dense_74 (Dense)            (None, 512)               262656    
                                                                 
 dropout_56 (Dropout)        (None, 512)               0         
                                                                 
 dense_75 (Dense)            (None, 24)                12312     
                                                                 
=================================================================
Total params: 1,123,352
Trainable params: 1,123,352
Non-trainable params: 0
_________________________________________________________________
Epoch 1/200
222/222 [==============================] - 2s 7ms/step - loss: 0.4248 - val_loss: 0.1107
Epoch 2/200
222/222 [==============================] - 1s 6ms/step - loss: 0.2265 - val_loss: 0.1014
Epoch 3/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1928 - val_loss: 0.0989
Epoch 4/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1770 - val_loss: 0.0981
Epoch 5/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1665 - val_loss: 0.0941
Epoch 6/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1595 - val_loss: 0.0954
Epoch 7/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1543 - val_loss: 0.0945
Epoch 8/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1499 - val_loss: 0.0941
Epoch 9/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1463 - val_loss: 0.0955
Epoch 10/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1433 - val_loss: 0.0911
Epoch 11/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1406 - val_loss: 0.0928
Epoch 12/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1384 - val_loss: 0.0915
Epoch 13/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1364 - val_loss: 0.0926
Epoch 14/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1347 - val_loss: 0.0905
Epoch 15/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1332 - val_loss: 0.0900
Epoch 16/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1318 - val_loss: 0.0902
Epoch 17/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1303 - val_loss: 0.0892
Epoch 18/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1291 - val_loss: 0.0903
Epoch 19/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1282 - val_loss: 0.0907
Epoch 20/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1272 - val_loss: 0.0904
Epoch 21/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1263 - val_loss: 0.0893
Epoch 22/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1254 - val_loss: 0.0891
Epoch 23/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1247 - val_loss: 0.0879
Epoch 24/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1241 - val_loss: 0.0883
Epoch 25/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1235 - val_loss: 0.0890
Epoch 26/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1226 - val_loss: 0.0880
Epoch 27/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1222 - val_loss: 0.0894
Epoch 28/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1217 - val_loss: 0.0865
Epoch 29/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1211 - val_loss: 0.0863
Epoch 30/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1206 - val_loss: 0.0877
Epoch 31/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1200 - val_loss: 0.0880
Epoch 32/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1198 - val_loss: 0.0866
Epoch 33/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1194 - val_loss: 0.0883
Epoch 34/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1188 - val_loss: 0.0860
Epoch 35/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1185 - val_loss: 0.0885
Epoch 36/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1184 - val_loss: 0.0873
Epoch 37/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1177 - val_loss: 0.0857
Epoch 38/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1174 - val_loss: 0.0863
Epoch 39/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1171 - val_loss: 0.0852
Epoch 40/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1168 - val_loss: 0.0853
Epoch 41/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1163 - val_loss: 0.0871
Epoch 42/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1159 - val_loss: 0.0862
Epoch 43/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1157 - val_loss: 0.0870
Epoch 44/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1154 - val_loss: 0.0853
Epoch 45/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1148 - val_loss: 0.0867
Epoch 46/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1145 - val_loss: 0.0841
Epoch 47/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1141 - val_loss: 0.0842
Epoch 48/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1141 - val_loss: 0.0837
Epoch 49/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1137 - val_loss: 0.0835
Epoch 50/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1136 - val_loss: 0.0839
Epoch 51/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1128 - val_loss: 0.0844
Epoch 52/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1128 - val_loss: 0.0846
Epoch 53/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1124 - val_loss: 0.0859
Epoch 54/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1122 - val_loss: 0.0832
Epoch 55/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1118 - val_loss: 0.0825
Epoch 56/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1115 - val_loss: 0.0840
Epoch 57/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1113 - val_loss: 0.0841
Epoch 58/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1109 - val_loss: 0.0820
Epoch 59/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1108 - val_loss: 0.0816
Epoch 60/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1104 - val_loss: 0.0816
Epoch 61/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1103 - val_loss: 0.0851
Epoch 62/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1101 - val_loss: 0.0822
Epoch 63/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1098 - val_loss: 0.0813
Epoch 64/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1095 - val_loss: 0.0815
Epoch 65/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1093 - val_loss: 0.0823
Epoch 66/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1090 - val_loss: 0.0813
Epoch 67/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1086 - val_loss: 0.0820
Epoch 68/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1086 - val_loss: 0.0808
Epoch 69/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1081 - val_loss: 0.0824
Epoch 70/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1081 - val_loss: 0.0811
Epoch 71/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1079 - val_loss: 0.0828
Epoch 72/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1077 - val_loss: 0.0823
Epoch 73/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1075 - val_loss: 0.0816
Epoch 74/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1071 - val_loss: 0.0826
Epoch 75/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1072 - val_loss: 0.0818
Epoch 76/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1070 - val_loss: 0.0814
Epoch 77/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1066 - val_loss: 0.0822
Epoch 78/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1065 - val_loss: 0.0811
Val_yp Shape is 
(75732, 24)
Results === Test == Validation ===== 45
=============================
step 1    -  0.992    0.991
step 2    -  0.985    0.983
step 3    -  0.976    0.974
step 4    -  0.968    0.965
step 5    -  0.958    0.956
step 6    -  0.949    0.947
step 7    -  0.940    0.939
step 8    -  0.931    0.931
step 9    -  0.922    0.923
step 10   -  0.914    0.916
step 11   -  0.906    0.909
step 12   -  0.899    0.902
step 13   -  0.892    0.896
step 14   -  0.886    0.890
step 15   -  0.881    0.885
step 16   -  0.875    0.880
step 17   -  0.869    0.875
step 18   -  0.864    0.870
step 19   -  0.857    0.864
step 20   -  0.852    0.860
step 21   -  0.846    0.854
step 22   -  0.840    0.849
step 23   -  0.833    0.843
step 24   -  0.827    0.838
=============================
Summary   -  21.664    21.740
V_y Shape is 
(75732, 1)
=========== SHAPE INPUT-OUTPUT MATRIXES =====================
shape x_training : (227228, 322)  y_training: :  (227228, 24)
shape x_test     :  (75731, 322)  y_test      :  (75731, 24)
shape x_val      :  (75731, 322)  y_val       :  (75731, 24)
=============================================================
(322,)
Model: "MLP"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_20 (InputLayer)       [(None, 322)]             0         
                                                                 
 dense_76 (Dense)            (None, 1024)              330752    
                                                                 
 elu_19 (ELU)                (None, 1024)              0         
                                                                 
 dropout_57 (Dropout)        (None, 1024)              0         
                                                                 
 dense_77 (Dense)            (None, 512)               524800    
                                                                 
 dropout_58 (Dropout)        (None, 512)               0         
                                                                 
 dense_78 (Dense)            (None, 512)               262656    
                                                                 
 dropout_59 (Dropout)        (None, 512)               0         
                                                                 
 dense_79 (Dense)            (None, 24)                12312     
                                                                 
=================================================================
Total params: 1,130,520
Trainable params: 1,130,520
Non-trainable params: 0
_________________________________________________________________
Epoch 1/200
222/222 [==============================] - 2s 8ms/step - loss: 0.4220 - val_loss: 0.1119
Epoch 2/200
222/222 [==============================] - 1s 6ms/step - loss: 0.2267 - val_loss: 0.1018
Epoch 3/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1932 - val_loss: 0.0997
Epoch 4/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1766 - val_loss: 0.0966
Epoch 5/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1669 - val_loss: 0.0981
Epoch 6/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1600 - val_loss: 0.0955
Epoch 7/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1539 - val_loss: 0.0956
Epoch 8/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1498 - val_loss: 0.0962
Epoch 9/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1464 - val_loss: 0.0925
Epoch 10/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1432 - val_loss: 0.0916
Epoch 11/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1405 - val_loss: 0.0927
Epoch 12/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1384 - val_loss: 0.0949
Epoch 13/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1366 - val_loss: 0.0906
Epoch 14/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1346 - val_loss: 0.0911
Epoch 15/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1330 - val_loss: 0.0892
Epoch 16/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1316 - val_loss: 0.0912
Epoch 17/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1300 - val_loss: 0.0917
Epoch 18/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1291 - val_loss: 0.0896
Epoch 19/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1279 - val_loss: 0.0920
Epoch 20/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1269 - val_loss: 0.0883
Epoch 21/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1260 - val_loss: 0.0881
Epoch 22/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1251 - val_loss: 0.0894
Epoch 23/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1243 - val_loss: 0.0892
Epoch 24/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1236 - val_loss: 0.0889
Epoch 25/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1231 - val_loss: 0.0895
Epoch 26/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1225 - val_loss: 0.0870
Epoch 27/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1219 - val_loss: 0.0868
Epoch 28/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1213 - val_loss: 0.0877
Epoch 29/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1207 - val_loss: 0.0866
Epoch 30/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1204 - val_loss: 0.0870
Epoch 31/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1198 - val_loss: 0.0894
Epoch 32/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1196 - val_loss: 0.0886
Epoch 33/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1192 - val_loss: 0.0873
Epoch 34/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1186 - val_loss: 0.0870
Epoch 35/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1184 - val_loss: 0.0858
Epoch 36/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1181 - val_loss: 0.0868
Epoch 37/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1177 - val_loss: 0.0886
Epoch 38/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1173 - val_loss: 0.0856
Epoch 39/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1170 - val_loss: 0.0856
Epoch 40/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1169 - val_loss: 0.0884
Epoch 41/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1162 - val_loss: 0.0855
Epoch 42/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1160 - val_loss: 0.0880
Epoch 43/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1156 - val_loss: 0.0852
Epoch 44/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1151 - val_loss: 0.0873
Epoch 45/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1148 - val_loss: 0.0861
Epoch 46/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1146 - val_loss: 0.0865
Epoch 47/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1142 - val_loss: 0.0852
Epoch 48/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1140 - val_loss: 0.0827
Epoch 49/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1136 - val_loss: 0.0852
Epoch 50/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1134 - val_loss: 0.0836
Epoch 51/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1131 - val_loss: 0.0848
Epoch 52/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1126 - val_loss: 0.0837
Epoch 53/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1122 - val_loss: 0.0824
Epoch 54/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1118 - val_loss: 0.0827
Epoch 55/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1116 - val_loss: 0.0842
Epoch 56/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1112 - val_loss: 0.0822
Epoch 57/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1111 - val_loss: 0.0828
Epoch 58/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1108 - val_loss: 0.0840
Epoch 59/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1106 - val_loss: 0.0822
Epoch 60/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1102 - val_loss: 0.0827
Epoch 61/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1099 - val_loss: 0.0828
Epoch 62/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1099 - val_loss: 0.0830
Epoch 63/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1094 - val_loss: 0.0827
Epoch 64/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1092 - val_loss: 0.0836
Epoch 65/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1092 - val_loss: 0.0823
Epoch 66/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1087 - val_loss: 0.0839
Val_yp Shape is 
(75731, 24)
Results === Test == Validation ===== 46
=============================
step 1    -  0.991    0.990
step 2    -  0.984    0.982
step 3    -  0.975    0.973
step 4    -  0.965    0.963
step 5    -  0.956    0.954
step 6    -  0.946    0.945
step 7    -  0.936    0.935
step 8    -  0.927    0.926
step 9    -  0.916    0.917
step 10   -  0.908    0.909
step 11   -  0.899    0.901
step 12   -  0.892    0.894
step 13   -  0.884    0.887
step 14   -  0.876    0.879
step 15   -  0.870    0.873
step 16   -  0.863    0.867
step 17   -  0.858    0.862
step 18   -  0.854    0.859
step 19   -  0.849    0.854
step 20   -  0.842    0.848
step 21   -  0.837    0.844
step 22   -  0.833    0.840
step 23   -  0.827    0.836
step 24   -  0.823    0.832
=============================
Summary   -  21.511    21.572
V_y Shape is 
(75731, 1)
=========== SHAPE INPUT-OUTPUT MATRIXES =====================
shape x_training : (227227, 329)  y_training: :  (227227, 24)
shape x_test     :  (75730, 329)  y_test      :  (75730, 24)
shape x_val      :  (75731, 329)  y_val       :  (75731, 24)
=============================================================
(329,)
Model: "MLP"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_21 (InputLayer)       [(None, 329)]             0         
                                                                 
 dense_80 (Dense)            (None, 1024)              337920    
                                                                 
 elu_20 (ELU)                (None, 1024)              0         
                                                                 
 dropout_60 (Dropout)        (None, 1024)              0         
                                                                 
 dense_81 (Dense)            (None, 512)               524800    
                                                                 
 dropout_61 (Dropout)        (None, 512)               0         
                                                                 
 dense_82 (Dense)            (None, 512)               262656    
                                                                 
 dropout_62 (Dropout)        (None, 512)               0         
                                                                 
 dense_83 (Dense)            (None, 24)                12312     
                                                                 
=================================================================
Total params: 1,137,688
Trainable params: 1,137,688
Non-trainable params: 0
_________________________________________________________________
Epoch 1/200
222/222 [==============================] - 2s 8ms/step - loss: 0.4158 - val_loss: 0.1109
Epoch 2/200
222/222 [==============================] - 1s 6ms/step - loss: 0.2258 - val_loss: 0.1012
Epoch 3/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1927 - val_loss: 0.1001
Epoch 4/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1765 - val_loss: 0.0959
Epoch 5/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1668 - val_loss: 0.0963
Epoch 6/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1593 - val_loss: 0.0925
Epoch 7/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1539 - val_loss: 0.0959
Epoch 8/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1494 - val_loss: 0.0965
Epoch 9/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1461 - val_loss: 0.0932
Epoch 10/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1430 - val_loss: 0.0911
Epoch 11/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1401 - val_loss: 0.0932
Epoch 12/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1380 - val_loss: 0.0906
Epoch 13/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1360 - val_loss: 0.0916
Epoch 14/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1343 - val_loss: 0.0891
Epoch 15/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1324 - val_loss: 0.0918
Epoch 16/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1310 - val_loss: 0.0911
Epoch 17/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1297 - val_loss: 0.0912
Epoch 18/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1284 - val_loss: 0.0902
Epoch 19/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1277 - val_loss: 0.0905
Epoch 20/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1265 - val_loss: 0.0890
Epoch 21/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1255 - val_loss: 0.0883
Epoch 22/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1246 - val_loss: 0.0895
Epoch 23/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1239 - val_loss: 0.0902
Epoch 24/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1232 - val_loss: 0.0881
Epoch 25/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1226 - val_loss: 0.0877
Epoch 26/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1223 - val_loss: 0.0884
Epoch 27/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1216 - val_loss: 0.0885
Epoch 28/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1209 - val_loss: 0.0885
Epoch 29/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1205 - val_loss: 0.0865
Epoch 30/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1199 - val_loss: 0.0879
Epoch 31/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1195 - val_loss: 0.0870
Epoch 32/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1191 - val_loss: 0.0878
Epoch 33/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1189 - val_loss: 0.0885
Epoch 34/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1184 - val_loss: 0.0886
Epoch 35/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1180 - val_loss: 0.0873
Epoch 36/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1174 - val_loss: 0.0876
Epoch 37/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1173 - val_loss: 0.0867
Epoch 38/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1171 - val_loss: 0.0865
Epoch 39/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1166 - val_loss: 0.0857
Epoch 40/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1162 - val_loss: 0.0855
Epoch 41/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1157 - val_loss: 0.0849
Epoch 42/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1156 - val_loss: 0.0869
Epoch 43/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1153 - val_loss: 0.0846
Epoch 44/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1149 - val_loss: 0.0851
Epoch 45/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1143 - val_loss: 0.0852
Epoch 46/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1142 - val_loss: 0.0843
Epoch 47/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1138 - val_loss: 0.0852
Epoch 48/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1135 - val_loss: 0.0842
Epoch 49/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1131 - val_loss: 0.0833
Epoch 50/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1129 - val_loss: 0.0832
Epoch 51/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1125 - val_loss: 0.0824
Epoch 52/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1123 - val_loss: 0.0852
Epoch 53/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1118 - val_loss: 0.0841
Epoch 54/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1117 - val_loss: 0.0834
Epoch 55/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1113 - val_loss: 0.0842
Epoch 56/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1110 - val_loss: 0.0826
Epoch 57/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1106 - val_loss: 0.0818
Epoch 58/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1103 - val_loss: 0.0827
Epoch 59/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1102 - val_loss: 0.0837
Epoch 60/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1100 - val_loss: 0.0819
Epoch 61/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1097 - val_loss: 0.0828
Epoch 62/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1095 - val_loss: 0.0831
Epoch 63/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1092 - val_loss: 0.0824
Epoch 64/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1087 - val_loss: 0.0834
Epoch 65/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1085 - val_loss: 0.0825
Epoch 66/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1084 - val_loss: 0.0831
Epoch 67/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1082 - val_loss: 0.0824
Val_yp Shape is 
(75731, 24)
Results === Test == Validation ===== 47
=============================
step 1    -  0.992    0.991
step 2    -  0.985    0.983
step 3    -  0.976    0.974
step 4    -  0.967    0.965
step 5    -  0.957    0.955
step 6    -  0.946    0.945
step 7    -  0.936    0.935
step 8    -  0.926    0.926
step 9    -  0.916    0.917
step 10   -  0.908    0.910
step 11   -  0.900    0.902
step 12   -  0.892    0.894
step 13   -  0.885    0.887
step 14   -  0.879    0.882
step 15   -  0.873    0.876
step 16   -  0.867    0.870
step 17   -  0.862    0.866
step 18   -  0.858    0.863
step 19   -  0.853    0.858
step 20   -  0.849    0.854
step 21   -  0.845    0.851
step 22   -  0.842    0.849
step 23   -  0.839    0.846
step 24   -  0.833    0.841
=============================
Summary   -  21.586    21.641
V_y Shape is 
(75731, 1)
=========== SHAPE INPUT-OUTPUT MATRIXES =====================
shape x_training : (227226, 336)  y_training: :  (227226, 24)
shape x_test     :  (75730, 336)  y_test      :  (75730, 24)
shape x_val      :  (75730, 336)  y_val       :  (75730, 24)
=============================================================
(336,)
Model: "MLP"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_22 (InputLayer)       [(None, 336)]             0         
                                                                 
 dense_84 (Dense)            (None, 1024)              345088    
                                                                 
 elu_21 (ELU)                (None, 1024)              0         
                                                                 
 dropout_63 (Dropout)        (None, 1024)              0         
                                                                 
 dense_85 (Dense)            (None, 512)               524800    
                                                                 
 dropout_64 (Dropout)        (None, 512)               0         
                                                                 
 dense_86 (Dense)            (None, 512)               262656    
                                                                 
 dropout_65 (Dropout)        (None, 512)               0         
                                                                 
 dense_87 (Dense)            (None, 24)                12312     
                                                                 
=================================================================
Total params: 1,144,856
Trainable params: 1,144,856
Non-trainable params: 0
_________________________________________________________________
Epoch 1/200
222/222 [==============================] - 2s 7ms/step - loss: 0.4328 - val_loss: 0.1111
Epoch 2/200
222/222 [==============================] - 1s 6ms/step - loss: 0.2283 - val_loss: 0.1028
Epoch 3/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1937 - val_loss: 0.0988
Epoch 4/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1775 - val_loss: 0.0985
Epoch 5/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1674 - val_loss: 0.0962
Epoch 6/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1599 - val_loss: 0.0933
Epoch 7/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1544 - val_loss: 0.0950
Epoch 8/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1500 - val_loss: 0.0956
Epoch 9/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1465 - val_loss: 0.0915
Epoch 10/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1432 - val_loss: 0.0928
Epoch 11/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1409 - val_loss: 0.0928
Epoch 12/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1386 - val_loss: 0.0971
Epoch 13/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1362 - val_loss: 0.0911
Epoch 14/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1345 - val_loss: 0.0907
Epoch 15/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1331 - val_loss: 0.0899
Epoch 16/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1316 - val_loss: 0.0915
Epoch 17/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1297 - val_loss: 0.0897
Epoch 18/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1289 - val_loss: 0.0911
Epoch 19/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1278 - val_loss: 0.0909
Epoch 20/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1267 - val_loss: 0.0893
Epoch 21/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1257 - val_loss: 0.0890
Epoch 22/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1249 - val_loss: 0.0897
Epoch 23/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1241 - val_loss: 0.0891
Epoch 24/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1234 - val_loss: 0.0887
Epoch 25/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1228 - val_loss: 0.0885
Epoch 26/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1221 - val_loss: 0.0877
Epoch 27/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1215 - val_loss: 0.0878
Epoch 28/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1210 - val_loss: 0.0885
Epoch 29/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1204 - val_loss: 0.0882
Epoch 30/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1200 - val_loss: 0.0869
Epoch 31/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1193 - val_loss: 0.0870
Epoch 32/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1191 - val_loss: 0.0877
Epoch 33/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1187 - val_loss: 0.0873
Epoch 34/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1184 - val_loss: 0.0881
Epoch 35/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1178 - val_loss: 0.0864
Epoch 36/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1177 - val_loss: 0.0869
Epoch 37/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1172 - val_loss: 0.0877
Epoch 38/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1168 - val_loss: 0.0871
Epoch 39/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1166 - val_loss: 0.0872
Epoch 40/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1162 - val_loss: 0.0853
Epoch 41/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1157 - val_loss: 0.0848
Epoch 42/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1154 - val_loss: 0.0851
Epoch 43/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1153 - val_loss: 0.0865
Epoch 44/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1147 - val_loss: 0.0854
Epoch 45/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1145 - val_loss: 0.0856
Epoch 46/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1140 - val_loss: 0.0860
Epoch 47/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1137 - val_loss: 0.0831
Epoch 48/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1136 - val_loss: 0.0847
Epoch 49/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1131 - val_loss: 0.0846
Epoch 50/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1128 - val_loss: 0.0836
Epoch 51/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1125 - val_loss: 0.0858
Epoch 52/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1124 - val_loss: 0.0845
Epoch 53/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1118 - val_loss: 0.0831
Epoch 54/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1116 - val_loss: 0.0830
Epoch 55/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1112 - val_loss: 0.0840
Epoch 56/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1109 - val_loss: 0.0834
Epoch 57/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1105 - val_loss: 0.0828
Epoch 58/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1103 - val_loss: 0.0830
Epoch 59/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1100 - val_loss: 0.0825
Epoch 60/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1097 - val_loss: 0.0825
Epoch 61/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1094 - val_loss: 0.0829
Epoch 62/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1093 - val_loss: 0.0833
Epoch 63/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1090 - val_loss: 0.0824
Epoch 64/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1088 - val_loss: 0.0810
Epoch 65/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1083 - val_loss: 0.0821
Epoch 66/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1081 - val_loss: 0.0845
Epoch 67/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1081 - val_loss: 0.0822
Epoch 68/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1077 - val_loss: 0.0829
Epoch 69/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1076 - val_loss: 0.0811
Epoch 70/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1074 - val_loss: 0.0815
Epoch 71/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1070 - val_loss: 0.0801
Epoch 72/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1067 - val_loss: 0.0820
Epoch 73/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1066 - val_loss: 0.0805
Epoch 74/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1063 - val_loss: 0.0813
Epoch 75/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1062 - val_loss: 0.0827
Epoch 76/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1059 - val_loss: 0.0810
Epoch 77/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1059 - val_loss: 0.0810
Epoch 78/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1056 - val_loss: 0.0802
Epoch 79/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1054 - val_loss: 0.0820
Epoch 80/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1052 - val_loss: 0.0796
Epoch 81/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1049 - val_loss: 0.0803
Epoch 82/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1047 - val_loss: 0.0823
Epoch 83/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1048 - val_loss: 0.0821
Epoch 84/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1044 - val_loss: 0.0803
Epoch 85/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1041 - val_loss: 0.0813
Epoch 86/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1040 - val_loss: 0.0802
Epoch 87/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1040 - val_loss: 0.0815
Epoch 88/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1039 - val_loss: 0.0813
Epoch 89/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1037 - val_loss: 0.0801
Epoch 90/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1033 - val_loss: 0.0796
Val_yp Shape is 
(75730, 24)
Results === Test == Validation ===== 48
=============================
step 1    -  0.992    0.991
step 2    -  0.985    0.984
step 3    -  0.977    0.975
step 4    -  0.967    0.965
step 5    -  0.958    0.956
step 6    -  0.949    0.947
step 7    -  0.940    0.938
step 8    -  0.931    0.931
step 9    -  0.924    0.923
step 10   -  0.916    0.916
step 11   -  0.907    0.908
step 12   -  0.899    0.901
step 13   -  0.892    0.894
step 14   -  0.887    0.889
step 15   -  0.881    0.885
step 16   -  0.876    0.880
step 17   -  0.871    0.876
step 18   -  0.868    0.873
step 19   -  0.864    0.870
step 20   -  0.858    0.865
step 21   -  0.854    0.861
step 22   -  0.850    0.857
step 23   -  0.843    0.851
step 24   -  0.838    0.846
=============================
Summary   -  21.724    21.782
V_y Shape is 
(75730, 1)
=========== SHAPE INPUT-OUTPUT MATRIXES =====================
shape x_training : (227225, 343)  y_training: :  (227225, 24)
shape x_test     :  (75729, 343)  y_test      :  (75729, 24)
shape x_val      :  (75730, 343)  y_val       :  (75730, 24)
=============================================================
(343,)
Model: "MLP"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_23 (InputLayer)       [(None, 343)]             0         
                                                                 
 dense_88 (Dense)            (None, 1024)              352256    
                                                                 
 elu_22 (ELU)                (None, 1024)              0         
                                                                 
 dropout_66 (Dropout)        (None, 1024)              0         
                                                                 
 dense_89 (Dense)            (None, 512)               524800    
                                                                 
 dropout_67 (Dropout)        (None, 512)               0         
                                                                 
 dense_90 (Dense)            (None, 512)               262656    
                                                                 
 dropout_68 (Dropout)        (None, 512)               0         
                                                                 
 dense_91 (Dense)            (None, 24)                12312     
                                                                 
=================================================================
Total params: 1,152,024
Trainable params: 1,152,024
Non-trainable params: 0
_________________________________________________________________
Epoch 1/200
222/222 [==============================] - 2s 8ms/step - loss: 0.4270 - val_loss: 0.1134
Epoch 2/200
222/222 [==============================] - 1s 6ms/step - loss: 0.2285 - val_loss: 0.1040
Epoch 3/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1943 - val_loss: 0.0989
Epoch 4/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1779 - val_loss: 0.0978
Epoch 5/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1676 - val_loss: 0.0954
Epoch 6/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1601 - val_loss: 0.0969
Epoch 7/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1546 - val_loss: 0.0955
Epoch 8/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1498 - val_loss: 0.0948
Epoch 9/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1462 - val_loss: 0.0926
Epoch 10/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1432 - val_loss: 0.0923
Epoch 11/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1404 - val_loss: 0.0918
Epoch 12/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1381 - val_loss: 0.0928
Epoch 13/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1359 - val_loss: 0.0918
Epoch 14/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1340 - val_loss: 0.0901
Epoch 15/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1325 - val_loss: 0.0902
Epoch 16/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1311 - val_loss: 0.0902
Epoch 17/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1298 - val_loss: 0.0909
Epoch 18/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1286 - val_loss: 0.0892
Epoch 19/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1272 - val_loss: 0.0899
Epoch 20/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1263 - val_loss: 0.0890
Epoch 21/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1256 - val_loss: 0.0889
Epoch 22/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1244 - val_loss: 0.0916
Epoch 23/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1236 - val_loss: 0.0897
Epoch 24/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1230 - val_loss: 0.0876
Epoch 25/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1226 - val_loss: 0.0882
Epoch 26/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1216 - val_loss: 0.0875
Epoch 27/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1213 - val_loss: 0.0887
Epoch 28/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1207 - val_loss: 0.0878
Epoch 29/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1203 - val_loss: 0.0865
Epoch 30/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1197 - val_loss: 0.0883
Epoch 31/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1190 - val_loss: 0.0867
Epoch 32/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1188 - val_loss: 0.0888
Epoch 33/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1184 - val_loss: 0.0893
Epoch 34/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1179 - val_loss: 0.0873
Epoch 35/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1176 - val_loss: 0.0868
Epoch 36/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1175 - val_loss: 0.0854
Epoch 37/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1170 - val_loss: 0.0870
Epoch 38/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1168 - val_loss: 0.0876
Epoch 39/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1162 - val_loss: 0.0847
Epoch 40/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1159 - val_loss: 0.0867
Epoch 41/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1155 - val_loss: 0.0854
Epoch 42/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1152 - val_loss: 0.0853
Epoch 43/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1151 - val_loss: 0.0858
Epoch 44/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1145 - val_loss: 0.0851
Epoch 45/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1142 - val_loss: 0.0870
Epoch 46/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1139 - val_loss: 0.0865
Epoch 47/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1137 - val_loss: 0.0833
Epoch 48/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1132 - val_loss: 0.0854
Epoch 49/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1127 - val_loss: 0.0873
Epoch 50/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1124 - val_loss: 0.0836
Epoch 51/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1119 - val_loss: 0.0830
Epoch 52/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1118 - val_loss: 0.0832
Epoch 53/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1112 - val_loss: 0.0829
Epoch 54/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1113 - val_loss: 0.0829
Epoch 55/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1106 - val_loss: 0.0841
Epoch 56/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1106 - val_loss: 0.0835
Epoch 57/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1101 - val_loss: 0.0842
Epoch 58/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1102 - val_loss: 0.0828
Epoch 59/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1097 - val_loss: 0.0819
Epoch 60/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1094 - val_loss: 0.0829
Epoch 61/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1090 - val_loss: 0.0834
Epoch 62/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1087 - val_loss: 0.0810
Epoch 63/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1085 - val_loss: 0.0826
Epoch 64/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1082 - val_loss: 0.0810
Epoch 65/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1080 - val_loss: 0.0822
Epoch 66/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1077 - val_loss: 0.0819
Epoch 67/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1075 - val_loss: 0.0831
Epoch 68/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1073 - val_loss: 0.0812
Epoch 69/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1069 - val_loss: 0.0809
Epoch 70/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1067 - val_loss: 0.0808
Epoch 71/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1066 - val_loss: 0.0825
Epoch 72/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1063 - val_loss: 0.0809
Epoch 73/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1063 - val_loss: 0.0790
Epoch 74/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1060 - val_loss: 0.0805
Epoch 75/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1058 - val_loss: 0.0813
Epoch 76/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1056 - val_loss: 0.0804
Epoch 77/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1051 - val_loss: 0.0805
Epoch 78/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1051 - val_loss: 0.0805
Epoch 79/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1048 - val_loss: 0.0812
Epoch 80/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1047 - val_loss: 0.0798
Epoch 81/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1044 - val_loss: 0.0804
Epoch 82/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1043 - val_loss: 0.0805
Epoch 83/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1041 - val_loss: 0.0811
Val_yp Shape is 
(75730, 24)
Results === Test == Validation ===== 49
=============================
step 1    -  0.990    0.989
step 2    -  0.984    0.982
step 3    -  0.975    0.973
step 4    -  0.966    0.964
step 5    -  0.956    0.954
step 6    -  0.946    0.945
step 7    -  0.937    0.936
step 8    -  0.928    0.928
step 9    -  0.918    0.918
step 10   -  0.909    0.910
step 11   -  0.900    0.902
step 12   -  0.892    0.894
step 13   -  0.884    0.886
step 14   -  0.876    0.879
step 15   -  0.870    0.873
step 16   -  0.863    0.866
step 17   -  0.857    0.860
step 18   -  0.851    0.855
step 19   -  0.846    0.851
step 20   -  0.842    0.847
step 21   -  0.835    0.841
step 22   -  0.829    0.836
step 23   -  0.824    0.831
step 24   -  0.818    0.826
=============================
Summary   -  21.497    21.544
V_y Shape is 
(75730, 1)
=========== SHAPE INPUT-OUTPUT MATRIXES =====================
shape x_training : (227224, 350)  y_training: :  (227224, 24)
shape x_test     :  (75729, 350)  y_test      :  (75729, 24)
shape x_val      :  (75729, 350)  y_val       :  (75729, 24)
=============================================================
(350,)
Model: "MLP"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_24 (InputLayer)       [(None, 350)]             0         
                                                                 
 dense_92 (Dense)            (None, 1024)              359424    
                                                                 
 elu_23 (ELU)                (None, 1024)              0         
                                                                 
 dropout_69 (Dropout)        (None, 1024)              0         
                                                                 
 dense_93 (Dense)            (None, 512)               524800    
                                                                 
 dropout_70 (Dropout)        (None, 512)               0         
                                                                 
 dense_94 (Dense)            (None, 512)               262656    
                                                                 
 dropout_71 (Dropout)        (None, 512)               0         
                                                                 
 dense_95 (Dense)            (None, 24)                12312     
                                                                 
=================================================================
Total params: 1,159,192
Trainable params: 1,159,192
Non-trainable params: 0
_________________________________________________________________
Epoch 1/200
222/222 [==============================] - 2s 7ms/step - loss: 0.4255 - val_loss: 0.1110
Epoch 2/200
222/222 [==============================] - 1s 6ms/step - loss: 0.2281 - val_loss: 0.1017
Epoch 3/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1943 - val_loss: 0.1002
Epoch 4/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1776 - val_loss: 0.0958
Epoch 5/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1671 - val_loss: 0.0971
Epoch 6/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1598 - val_loss: 0.0944
Epoch 7/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1542 - val_loss: 0.0934
Epoch 8/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1495 - val_loss: 0.0967
Epoch 9/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1460 - val_loss: 0.0923
Epoch 10/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1425 - val_loss: 0.0913
Epoch 11/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1401 - val_loss: 0.0909
Epoch 12/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1375 - val_loss: 0.0915
Epoch 13/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1357 - val_loss: 0.0904
Epoch 14/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1336 - val_loss: 0.0912
Epoch 15/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1322 - val_loss: 0.0915
Epoch 16/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1305 - val_loss: 0.0910
Epoch 17/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1292 - val_loss: 0.0904
Epoch 18/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1284 - val_loss: 0.0919
Epoch 19/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1269 - val_loss: 0.0890
Epoch 20/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1258 - val_loss: 0.0887
Epoch 21/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1250 - val_loss: 0.0882
Epoch 22/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1241 - val_loss: 0.0889
Epoch 23/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1233 - val_loss: 0.0889
Epoch 24/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1226 - val_loss: 0.0871
Epoch 25/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1219 - val_loss: 0.0882
Epoch 26/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1213 - val_loss: 0.0879
Epoch 27/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1208 - val_loss: 0.0881
Epoch 28/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1202 - val_loss: 0.0878
Epoch 29/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1196 - val_loss: 0.0871
Epoch 30/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1191 - val_loss: 0.0862
Epoch 31/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1190 - val_loss: 0.0860
Epoch 32/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1186 - val_loss: 0.0864
Epoch 33/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1180 - val_loss: 0.0863
Epoch 34/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1176 - val_loss: 0.0876
Epoch 35/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1174 - val_loss: 0.0865
Epoch 36/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1168 - val_loss: 0.0876
Epoch 37/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1166 - val_loss: 0.0869
Epoch 38/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1162 - val_loss: 0.0871
Epoch 39/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1159 - val_loss: 0.0871
Epoch 40/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1155 - val_loss: 0.0860
Epoch 41/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1154 - val_loss: 0.0863
Epoch 42/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1149 - val_loss: 0.0846
Epoch 43/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1145 - val_loss: 0.0846
Epoch 44/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1143 - val_loss: 0.0849
Epoch 45/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1140 - val_loss: 0.0850
Epoch 46/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1136 - val_loss: 0.0841
Epoch 47/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1132 - val_loss: 0.0840
Epoch 48/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1129 - val_loss: 0.0842
Epoch 49/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1124 - val_loss: 0.0863
Epoch 50/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1123 - val_loss: 0.0848
Epoch 51/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1118 - val_loss: 0.0842
Epoch 52/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1117 - val_loss: 0.0846
Epoch 53/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1113 - val_loss: 0.0839
Epoch 54/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1110 - val_loss: 0.0851
Epoch 55/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1107 - val_loss: 0.0833
Epoch 56/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1104 - val_loss: 0.0845
Epoch 57/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1098 - val_loss: 0.0836
Epoch 58/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1097 - val_loss: 0.0834
Epoch 59/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1095 - val_loss: 0.0830
Epoch 60/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1092 - val_loss: 0.0815
Epoch 61/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1089 - val_loss: 0.0824
Epoch 62/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1085 - val_loss: 0.0816
Epoch 63/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1084 - val_loss: 0.0843
Epoch 64/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1081 - val_loss: 0.0819
Epoch 65/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1079 - val_loss: 0.0811
Epoch 66/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1075 - val_loss: 0.0828
Epoch 67/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1072 - val_loss: 0.0829
Epoch 68/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1071 - val_loss: 0.0822
Epoch 69/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1066 - val_loss: 0.0820
Epoch 70/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1068 - val_loss: 0.0815
Epoch 71/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1064 - val_loss: 0.0805
Epoch 72/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1060 - val_loss: 0.0812
Epoch 73/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1059 - val_loss: 0.0816
Epoch 74/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1057 - val_loss: 0.0807
Epoch 75/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1054 - val_loss: 0.0825
Epoch 76/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1054 - val_loss: 0.0803
Epoch 77/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1050 - val_loss: 0.0817
Epoch 78/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1050 - val_loss: 0.0812
Epoch 79/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1045 - val_loss: 0.0822
Epoch 80/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1044 - val_loss: 0.0807
Epoch 81/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1042 - val_loss: 0.0832
Epoch 82/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1040 - val_loss: 0.0807
Epoch 83/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1038 - val_loss: 0.0801
Epoch 84/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1038 - val_loss: 0.0815
Epoch 85/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1035 - val_loss: 0.0819
Epoch 86/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1033 - val_loss: 0.0819
Epoch 87/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1030 - val_loss: 0.0814
Epoch 88/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1032 - val_loss: 0.0802
Epoch 89/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1027 - val_loss: 0.0803
Epoch 90/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1028 - val_loss: 0.0801
Epoch 91/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1023 - val_loss: 0.0812
Epoch 92/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1025 - val_loss: 0.0812
Epoch 93/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1021 - val_loss: 0.0799
Epoch 94/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1019 - val_loss: 0.0799
Epoch 95/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1018 - val_loss: 0.0802
Epoch 96/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1014 - val_loss: 0.0803
Epoch 97/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1014 - val_loss: 0.0808
Epoch 98/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1011 - val_loss: 0.0805
Epoch 99/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1011 - val_loss: 0.0809
Epoch 100/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1009 - val_loss: 0.0799
Epoch 101/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1007 - val_loss: 0.0801
Epoch 102/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1007 - val_loss: 0.0801
Epoch 103/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1005 - val_loss: 0.0793
Epoch 104/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1002 - val_loss: 0.0801
Epoch 105/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1001 - val_loss: 0.0792
Epoch 106/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1000 - val_loss: 0.0798
Epoch 107/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0999 - val_loss: 0.0796
Epoch 108/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0997 - val_loss: 0.0808
Epoch 109/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0995 - val_loss: 0.0801
Epoch 110/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0995 - val_loss: 0.0805
Epoch 111/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0994 - val_loss: 0.0793
Epoch 112/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0991 - val_loss: 0.0794
Epoch 113/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0990 - val_loss: 0.0813
Epoch 114/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0988 - val_loss: 0.0804
Epoch 115/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0987 - val_loss: 0.0794
Val_yp Shape is 
(75729, 24)
Results === Test == Validation ===== 50
=============================
step 1    -  0.992    0.991
step 2    -  0.985    0.984
step 3    -  0.977    0.974
step 4    -  0.968    0.965
step 5    -  0.958    0.956
step 6    -  0.949    0.947
step 7    -  0.940    0.939
step 8    -  0.932    0.932
step 9    -  0.923    0.923
step 10   -  0.914    0.915
step 11   -  0.906    0.908
step 12   -  0.899    0.901
step 13   -  0.893    0.896
step 14   -  0.887    0.890
step 15   -  0.881    0.884
step 16   -  0.875    0.879
step 17   -  0.871    0.875
step 18   -  0.865    0.870
step 19   -  0.860    0.864
step 20   -  0.855    0.859
step 21   -  0.849    0.855
step 22   -  0.846    0.852
step 23   -  0.841    0.849
step 24   -  0.836    0.845
=============================
Summary   -  21.704    21.754
V_y Shape is 
(75729, 1)
=========== SHAPE INPUT-OUTPUT MATRIXES =====================
shape x_training : (227223, 357)  y_training: :  (227223, 24)
shape x_test     :  (75728, 357)  y_test      :  (75728, 24)
shape x_val      :  (75729, 357)  y_val       :  (75729, 24)
=============================================================
(357,)
Model: "MLP"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_25 (InputLayer)       [(None, 357)]             0         
                                                                 
 dense_96 (Dense)            (None, 1024)              366592    
                                                                 
 elu_24 (ELU)                (None, 1024)              0         
                                                                 
 dropout_72 (Dropout)        (None, 1024)              0         
                                                                 
 dense_97 (Dense)            (None, 512)               524800    
                                                                 
 dropout_73 (Dropout)        (None, 512)               0         
                                                                 
 dense_98 (Dense)            (None, 512)               262656    
                                                                 
 dropout_74 (Dropout)        (None, 512)               0         
                                                                 
 dense_99 (Dense)            (None, 24)                12312     
                                                                 
=================================================================
Total params: 1,166,360
Trainable params: 1,166,360
Non-trainable params: 0
_________________________________________________________________
Epoch 1/200
222/222 [==============================] - 2s 7ms/step - loss: 0.4337 - val_loss: 0.1110
Epoch 2/200
222/222 [==============================] - 1s 6ms/step - loss: 0.2303 - val_loss: 0.1029
Epoch 3/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1958 - val_loss: 0.1015
Epoch 4/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1789 - val_loss: 0.0990
Epoch 5/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1683 - val_loss: 0.0959
Epoch 6/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1607 - val_loss: 0.0954
Epoch 7/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1547 - val_loss: 0.0951
Epoch 8/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1503 - val_loss: 0.0930
Epoch 9/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1467 - val_loss: 0.0910
Epoch 10/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1434 - val_loss: 0.0906
Epoch 11/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1402 - val_loss: 0.0947
Epoch 12/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1382 - val_loss: 0.0935
Epoch 13/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1361 - val_loss: 0.0915
Epoch 14/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1339 - val_loss: 0.0904
Epoch 15/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1322 - val_loss: 0.0896
Epoch 16/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1307 - val_loss: 0.0888
Epoch 17/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1294 - val_loss: 0.0884
Epoch 18/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1282 - val_loss: 0.0905
Epoch 19/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1270 - val_loss: 0.0888
Epoch 20/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1259 - val_loss: 0.0888
Epoch 21/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1250 - val_loss: 0.0930
Epoch 22/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1243 - val_loss: 0.0897
Epoch 23/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1232 - val_loss: 0.0869
Epoch 24/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1228 - val_loss: 0.0880
Epoch 25/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1220 - val_loss: 0.0909
Epoch 26/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1214 - val_loss: 0.0865
Epoch 27/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1207 - val_loss: 0.0875
Epoch 28/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1202 - val_loss: 0.0879
Epoch 29/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1197 - val_loss: 0.0855
Epoch 30/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1192 - val_loss: 0.0869
Epoch 31/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1187 - val_loss: 0.0861
Epoch 32/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1182 - val_loss: 0.0869
Epoch 33/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1180 - val_loss: 0.0866
Epoch 34/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1174 - val_loss: 0.0853
Epoch 35/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1170 - val_loss: 0.0871
Epoch 36/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1167 - val_loss: 0.0859
Epoch 37/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1165 - val_loss: 0.0861
Epoch 38/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1160 - val_loss: 0.0861
Epoch 39/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1156 - val_loss: 0.0872
Epoch 40/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1154 - val_loss: 0.0855
Epoch 41/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1152 - val_loss: 0.0864
Epoch 42/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1147 - val_loss: 0.0853
Epoch 43/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1144 - val_loss: 0.0867
Epoch 44/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1139 - val_loss: 0.0845
Epoch 45/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1138 - val_loss: 0.0853
Epoch 46/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1135 - val_loss: 0.0850
Epoch 47/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1128 - val_loss: 0.0854
Epoch 48/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1126 - val_loss: 0.0850
Epoch 49/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1125 - val_loss: 0.0852
Epoch 50/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1119 - val_loss: 0.0845
Epoch 51/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1117 - val_loss: 0.0838
Epoch 52/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1113 - val_loss: 0.0834
Epoch 53/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1110 - val_loss: 0.0841
Epoch 54/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1106 - val_loss: 0.0845
Epoch 55/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1105 - val_loss: 0.0834
Epoch 56/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1101 - val_loss: 0.0837
Epoch 57/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1098 - val_loss: 0.0831
Epoch 58/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1094 - val_loss: 0.0816
Epoch 59/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1092 - val_loss: 0.0824
Epoch 60/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1090 - val_loss: 0.0816
Epoch 61/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1085 - val_loss: 0.0825
Epoch 62/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1080 - val_loss: 0.0841
Epoch 63/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1080 - val_loss: 0.0828
Epoch 64/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1078 - val_loss: 0.0820
Epoch 65/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1075 - val_loss: 0.0820
Epoch 66/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1073 - val_loss: 0.0816
Epoch 67/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1070 - val_loss: 0.0824
Epoch 68/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1069 - val_loss: 0.0827
Epoch 69/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1064 - val_loss: 0.0823
Epoch 70/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1061 - val_loss: 0.0828
Val_yp Shape is 
(75729, 24)
Results === Test == Validation ===== 51
=============================
step 1    -  0.991    0.990
step 2    -  0.985    0.983
step 3    -  0.976    0.974
step 4    -  0.967    0.964
step 5    -  0.957    0.955
step 6    -  0.947    0.945
step 7    -  0.938    0.937
step 8    -  0.929    0.929
step 9    -  0.921    0.922
step 10   -  0.914    0.915
step 11   -  0.906    0.908
step 12   -  0.899    0.901
step 13   -  0.893    0.895
step 14   -  0.886    0.889
step 15   -  0.880    0.883
step 16   -  0.875    0.878
step 17   -  0.869    0.873
step 18   -  0.864    0.868
step 19   -  0.860    0.864
step 20   -  0.855    0.860
step 21   -  0.850    0.856
step 22   -  0.846    0.852
step 23   -  0.839    0.846
step 24   -  0.834    0.842
=============================
Summary   -  21.682    21.728
V_y Shape is 
(75729, 1)
=========== SHAPE INPUT-OUTPUT MATRIXES =====================
shape x_training : (227222, 364)  y_training: :  (227222, 24)
shape x_test     :  (75728, 364)  y_test      :  (75728, 24)
shape x_val      :  (75728, 364)  y_val       :  (75728, 24)
=============================================================
(364,)
Model: "MLP"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_26 (InputLayer)       [(None, 364)]             0         
                                                                 
 dense_100 (Dense)           (None, 1024)              373760    
                                                                 
 elu_25 (ELU)                (None, 1024)              0         
                                                                 
 dropout_75 (Dropout)        (None, 1024)              0         
                                                                 
 dense_101 (Dense)           (None, 512)               524800    
                                                                 
 dropout_76 (Dropout)        (None, 512)               0         
                                                                 
 dense_102 (Dense)           (None, 512)               262656    
                                                                 
 dropout_77 (Dropout)        (None, 512)               0         
                                                                 
 dense_103 (Dense)           (None, 24)                12312     
                                                                 
=================================================================
Total params: 1,173,528
Trainable params: 1,173,528
Non-trainable params: 0
_________________________________________________________________
Epoch 1/200
222/222 [==============================] - 2s 8ms/step - loss: 0.4360 - val_loss: 0.1115
Epoch 2/200
222/222 [==============================] - 1s 6ms/step - loss: 0.2311 - val_loss: 0.1045
Epoch 3/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1958 - val_loss: 0.0979
Epoch 4/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1785 - val_loss: 0.0986
Epoch 5/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1678 - val_loss: 0.0965
Epoch 6/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1604 - val_loss: 0.0950
Epoch 7/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1547 - val_loss: 0.0942
Epoch 8/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1502 - val_loss: 0.0923
Epoch 9/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1461 - val_loss: 0.0916
Epoch 10/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1428 - val_loss: 0.0951
Epoch 11/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1403 - val_loss: 0.0935
Epoch 12/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1377 - val_loss: 0.0895
Epoch 13/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1356 - val_loss: 0.0914
Epoch 14/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1337 - val_loss: 0.0914
Epoch 15/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1321 - val_loss: 0.0907
Epoch 16/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1307 - val_loss: 0.0939
Epoch 17/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1292 - val_loss: 0.0894
Epoch 18/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1278 - val_loss: 0.0891
Epoch 19/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1265 - val_loss: 0.0901
Epoch 20/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1256 - val_loss: 0.0911
Epoch 21/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1247 - val_loss: 0.0903
Epoch 22/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1241 - val_loss: 0.0888
Epoch 23/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1230 - val_loss: 0.0871
Epoch 24/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1223 - val_loss: 0.0875
Epoch 25/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1217 - val_loss: 0.0884
Epoch 26/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1210 - val_loss: 0.0894
Epoch 27/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1205 - val_loss: 0.0870
Epoch 28/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1198 - val_loss: 0.0871
Epoch 29/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1196 - val_loss: 0.0866
Epoch 30/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1191 - val_loss: 0.0896
Epoch 31/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1182 - val_loss: 0.0870
Epoch 32/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1180 - val_loss: 0.0869
Epoch 33/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1178 - val_loss: 0.0869
Epoch 34/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1172 - val_loss: 0.0865
Epoch 35/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1168 - val_loss: 0.0858
Epoch 36/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1165 - val_loss: 0.0861
Epoch 37/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1163 - val_loss: 0.0851
Epoch 38/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1157 - val_loss: 0.0878
Epoch 39/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1155 - val_loss: 0.0874
Epoch 40/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1151 - val_loss: 0.0869
Epoch 41/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1148 - val_loss: 0.0865
Epoch 42/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1145 - val_loss: 0.0857
Epoch 43/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1142 - val_loss: 0.0850
Epoch 44/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1138 - val_loss: 0.0856
Epoch 45/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1135 - val_loss: 0.0862
Epoch 46/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1133 - val_loss: 0.0862
Epoch 47/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1128 - val_loss: 0.0845
Epoch 48/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1124 - val_loss: 0.0851
Epoch 49/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1122 - val_loss: 0.0842
Epoch 50/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1117 - val_loss: 0.0834
Epoch 51/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1117 - val_loss: 0.0835
Epoch 52/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1112 - val_loss: 0.0842
Epoch 53/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1108 - val_loss: 0.0849
Epoch 54/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1105 - val_loss: 0.0835
Epoch 55/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1101 - val_loss: 0.0842
Epoch 56/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1099 - val_loss: 0.0843
Epoch 57/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1095 - val_loss: 0.0825
Epoch 58/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1095 - val_loss: 0.0844
Epoch 59/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1090 - val_loss: 0.0832
Epoch 60/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1087 - val_loss: 0.0827
Epoch 61/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1084 - val_loss: 0.0821
Epoch 62/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1080 - val_loss: 0.0820
Epoch 63/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1077 - val_loss: 0.0821
Epoch 64/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1077 - val_loss: 0.0830
Epoch 65/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1071 - val_loss: 0.0814
Epoch 66/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1068 - val_loss: 0.0824
Epoch 67/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1068 - val_loss: 0.0807
Epoch 68/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1064 - val_loss: 0.0840
Epoch 69/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1062 - val_loss: 0.0812
Epoch 70/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1060 - val_loss: 0.0809
Epoch 71/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1059 - val_loss: 0.0806
Epoch 72/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1055 - val_loss: 0.0834
Epoch 73/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1052 - val_loss: 0.0809
Epoch 74/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1052 - val_loss: 0.0811
Epoch 75/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1050 - val_loss: 0.0820
Epoch 76/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1046 - val_loss: 0.0829
Epoch 77/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1044 - val_loss: 0.0814
Epoch 78/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1044 - val_loss: 0.0819
Epoch 79/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1041 - val_loss: 0.0790
Epoch 80/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1039 - val_loss: 0.0806
Epoch 81/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1036 - val_loss: 0.0820
Epoch 82/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1032 - val_loss: 0.0794
Epoch 83/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1029 - val_loss: 0.0809
Epoch 84/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1030 - val_loss: 0.0803
Epoch 85/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1027 - val_loss: 0.0811
Epoch 86/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1027 - val_loss: 0.0816
Epoch 87/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1025 - val_loss: 0.0809
Epoch 88/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1023 - val_loss: 0.0813
Epoch 89/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1023 - val_loss: 0.0809
Val_yp Shape is 
(75728, 24)
Results === Test == Validation ===== 52
=============================
step 1    -  0.992    0.991
step 2    -  0.985    0.983
step 3    -  0.976    0.974
step 4    -  0.966    0.964
step 5    -  0.957    0.955
step 6    -  0.947    0.946
step 7    -  0.938    0.938
step 8    -  0.930    0.930
step 9    -  0.922    0.923
step 10   -  0.914    0.916
step 11   -  0.908    0.910
step 12   -  0.902    0.904
step 13   -  0.896    0.899
step 14   -  0.891    0.893
step 15   -  0.885    0.888
step 16   -  0.879    0.883
step 17   -  0.875    0.879
step 18   -  0.870    0.874
step 19   -  0.866    0.871
step 20   -  0.860    0.866
step 21   -  0.854    0.861
step 22   -  0.849    0.856
step 23   -  0.841    0.849
step 24   -  0.834    0.842
=============================
Summary   -  21.738    21.792
V_y Shape is 
(75728, 1)
=========== SHAPE INPUT-OUTPUT MATRIXES =====================
shape x_training : (227221, 371)  y_training: :  (227221, 24)
shape x_test     :  (75727, 371)  y_test      :  (75727, 24)
shape x_val      :  (75728, 371)  y_val       :  (75728, 24)
=============================================================
(371,)
Model: "MLP"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_27 (InputLayer)       [(None, 371)]             0         
                                                                 
 dense_104 (Dense)           (None, 1024)              380928    
                                                                 
 elu_26 (ELU)                (None, 1024)              0         
                                                                 
 dropout_78 (Dropout)        (None, 1024)              0         
                                                                 
 dense_105 (Dense)           (None, 512)               524800    
                                                                 
 dropout_79 (Dropout)        (None, 512)               0         
                                                                 
 dense_106 (Dense)           (None, 512)               262656    
                                                                 
 dropout_80 (Dropout)        (None, 512)               0         
                                                                 
 dense_107 (Dense)           (None, 24)                12312     
                                                                 
=================================================================
Total params: 1,180,696
Trainable params: 1,180,696
Non-trainable params: 0
_________________________________________________________________
Epoch 1/200
222/222 [==============================] - 2s 8ms/step - loss: 0.4382 - val_loss: 0.1118
Epoch 2/200
222/222 [==============================] - 1s 6ms/step - loss: 0.2324 - val_loss: 0.1074
Epoch 3/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1967 - val_loss: 0.1006
Epoch 4/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1792 - val_loss: 0.0974
Epoch 5/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1685 - val_loss: 0.0975
Epoch 6/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1605 - val_loss: 0.0952
Epoch 7/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1547 - val_loss: 0.0942
Epoch 8/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1499 - val_loss: 0.0928
Epoch 9/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1459 - val_loss: 0.0954
Epoch 10/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1430 - val_loss: 0.0908
Epoch 11/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1399 - val_loss: 0.0920
Epoch 12/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1375 - val_loss: 0.0913
Epoch 13/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1353 - val_loss: 0.0913
Epoch 14/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1334 - val_loss: 0.0923
Epoch 15/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1316 - val_loss: 0.0917
Epoch 16/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1302 - val_loss: 0.0882
Epoch 17/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1287 - val_loss: 0.0924
Epoch 18/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1276 - val_loss: 0.0892
Epoch 19/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1262 - val_loss: 0.0924
Epoch 20/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1254 - val_loss: 0.0882
Epoch 21/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1245 - val_loss: 0.0889
Epoch 22/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1235 - val_loss: 0.0884
Epoch 23/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1228 - val_loss: 0.0886
Epoch 24/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1219 - val_loss: 0.0888
Epoch 25/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1213 - val_loss: 0.0877
Epoch 26/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1206 - val_loss: 0.0883
Epoch 27/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1202 - val_loss: 0.0898
Epoch 28/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1195 - val_loss: 0.0865
Epoch 29/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1190 - val_loss: 0.0900
Epoch 30/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1185 - val_loss: 0.0876
Epoch 31/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1183 - val_loss: 0.0872
Epoch 32/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1177 - val_loss: 0.0860
Epoch 33/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1174 - val_loss: 0.0870
Epoch 34/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1169 - val_loss: 0.0887
Epoch 35/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1166 - val_loss: 0.0862
Epoch 36/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1162 - val_loss: 0.0867
Epoch 37/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1160 - val_loss: 0.0880
Epoch 38/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1155 - val_loss: 0.0867
Epoch 39/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1152 - val_loss: 0.0861
Epoch 40/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1148 - val_loss: 0.0854
Epoch 41/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1144 - val_loss: 0.0852
Epoch 42/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1143 - val_loss: 0.0865
Epoch 43/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1138 - val_loss: 0.0861
Epoch 44/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1134 - val_loss: 0.0855
Epoch 45/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1130 - val_loss: 0.0843
Epoch 46/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1128 - val_loss: 0.0849
Epoch 47/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1124 - val_loss: 0.0848
Epoch 48/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1120 - val_loss: 0.0851
Epoch 49/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1116 - val_loss: 0.0855
Epoch 50/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1114 - val_loss: 0.0847
Epoch 51/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1111 - val_loss: 0.0853
Epoch 52/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1106 - val_loss: 0.0853
Epoch 53/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1103 - val_loss: 0.0847
Epoch 54/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1100 - val_loss: 0.0847
Epoch 55/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1098 - val_loss: 0.0844
Val_yp Shape is 
(75728, 24)
Results === Test == Validation ===== 53
=============================
step 1    -  0.991    0.990
step 2    -  0.984    0.982
step 3    -  0.975    0.973
step 4    -  0.965    0.963
step 5    -  0.955    0.953
step 6    -  0.945    0.944
step 7    -  0.935    0.935
step 8    -  0.926    0.926
step 9    -  0.918    0.919
step 10   -  0.910    0.911
step 11   -  0.902    0.904
step 12   -  0.894    0.896
step 13   -  0.888    0.890
step 14   -  0.882    0.884
step 15   -  0.876    0.878
step 16   -  0.870    0.873
step 17   -  0.864    0.867
step 18   -  0.858    0.862
step 19   -  0.852    0.856
step 20   -  0.848    0.853
step 21   -  0.842    0.848
step 22   -  0.837    0.843
step 23   -  0.831    0.838
step 24   -  0.825    0.833
=============================
Summary   -  21.571    21.621
V_y Shape is 
(75728, 1)
=========== SHAPE INPUT-OUTPUT MATRIXES =====================
shape x_training : (227220, 378)  y_training: :  (227220, 24)
shape x_test     :  (75727, 378)  y_test      :  (75727, 24)
shape x_val      :  (75727, 378)  y_val       :  (75727, 24)
=============================================================
(378,)
Model: "MLP"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_28 (InputLayer)       [(None, 378)]             0         
                                                                 
 dense_108 (Dense)           (None, 1024)              388096    
                                                                 
 elu_27 (ELU)                (None, 1024)              0         
                                                                 
 dropout_81 (Dropout)        (None, 1024)              0         
                                                                 
 dense_109 (Dense)           (None, 512)               524800    
                                                                 
 dropout_82 (Dropout)        (None, 512)               0         
                                                                 
 dense_110 (Dense)           (None, 512)               262656    
                                                                 
 dropout_83 (Dropout)        (None, 512)               0         
                                                                 
 dense_111 (Dense)           (None, 24)                12312     
                                                                 
=================================================================
Total params: 1,187,864
Trainable params: 1,187,864
Non-trainable params: 0
_________________________________________________________________
Epoch 1/200
222/222 [==============================] - 2s 7ms/step - loss: 0.4402 - val_loss: 0.1118
Epoch 2/200
222/222 [==============================] - 1s 6ms/step - loss: 0.2327 - val_loss: 0.1025
Epoch 3/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1969 - val_loss: 0.0996
Epoch 4/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1795 - val_loss: 0.0972
Epoch 5/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1689 - val_loss: 0.0964
Epoch 6/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1611 - val_loss: 0.0948
Epoch 7/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1548 - val_loss: 0.0933
Epoch 8/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1501 - val_loss: 0.0932
Epoch 9/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1461 - val_loss: 0.0939
Epoch 10/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1430 - val_loss: 0.0910
Epoch 11/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1403 - val_loss: 0.0917
Epoch 12/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1377 - val_loss: 0.0915
Epoch 13/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1356 - val_loss: 0.0891
Epoch 14/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1339 - val_loss: 0.0893
Epoch 15/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1315 - val_loss: 0.0908
Epoch 16/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1303 - val_loss: 0.0898
Epoch 17/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1290 - val_loss: 0.0939
Epoch 18/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1276 - val_loss: 0.0891
Epoch 19/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1265 - val_loss: 0.0890
Epoch 20/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1252 - val_loss: 0.0875
Epoch 21/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1244 - val_loss: 0.0891
Epoch 22/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1236 - val_loss: 0.0902
Epoch 23/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1226 - val_loss: 0.0875
Epoch 24/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1220 - val_loss: 0.0886
Epoch 25/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1214 - val_loss: 0.0875
Epoch 26/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1206 - val_loss: 0.0886
Epoch 27/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1200 - val_loss: 0.0880
Epoch 28/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1197 - val_loss: 0.0870
Epoch 29/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1190 - val_loss: 0.0860
Epoch 30/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1185 - val_loss: 0.0887
Epoch 31/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1179 - val_loss: 0.0873
Epoch 32/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1176 - val_loss: 0.0867
Epoch 33/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1170 - val_loss: 0.0875
Epoch 34/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1165 - val_loss: 0.0856
Epoch 35/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1165 - val_loss: 0.0864
Epoch 36/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1160 - val_loss: 0.0858
Epoch 37/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1156 - val_loss: 0.0859
Epoch 38/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1153 - val_loss: 0.0857
Epoch 39/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1148 - val_loss: 0.0857
Epoch 40/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1145 - val_loss: 0.0857
Epoch 41/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1143 - val_loss: 0.0862
Epoch 42/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1139 - val_loss: 0.0843
Epoch 43/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1135 - val_loss: 0.0850
Epoch 44/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1131 - val_loss: 0.0843
Epoch 45/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1127 - val_loss: 0.0861
Epoch 46/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1124 - val_loss: 0.0845
Epoch 47/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1120 - val_loss: 0.0823
Epoch 48/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1119 - val_loss: 0.0849
Epoch 49/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1114 - val_loss: 0.0830
Epoch 50/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1111 - val_loss: 0.0845
Epoch 51/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1107 - val_loss: 0.0835
Epoch 52/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1104 - val_loss: 0.0837
Epoch 53/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1102 - val_loss: 0.0863
Epoch 54/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1098 - val_loss: 0.0829
Epoch 55/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1094 - val_loss: 0.0829
Epoch 56/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1090 - val_loss: 0.0843
Epoch 57/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1089 - val_loss: 0.0821
Epoch 58/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1083 - val_loss: 0.0828
Epoch 59/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1081 - val_loss: 0.0827
Epoch 60/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1079 - val_loss: 0.0835
Epoch 61/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1075 - val_loss: 0.0824
Epoch 62/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1070 - val_loss: 0.0819
Epoch 63/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1070 - val_loss: 0.0807
Epoch 64/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1067 - val_loss: 0.0828
Epoch 65/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1065 - val_loss: 0.0816
Epoch 66/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1060 - val_loss: 0.0807
Epoch 67/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1059 - val_loss: 0.0824
Epoch 68/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1055 - val_loss: 0.0845
Epoch 69/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1053 - val_loss: 0.0815
Epoch 70/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1052 - val_loss: 0.0814
Epoch 71/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1050 - val_loss: 0.0822
Epoch 72/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1046 - val_loss: 0.0833
Epoch 73/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1044 - val_loss: 0.0842
Epoch 74/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1042 - val_loss: 0.0804
Epoch 75/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1039 - val_loss: 0.0818
Epoch 76/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1036 - val_loss: 0.0814
Epoch 77/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1037 - val_loss: 0.0816
Epoch 78/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1036 - val_loss: 0.0838
Epoch 79/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1033 - val_loss: 0.0802
Epoch 80/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1030 - val_loss: 0.0807
Epoch 81/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1026 - val_loss: 0.0800
Epoch 82/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1026 - val_loss: 0.0810
Epoch 83/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1023 - val_loss: 0.0809
Epoch 84/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1023 - val_loss: 0.0820
Epoch 85/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1020 - val_loss: 0.0805
Epoch 86/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1017 - val_loss: 0.0805
Epoch 87/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1016 - val_loss: 0.0808
Epoch 88/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1014 - val_loss: 0.0815
Epoch 89/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1011 - val_loss: 0.0800
Epoch 90/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1008 - val_loss: 0.0806
Epoch 91/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1010 - val_loss: 0.0800
Epoch 92/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1006 - val_loss: 0.0797
Epoch 93/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1005 - val_loss: 0.0807
Epoch 94/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1002 - val_loss: 0.0820
Epoch 95/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1001 - val_loss: 0.0814
Epoch 96/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0999 - val_loss: 0.0813
Epoch 97/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0995 - val_loss: 0.0800
Epoch 98/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0996 - val_loss: 0.0801
Epoch 99/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0993 - val_loss: 0.0803
Epoch 100/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0990 - val_loss: 0.0799
Epoch 101/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0991 - val_loss: 0.0802
Epoch 102/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0989 - val_loss: 0.0807
Val_yp Shape is 
(75727, 24)
Results === Test == Validation ===== 54
=============================
step 1    -  0.992    0.991
step 2    -  0.985    0.984
step 3    -  0.977    0.975
step 4    -  0.968    0.965
step 5    -  0.958    0.955
step 6    -  0.947    0.946
step 7    -  0.938    0.936
step 8    -  0.927    0.927
step 9    -  0.919    0.919
step 10   -  0.910    0.911
step 11   -  0.901    0.903
step 12   -  0.894    0.896
step 13   -  0.887    0.889
step 14   -  0.879    0.882
step 15   -  0.873    0.877
step 16   -  0.868    0.872
step 17   -  0.861    0.866
step 18   -  0.856    0.861
step 19   -  0.850    0.856
step 20   -  0.846    0.852
step 21   -  0.843    0.850
step 22   -  0.840    0.847
step 23   -  0.836    0.844
step 24   -  0.831    0.840
=============================
Summary   -  21.584    21.646
V_y Shape is 
(75727, 1)
=========== SHAPE INPUT-OUTPUT MATRIXES =====================
shape x_training : (227219, 385)  y_training: :  (227219, 24)
shape x_test     :  (75726, 385)  y_test      :  (75726, 24)
shape x_val      :  (75727, 385)  y_val       :  (75727, 24)
=============================================================
(385,)
Model: "MLP"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_29 (InputLayer)       [(None, 385)]             0         
                                                                 
 dense_112 (Dense)           (None, 1024)              395264    
                                                                 
 elu_28 (ELU)                (None, 1024)              0         
                                                                 
 dropout_84 (Dropout)        (None, 1024)              0         
                                                                 
 dense_113 (Dense)           (None, 512)               524800    
                                                                 
 dropout_85 (Dropout)        (None, 512)               0         
                                                                 
 dense_114 (Dense)           (None, 512)               262656    
                                                                 
 dropout_86 (Dropout)        (None, 512)               0         
                                                                 
 dense_115 (Dense)           (None, 24)                12312     
                                                                 
=================================================================
Total params: 1,195,032
Trainable params: 1,195,032
Non-trainable params: 0
_________________________________________________________________
Epoch 1/200
222/222 [==============================] - 2s 8ms/step - loss: 0.4424 - val_loss: 0.1127
Epoch 2/200
222/222 [==============================] - 1s 6ms/step - loss: 0.2334 - val_loss: 0.1045
Epoch 3/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1970 - val_loss: 0.0980
Epoch 4/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1799 - val_loss: 0.0979
Epoch 5/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1684 - val_loss: 0.0957
Epoch 6/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1610 - val_loss: 0.0936
Epoch 7/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1549 - val_loss: 0.0954
Epoch 8/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1501 - val_loss: 0.0939
Epoch 9/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1460 - val_loss: 0.0913
Epoch 10/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1429 - val_loss: 0.0906
Epoch 11/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1398 - val_loss: 0.0910
Epoch 12/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1376 - val_loss: 0.0925
Epoch 13/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1353 - val_loss: 0.0924
Epoch 14/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1332 - val_loss: 0.0918
Epoch 15/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1317 - val_loss: 0.0919
Epoch 16/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1298 - val_loss: 0.0908
Epoch 17/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1286 - val_loss: 0.0897
Epoch 18/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1275 - val_loss: 0.0880
Epoch 19/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1259 - val_loss: 0.0885
Epoch 20/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1254 - val_loss: 0.0906
Epoch 21/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1241 - val_loss: 0.0889
Epoch 22/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1232 - val_loss: 0.0887
Epoch 23/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1224 - val_loss: 0.0884
Epoch 24/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1218 - val_loss: 0.0902
Epoch 25/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1211 - val_loss: 0.0888
Epoch 26/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1201 - val_loss: 0.0891
Epoch 27/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1198 - val_loss: 0.0876
Epoch 28/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1191 - val_loss: 0.0884
Epoch 29/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1188 - val_loss: 0.0865
Epoch 30/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1183 - val_loss: 0.0880
Epoch 31/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1176 - val_loss: 0.0877
Epoch 32/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1173 - val_loss: 0.0869
Epoch 33/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1169 - val_loss: 0.0871
Epoch 34/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1165 - val_loss: 0.0871
Epoch 35/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1162 - val_loss: 0.0859
Epoch 36/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1159 - val_loss: 0.0862
Epoch 37/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1154 - val_loss: 0.0865
Epoch 38/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1148 - val_loss: 0.0856
Epoch 39/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1147 - val_loss: 0.0865
Epoch 40/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1142 - val_loss: 0.0858
Epoch 41/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1139 - val_loss: 0.0854
Epoch 42/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1134 - val_loss: 0.0850
Epoch 43/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1132 - val_loss: 0.0840
Epoch 44/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1127 - val_loss: 0.0858
Epoch 45/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1125 - val_loss: 0.0831
Epoch 46/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1122 - val_loss: 0.0848
Epoch 47/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1118 - val_loss: 0.0839
Epoch 48/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1112 - val_loss: 0.0856
Epoch 49/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1111 - val_loss: 0.0832
Epoch 50/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1105 - val_loss: 0.0844
Epoch 51/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1104 - val_loss: 0.0868
Epoch 52/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1100 - val_loss: 0.0831
Epoch 53/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1094 - val_loss: 0.0839
Epoch 54/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1094 - val_loss: 0.0830
Epoch 55/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1089 - val_loss: 0.0834
Epoch 56/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1086 - val_loss: 0.0839
Epoch 57/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1081 - val_loss: 0.0838
Epoch 58/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1079 - val_loss: 0.0830
Epoch 59/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1079 - val_loss: 0.0831
Epoch 60/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1075 - val_loss: 0.0830
Epoch 61/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1072 - val_loss: 0.0821
Epoch 62/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1069 - val_loss: 0.0836
Epoch 63/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1067 - val_loss: 0.0821
Epoch 64/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1061 - val_loss: 0.0815
Epoch 65/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1061 - val_loss: 0.0831
Epoch 66/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1059 - val_loss: 0.0820
Epoch 67/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1057 - val_loss: 0.0824
Epoch 68/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1053 - val_loss: 0.0820
Epoch 69/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1051 - val_loss: 0.0821
Epoch 70/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1049 - val_loss: 0.0816
Epoch 71/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1046 - val_loss: 0.0820
Epoch 72/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1045 - val_loss: 0.0823
Epoch 73/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1039 - val_loss: 0.0820
Epoch 74/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1040 - val_loss: 0.0813
Epoch 75/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1034 - val_loss: 0.0836
Epoch 76/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1034 - val_loss: 0.0808
Epoch 77/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1032 - val_loss: 0.0812
Epoch 78/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1030 - val_loss: 0.0806
Epoch 79/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1028 - val_loss: 0.0818
Epoch 80/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1026 - val_loss: 0.0815
Epoch 81/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1025 - val_loss: 0.0808
Epoch 82/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1023 - val_loss: 0.0824
Epoch 83/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1020 - val_loss: 0.0818
Epoch 84/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1018 - val_loss: 0.0819
Epoch 85/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1015 - val_loss: 0.0808
Epoch 86/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1013 - val_loss: 0.0804
Epoch 87/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1012 - val_loss: 0.0812
Epoch 88/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1009 - val_loss: 0.0802
Epoch 89/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1009 - val_loss: 0.0802
Epoch 90/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1007 - val_loss: 0.0824
Epoch 91/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1006 - val_loss: 0.0795
Epoch 92/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1003 - val_loss: 0.0820
Epoch 93/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0999 - val_loss: 0.0806
Epoch 94/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0999 - val_loss: 0.0811
Epoch 95/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0997 - val_loss: 0.0811
Epoch 96/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0995 - val_loss: 0.0803
Epoch 97/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0994 - val_loss: 0.0813
Epoch 98/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0993 - val_loss: 0.0798
Epoch 99/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0992 - val_loss: 0.0822
Epoch 100/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0990 - val_loss: 0.0792
Epoch 101/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0987 - val_loss: 0.0811
Epoch 102/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0987 - val_loss: 0.0815
Epoch 103/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0985 - val_loss: 0.0788
Epoch 104/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0981 - val_loss: 0.0795
Epoch 105/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0979 - val_loss: 0.0806
Epoch 106/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0980 - val_loss: 0.0797
Epoch 107/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0976 - val_loss: 0.0807
Epoch 108/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0975 - val_loss: 0.0810
Epoch 109/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0975 - val_loss: 0.0800
Epoch 110/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0973 - val_loss: 0.0802
Epoch 111/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0971 - val_loss: 0.0807
Epoch 112/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0970 - val_loss: 0.0807
Epoch 113/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0968 - val_loss: 0.0792
Val_yp Shape is 
(75727, 24)
Results === Test == Validation ===== 55
=============================
step 1    -  0.992    0.991
step 2    -  0.985    0.984
step 3    -  0.977    0.975
step 4    -  0.967    0.965
step 5    -  0.957    0.955
step 6    -  0.947    0.946
step 7    -  0.938    0.937
step 8    -  0.928    0.929
step 9    -  0.920    0.921
step 10   -  0.913    0.914
step 11   -  0.906    0.908
step 12   -  0.898    0.901
step 13   -  0.892    0.895
step 14   -  0.885    0.888
step 15   -  0.879    0.883
step 16   -  0.873    0.877
step 17   -  0.868    0.873
step 18   -  0.863    0.868
step 19   -  0.858    0.863
step 20   -  0.853    0.859
step 21   -  0.847    0.853
step 22   -  0.842    0.850
step 23   -  0.837    0.845
step 24   -  0.831    0.840
=============================
Summary   -  21.657    21.720
V_y Shape is 
(75727, 1)
=========== SHAPE INPUT-OUTPUT MATRIXES =====================
shape x_training : (227218, 392)  y_training: :  (227218, 24)
shape x_test     :  (75726, 392)  y_test      :  (75726, 24)
shape x_val      :  (75726, 392)  y_val       :  (75726, 24)
=============================================================
(392,)
Model: "MLP"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_30 (InputLayer)       [(None, 392)]             0         
                                                                 
 dense_116 (Dense)           (None, 1024)              402432    
                                                                 
 elu_29 (ELU)                (None, 1024)              0         
                                                                 
 dropout_87 (Dropout)        (None, 1024)              0         
                                                                 
 dense_117 (Dense)           (None, 512)               524800    
                                                                 
 dropout_88 (Dropout)        (None, 512)               0         
                                                                 
 dense_118 (Dense)           (None, 512)               262656    
                                                                 
 dropout_89 (Dropout)        (None, 512)               0         
                                                                 
 dense_119 (Dense)           (None, 24)                12312     
                                                                 
=================================================================
Total params: 1,202,200
Trainable params: 1,202,200
Non-trainable params: 0
_________________________________________________________________
Epoch 1/200
222/222 [==============================] - 2s 8ms/step - loss: 0.4393 - val_loss: 0.1116
Epoch 2/200
222/222 [==============================] - 1s 6ms/step - loss: 0.2338 - val_loss: 0.1038
Epoch 3/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1975 - val_loss: 0.0996
Epoch 4/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1802 - val_loss: 0.0984
Epoch 5/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1688 - val_loss: 0.0964
Epoch 6/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1608 - val_loss: 0.0928
Epoch 7/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1550 - val_loss: 0.0921
Epoch 8/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1502 - val_loss: 0.0944
Epoch 9/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1461 - val_loss: 0.0925
Epoch 10/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1428 - val_loss: 0.0929
Epoch 11/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1399 - val_loss: 0.0934
Epoch 12/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1374 - val_loss: 0.0898
Epoch 13/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1352 - val_loss: 0.0917
Epoch 14/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1333 - val_loss: 0.0899
Epoch 15/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1318 - val_loss: 0.0894
Epoch 16/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1296 - val_loss: 0.0883
Epoch 17/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1285 - val_loss: 0.0906
Epoch 18/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1271 - val_loss: 0.0885
Epoch 19/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1259 - val_loss: 0.0887
Epoch 20/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1250 - val_loss: 0.0895
Epoch 21/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1239 - val_loss: 0.0906
Epoch 22/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1229 - val_loss: 0.0898
Epoch 23/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1221 - val_loss: 0.0886
Epoch 24/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1211 - val_loss: 0.0898
Epoch 25/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1206 - val_loss: 0.0895
Epoch 26/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1202 - val_loss: 0.0874
Epoch 27/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1196 - val_loss: 0.0891
Epoch 28/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1188 - val_loss: 0.0906
Epoch 29/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1186 - val_loss: 0.0857
Epoch 30/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1179 - val_loss: 0.0889
Epoch 31/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1172 - val_loss: 0.0871
Epoch 32/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1169 - val_loss: 0.0877
Epoch 33/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1167 - val_loss: 0.0870
Epoch 34/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1164 - val_loss: 0.0854
Epoch 35/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1157 - val_loss: 0.0872
Epoch 36/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1155 - val_loss: 0.0887
Epoch 37/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1150 - val_loss: 0.0854
Epoch 38/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1145 - val_loss: 0.0856
Epoch 39/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1143 - val_loss: 0.0857
Epoch 40/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1141 - val_loss: 0.0858
Epoch 41/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1137 - val_loss: 0.0844
Epoch 42/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1130 - val_loss: 0.0855
Epoch 43/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1128 - val_loss: 0.0847
Epoch 44/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1123 - val_loss: 0.0847
Epoch 45/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1120 - val_loss: 0.0840
Epoch 46/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1118 - val_loss: 0.0842
Epoch 47/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1112 - val_loss: 0.0847
Epoch 48/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1108 - val_loss: 0.0844
Epoch 49/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1106 - val_loss: 0.0840
Epoch 50/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1103 - val_loss: 0.0823
Epoch 51/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1098 - val_loss: 0.0835
Epoch 52/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1095 - val_loss: 0.0832
Epoch 53/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1090 - val_loss: 0.0838
Epoch 54/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1088 - val_loss: 0.0829
Epoch 55/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1085 - val_loss: 0.0840
Epoch 56/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1081 - val_loss: 0.0817
Epoch 57/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1079 - val_loss: 0.0823
Epoch 58/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1076 - val_loss: 0.0836
Epoch 59/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1072 - val_loss: 0.0818
Epoch 60/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1069 - val_loss: 0.0834
Epoch 61/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1067 - val_loss: 0.0835
Epoch 62/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1063 - val_loss: 0.0824
Epoch 63/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1061 - val_loss: 0.0813
Epoch 64/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1060 - val_loss: 0.0821
Epoch 65/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1055 - val_loss: 0.0825
Epoch 66/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1053 - val_loss: 0.0819
Epoch 67/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1051 - val_loss: 0.0818
Epoch 68/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1048 - val_loss: 0.0816
Epoch 69/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1047 - val_loss: 0.0820
Epoch 70/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1044 - val_loss: 0.0823
Epoch 71/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1040 - val_loss: 0.0822
Epoch 72/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1038 - val_loss: 0.0820
Epoch 73/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1037 - val_loss: 0.0806
Epoch 74/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1033 - val_loss: 0.0813
Epoch 75/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1032 - val_loss: 0.0813
Epoch 76/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1031 - val_loss: 0.0819
Epoch 77/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1026 - val_loss: 0.0819
Epoch 78/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1025 - val_loss: 0.0808
Epoch 79/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1023 - val_loss: 0.0821
Epoch 80/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1021 - val_loss: 0.0795
Epoch 81/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1018 - val_loss: 0.0804
Epoch 82/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1017 - val_loss: 0.0822
Epoch 83/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1012 - val_loss: 0.0804
Epoch 84/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1011 - val_loss: 0.0808
Epoch 85/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1010 - val_loss: 0.0824
Epoch 86/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1008 - val_loss: 0.0818
Epoch 87/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1006 - val_loss: 0.0796
Epoch 88/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1003 - val_loss: 0.0812
Epoch 89/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1003 - val_loss: 0.0823
Epoch 90/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1002 - val_loss: 0.0812
Val_yp Shape is 
(75726, 24)
Results === Test == Validation ===== 56
=============================
step 1    -  0.991    0.990
step 2    -  0.984    0.983
step 3    -  0.975    0.973
step 4    -  0.965    0.963
step 5    -  0.955    0.954
step 6    -  0.946    0.944
step 7    -  0.936    0.935
step 8    -  0.927    0.927
step 9    -  0.918    0.919
step 10   -  0.910    0.912
step 11   -  0.902    0.904
step 12   -  0.895    0.897
step 13   -  0.889    0.891
step 14   -  0.883    0.885
step 15   -  0.876    0.879
step 16   -  0.871    0.874
step 17   -  0.865    0.869
step 18   -  0.860    0.864
step 19   -  0.855    0.859
step 20   -  0.849    0.855
step 21   -  0.845    0.851
step 22   -  0.839    0.846
step 23   -  0.834    0.841
step 24   -  0.829    0.837
=============================
Summary   -  21.598    21.653
V_y Shape is 
(75726, 1)
=========== SHAPE INPUT-OUTPUT MATRIXES =====================
shape x_training : (227217, 399)  y_training: :  (227217, 24)
shape x_test     :  (75725, 399)  y_test      :  (75725, 24)
shape x_val      :  (75726, 399)  y_val       :  (75726, 24)
=============================================================
(399,)
Model: "MLP"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_31 (InputLayer)       [(None, 399)]             0         
                                                                 
 dense_120 (Dense)           (None, 1024)              409600    
                                                                 
 elu_30 (ELU)                (None, 1024)              0         
                                                                 
 dropout_90 (Dropout)        (None, 1024)              0         
                                                                 
 dense_121 (Dense)           (None, 512)               524800    
                                                                 
 dropout_91 (Dropout)        (None, 512)               0         
                                                                 
 dense_122 (Dense)           (None, 512)               262656    
                                                                 
 dropout_92 (Dropout)        (None, 512)               0         
                                                                 
 dense_123 (Dense)           (None, 24)                12312     
                                                                 
=================================================================
Total params: 1,209,368
Trainable params: 1,209,368
Non-trainable params: 0
_________________________________________________________________
Epoch 1/200
222/222 [==============================] - 2s 8ms/step - loss: 0.4445 - val_loss: 0.1138
Epoch 2/200
222/222 [==============================] - 1s 6ms/step - loss: 0.2342 - val_loss: 0.1035
Epoch 3/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1978 - val_loss: 0.0991
Epoch 4/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1801 - val_loss: 0.0987
Epoch 5/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1690 - val_loss: 0.0956
Epoch 6/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1609 - val_loss: 0.0956
Epoch 7/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1547 - val_loss: 0.0932
Epoch 8/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1496 - val_loss: 0.0924
Epoch 9/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1459 - val_loss: 0.0913
Epoch 10/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1425 - val_loss: 0.0919
Epoch 11/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1398 - val_loss: 0.0942
Epoch 12/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1371 - val_loss: 0.0912
Epoch 13/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1349 - val_loss: 0.0894
Epoch 14/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1331 - val_loss: 0.0894
Epoch 15/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1312 - val_loss: 0.0885
Epoch 16/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1296 - val_loss: 0.0907
Epoch 17/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1282 - val_loss: 0.0919
Epoch 18/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1271 - val_loss: 0.0898
Epoch 19/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1255 - val_loss: 0.0898
Epoch 20/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1247 - val_loss: 0.0887
Epoch 21/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1237 - val_loss: 0.0866
Epoch 22/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1226 - val_loss: 0.0912
Epoch 23/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1220 - val_loss: 0.0880
Epoch 24/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1212 - val_loss: 0.0900
Epoch 25/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1206 - val_loss: 0.0878
Epoch 26/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1199 - val_loss: 0.0870
Epoch 27/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1193 - val_loss: 0.0874
Epoch 28/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1188 - val_loss: 0.0891
Epoch 29/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1183 - val_loss: 0.0865
Epoch 30/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1177 - val_loss: 0.0870
Epoch 31/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1173 - val_loss: 0.0869
Epoch 32/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1168 - val_loss: 0.0867
Epoch 33/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1163 - val_loss: 0.0859
Epoch 34/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1161 - val_loss: 0.0867
Epoch 35/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1158 - val_loss: 0.0864
Epoch 36/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1153 - val_loss: 0.0879
Epoch 37/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1149 - val_loss: 0.0862
Epoch 38/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1145 - val_loss: 0.0865
Epoch 39/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1143 - val_loss: 0.0852
Epoch 40/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1138 - val_loss: 0.0856
Epoch 41/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1134 - val_loss: 0.0854
Epoch 42/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1133 - val_loss: 0.0858
Epoch 43/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1127 - val_loss: 0.0882
Epoch 44/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1124 - val_loss: 0.0862
Epoch 45/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1120 - val_loss: 0.0847
Epoch 46/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1119 - val_loss: 0.0850
Epoch 47/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1114 - val_loss: 0.0871
Epoch 48/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1111 - val_loss: 0.0850
Epoch 49/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1105 - val_loss: 0.0849
Epoch 50/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1103 - val_loss: 0.0842
Epoch 51/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1100 - val_loss: 0.0830
Epoch 52/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1096 - val_loss: 0.0837
Epoch 53/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1093 - val_loss: 0.0834
Epoch 54/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1088 - val_loss: 0.0837
Epoch 55/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1086 - val_loss: 0.0819
Epoch 56/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1083 - val_loss: 0.0821
Epoch 57/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1080 - val_loss: 0.0835
Epoch 58/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1076 - val_loss: 0.0816
Epoch 59/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1074 - val_loss: 0.0825
Epoch 60/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1070 - val_loss: 0.0823
Epoch 61/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1066 - val_loss: 0.0824
Epoch 62/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1064 - val_loss: 0.0811
Epoch 63/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1062 - val_loss: 0.0817
Epoch 64/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1056 - val_loss: 0.0834
Epoch 65/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1055 - val_loss: 0.0823
Epoch 66/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1055 - val_loss: 0.0813
Epoch 67/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1049 - val_loss: 0.0820
Epoch 68/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1047 - val_loss: 0.0808
Epoch 69/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1043 - val_loss: 0.0827
Epoch 70/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1043 - val_loss: 0.0813
Epoch 71/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1039 - val_loss: 0.0816
Epoch 72/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1038 - val_loss: 0.0810
Epoch 73/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1033 - val_loss: 0.0818
Epoch 74/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1033 - val_loss: 0.0810
Epoch 75/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1029 - val_loss: 0.0820
Epoch 76/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1026 - val_loss: 0.0816
Epoch 77/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1025 - val_loss: 0.0809
Epoch 78/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1024 - val_loss: 0.0815
Val_yp Shape is 
(75726, 24)
Results === Test == Validation ===== 57
=============================
step 1    -  0.991    0.990
step 2    -  0.984    0.983
step 3    -  0.975    0.973
step 4    -  0.966    0.964
step 5    -  0.957    0.955
step 6    -  0.947    0.946
step 7    -  0.938    0.938
step 8    -  0.929    0.930
step 9    -  0.922    0.922
step 10   -  0.914    0.915
step 11   -  0.907    0.909
step 12   -  0.900    0.902
step 13   -  0.894    0.896
step 14   -  0.887    0.890
step 15   -  0.879    0.883
step 16   -  0.873    0.877
step 17   -  0.866    0.870
step 18   -  0.859    0.864
step 19   -  0.850    0.855
step 20   -  0.844    0.849
step 21   -  0.837    0.844
step 22   -  0.833    0.840
step 23   -  0.826    0.834
step 24   -  0.820    0.829
=============================
Summary   -  21.599    21.659
V_y Shape is 
(75726, 1)
=========== SHAPE INPUT-OUTPUT MATRIXES =====================
shape x_training : (227216, 406)  y_training: :  (227216, 24)
shape x_test     :  (75725, 406)  y_test      :  (75725, 24)
shape x_val      :  (75725, 406)  y_val       :  (75725, 24)
=============================================================
(406,)
Model: "MLP"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_32 (InputLayer)       [(None, 406)]             0         
                                                                 
 dense_124 (Dense)           (None, 1024)              416768    
                                                                 
 elu_31 (ELU)                (None, 1024)              0         
                                                                 
 dropout_93 (Dropout)        (None, 1024)              0         
                                                                 
 dense_125 (Dense)           (None, 512)               524800    
                                                                 
 dropout_94 (Dropout)        (None, 512)               0         
                                                                 
 dense_126 (Dense)           (None, 512)               262656    
                                                                 
 dropout_95 (Dropout)        (None, 512)               0         
                                                                 
 dense_127 (Dense)           (None, 24)                12312     
                                                                 
=================================================================
Total params: 1,216,536
Trainable params: 1,216,536
Non-trainable params: 0
_________________________________________________________________
Epoch 1/200
222/222 [==============================] - 2s 7ms/step - loss: 0.4426 - val_loss: 0.1122
Epoch 2/200
222/222 [==============================] - 1s 6ms/step - loss: 0.2338 - val_loss: 0.1033
Epoch 3/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1972 - val_loss: 0.0991
Epoch 4/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1796 - val_loss: 0.0983
Epoch 5/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1690 - val_loss: 0.0955
Epoch 6/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1607 - val_loss: 0.0973
Epoch 7/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1545 - val_loss: 0.0938
Epoch 8/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1499 - val_loss: 0.0954
Epoch 9/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1457 - val_loss: 0.0921
Epoch 10/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1424 - val_loss: 0.0917
Epoch 11/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1398 - val_loss: 0.0912
Epoch 12/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1370 - val_loss: 0.0902
Epoch 13/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1350 - val_loss: 0.0922
Epoch 14/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1328 - val_loss: 0.0912
Epoch 15/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1310 - val_loss: 0.0916
Epoch 16/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1296 - val_loss: 0.0915
Epoch 17/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1279 - val_loss: 0.0895
Epoch 18/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1267 - val_loss: 0.0904
Epoch 19/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1257 - val_loss: 0.0886
Epoch 20/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1244 - val_loss: 0.0884
Epoch 21/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1236 - val_loss: 0.0898
Epoch 22/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1224 - val_loss: 0.0879
Epoch 23/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1218 - val_loss: 0.0882
Epoch 24/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1210 - val_loss: 0.0878
Epoch 25/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1203 - val_loss: 0.0876
Epoch 26/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1197 - val_loss: 0.0889
Epoch 27/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1191 - val_loss: 0.0887
Epoch 28/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1186 - val_loss: 0.0882
Epoch 29/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1181 - val_loss: 0.0874
Epoch 30/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1176 - val_loss: 0.0899
Epoch 31/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1170 - val_loss: 0.0858
Epoch 32/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1169 - val_loss: 0.0870
Epoch 33/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1163 - val_loss: 0.0866
Epoch 34/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1159 - val_loss: 0.0865
Epoch 35/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1157 - val_loss: 0.0855
Epoch 36/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1152 - val_loss: 0.0869
Epoch 37/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1145 - val_loss: 0.0859
Epoch 38/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1145 - val_loss: 0.0878
Epoch 39/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1140 - val_loss: 0.0864
Epoch 40/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1135 - val_loss: 0.0861
Epoch 41/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1134 - val_loss: 0.0859
Epoch 42/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1129 - val_loss: 0.0859
Epoch 43/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1127 - val_loss: 0.0864
Epoch 44/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1123 - val_loss: 0.0843
Epoch 45/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1120 - val_loss: 0.0852
Epoch 46/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1115 - val_loss: 0.0870
Epoch 47/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1110 - val_loss: 0.0847
Epoch 48/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1106 - val_loss: 0.0856
Epoch 49/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1102 - val_loss: 0.0830
Epoch 50/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1101 - val_loss: 0.0832
Epoch 51/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1095 - val_loss: 0.0832
Epoch 52/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1093 - val_loss: 0.0826
Epoch 53/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1088 - val_loss: 0.0831
Epoch 54/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1087 - val_loss: 0.0839
Epoch 55/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1082 - val_loss: 0.0842
Epoch 56/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1079 - val_loss: 0.0822
Epoch 57/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1075 - val_loss: 0.0841
Epoch 58/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1069 - val_loss: 0.0835
Epoch 59/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1069 - val_loss: 0.0837
Epoch 60/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1066 - val_loss: 0.0818
Epoch 61/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1060 - val_loss: 0.0819
Epoch 62/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1059 - val_loss: 0.0839
Epoch 63/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1058 - val_loss: 0.0834
Epoch 64/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1052 - val_loss: 0.0821
Epoch 65/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1051 - val_loss: 0.0813
Epoch 66/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1048 - val_loss: 0.0830
Epoch 67/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1046 - val_loss: 0.0815
Epoch 68/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1044 - val_loss: 0.0807
Epoch 69/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1040 - val_loss: 0.0814
Epoch 70/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1037 - val_loss: 0.0813
Epoch 71/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1034 - val_loss: 0.0806
Epoch 72/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1032 - val_loss: 0.0812
Epoch 73/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1030 - val_loss: 0.0816
Epoch 74/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1028 - val_loss: 0.0809
Epoch 75/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1028 - val_loss: 0.0806
Epoch 76/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1024 - val_loss: 0.0819
Epoch 77/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1021 - val_loss: 0.0810
Epoch 78/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1019 - val_loss: 0.0814
Epoch 79/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1016 - val_loss: 0.0819
Epoch 80/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1013 - val_loss: 0.0817
Epoch 81/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1014 - val_loss: 0.0812
Epoch 82/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1012 - val_loss: 0.0815
Epoch 83/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1008 - val_loss: 0.0817
Epoch 84/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1006 - val_loss: 0.0801
Epoch 85/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1003 - val_loss: 0.0807
Epoch 86/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0999 - val_loss: 0.0806
Epoch 87/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0999 - val_loss: 0.0807
Epoch 88/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0999 - val_loss: 0.0802
Epoch 89/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0996 - val_loss: 0.0806
Epoch 90/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0994 - val_loss: 0.0808
Epoch 91/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0993 - val_loss: 0.0820
Epoch 92/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0990 - val_loss: 0.0824
Epoch 93/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0988 - val_loss: 0.0815
Epoch 94/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0988 - val_loss: 0.0795
Epoch 95/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0987 - val_loss: 0.0794
Epoch 96/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0983 - val_loss: 0.0799
Epoch 97/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0982 - val_loss: 0.0811
Epoch 98/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0979 - val_loss: 0.0806
Epoch 99/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0978 - val_loss: 0.0830
Epoch 100/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0975 - val_loss: 0.0815
Epoch 101/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0975 - val_loss: 0.0793
Epoch 102/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0973 - val_loss: 0.0827
Epoch 103/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0972 - val_loss: 0.0803
Epoch 104/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0968 - val_loss: 0.0804
Epoch 105/200
222/222 [==============================] - 1s 5ms/step - loss: 0.0969 - val_loss: 0.0801
Epoch 106/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0965 - val_loss: 0.0809
Epoch 107/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0964 - val_loss: 0.0815
Epoch 108/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0961 - val_loss: 0.0803
Epoch 109/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0961 - val_loss: 0.0794
Epoch 110/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0961 - val_loss: 0.0800
Epoch 111/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0959 - val_loss: 0.0808
Val_yp Shape is 
(75725, 24)
Results === Test == Validation ===== 58
=============================
step 1    -  0.992    0.991
step 2    -  0.985    0.983
step 3    -  0.977    0.975
step 4    -  0.968    0.966
step 5    -  0.959    0.957
step 6    -  0.949    0.948
step 7    -  0.940    0.939
step 8    -  0.931    0.931
step 9    -  0.922    0.923
step 10   -  0.914    0.916
step 11   -  0.906    0.908
step 12   -  0.899    0.901
step 13   -  0.893    0.896
step 14   -  0.887    0.890
step 15   -  0.881    0.884
step 16   -  0.874    0.878
step 17   -  0.868    0.872
step 18   -  0.862    0.867
step 19   -  0.857    0.862
step 20   -  0.853    0.858
step 21   -  0.846    0.852
step 22   -  0.841    0.848
step 23   -  0.837    0.844
step 24   -  0.830    0.838
=============================
Summary   -  21.671    21.727
V_y Shape is 
(75725, 1)
=========== SHAPE INPUT-OUTPUT MATRIXES =====================
shape x_training : (227215, 413)  y_training: :  (227215, 24)
shape x_test     :  (75724, 413)  y_test      :  (75724, 24)
shape x_val      :  (75725, 413)  y_val       :  (75725, 24)
=============================================================
(413,)
Model: "MLP"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_33 (InputLayer)       [(None, 413)]             0         
                                                                 
 dense_128 (Dense)           (None, 1024)              423936    
                                                                 
 elu_32 (ELU)                (None, 1024)              0         
                                                                 
 dropout_96 (Dropout)        (None, 1024)              0         
                                                                 
 dense_129 (Dense)           (None, 512)               524800    
                                                                 
 dropout_97 (Dropout)        (None, 512)               0         
                                                                 
 dense_130 (Dense)           (None, 512)               262656    
                                                                 
 dropout_98 (Dropout)        (None, 512)               0         
                                                                 
 dense_131 (Dense)           (None, 24)                12312     
                                                                 
=================================================================
Total params: 1,223,704
Trainable params: 1,223,704
Non-trainable params: 0
_________________________________________________________________
Epoch 1/200
222/222 [==============================] - 2s 8ms/step - loss: 0.4591 - val_loss: 0.1118
Epoch 2/200
222/222 [==============================] - 1s 6ms/step - loss: 0.2372 - val_loss: 0.1023
Epoch 3/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1998 - val_loss: 0.0986
Epoch 4/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1811 - val_loss: 0.0983
Epoch 5/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1696 - val_loss: 0.0976
Epoch 6/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1622 - val_loss: 0.0950
Epoch 7/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1559 - val_loss: 0.0939
Epoch 8/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1507 - val_loss: 0.0930
Epoch 9/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1463 - val_loss: 0.0937
Epoch 10/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1432 - val_loss: 0.0917
Epoch 11/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1401 - val_loss: 0.0916
Epoch 12/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1378 - val_loss: 0.0898
Epoch 13/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1352 - val_loss: 0.0893
Epoch 14/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1331 - val_loss: 0.0912
Epoch 15/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1313 - val_loss: 0.0891
Epoch 16/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1298 - val_loss: 0.0918
Epoch 17/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1282 - val_loss: 0.0888
Epoch 18/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1270 - val_loss: 0.0886
Epoch 19/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1257 - val_loss: 0.0907
Epoch 20/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1248 - val_loss: 0.0893
Epoch 21/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1235 - val_loss: 0.0875
Epoch 22/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1226 - val_loss: 0.0881
Epoch 23/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1218 - val_loss: 0.0885
Epoch 24/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1211 - val_loss: 0.0885
Epoch 25/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1202 - val_loss: 0.0884
Epoch 26/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1199 - val_loss: 0.0892
Epoch 27/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1192 - val_loss: 0.0891
Epoch 28/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1187 - val_loss: 0.0881
Epoch 29/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1181 - val_loss: 0.0868
Epoch 30/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1175 - val_loss: 0.0862
Epoch 31/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1172 - val_loss: 0.0886
Epoch 32/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1167 - val_loss: 0.0859
Epoch 33/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1162 - val_loss: 0.0872
Epoch 34/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1159 - val_loss: 0.0878
Epoch 35/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1156 - val_loss: 0.0860
Epoch 36/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1149 - val_loss: 0.0869
Epoch 37/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1147 - val_loss: 0.0846
Epoch 38/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1141 - val_loss: 0.0851
Epoch 39/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1139 - val_loss: 0.0862
Epoch 40/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1136 - val_loss: 0.0852
Epoch 41/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1133 - val_loss: 0.0847
Epoch 42/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1126 - val_loss: 0.0836
Epoch 43/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1123 - val_loss: 0.0846
Epoch 44/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1119 - val_loss: 0.0856
Epoch 45/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1119 - val_loss: 0.0845
Epoch 46/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1114 - val_loss: 0.0834
Epoch 47/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1107 - val_loss: 0.0836
Epoch 48/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1106 - val_loss: 0.0839
Epoch 49/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1103 - val_loss: 0.0843
Epoch 50/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1099 - val_loss: 0.0845
Epoch 51/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1095 - val_loss: 0.0838
Epoch 52/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1090 - val_loss: 0.0836
Epoch 53/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1090 - val_loss: 0.0841
Epoch 54/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1087 - val_loss: 0.0835
Epoch 55/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1080 - val_loss: 0.0826
Epoch 56/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1077 - val_loss: 0.0835
Epoch 57/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1075 - val_loss: 0.0844
Epoch 58/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1071 - val_loss: 0.0826
Epoch 59/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1069 - val_loss: 0.0830
Epoch 60/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1065 - val_loss: 0.0838
Epoch 61/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1062 - val_loss: 0.0827
Epoch 62/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1058 - val_loss: 0.0820
Epoch 63/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1056 - val_loss: 0.0814
Epoch 64/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1053 - val_loss: 0.0832
Epoch 65/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1049 - val_loss: 0.0814
Epoch 66/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1047 - val_loss: 0.0813
Epoch 67/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1044 - val_loss: 0.0812
Epoch 68/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1041 - val_loss: 0.0817
Epoch 69/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1040 - val_loss: 0.0814
Epoch 70/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1036 - val_loss: 0.0817
Epoch 71/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1034 - val_loss: 0.0825
Epoch 72/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1032 - val_loss: 0.0816
Epoch 73/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1030 - val_loss: 0.0831
Epoch 74/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1025 - val_loss: 0.0817
Epoch 75/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1025 - val_loss: 0.0806
Epoch 76/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1019 - val_loss: 0.0799
Epoch 77/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1020 - val_loss: 0.0827
Epoch 78/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1016 - val_loss: 0.0812
Epoch 79/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1014 - val_loss: 0.0815
Epoch 80/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1013 - val_loss: 0.0818
Epoch 81/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1010 - val_loss: 0.0813
Epoch 82/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1007 - val_loss: 0.0802
Epoch 83/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1008 - val_loss: 0.0819
Epoch 84/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1002 - val_loss: 0.0804
Epoch 85/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1000 - val_loss: 0.0802
Epoch 86/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0996 - val_loss: 0.0806
Val_yp Shape is 
(75725, 24)
Results === Test == Validation ===== 59
=============================
step 1    -  0.991    0.990
step 2    -  0.984    0.983
step 3    -  0.976    0.974
step 4    -  0.966    0.964
step 5    -  0.957    0.955
step 6    -  0.947    0.945
step 7    -  0.938    0.937
step 8    -  0.929    0.928
step 9    -  0.921    0.921
step 10   -  0.913    0.914
step 11   -  0.906    0.907
step 12   -  0.898    0.900
step 13   -  0.890    0.892
step 14   -  0.883    0.885
step 15   -  0.878    0.880
step 16   -  0.870    0.873
step 17   -  0.864    0.867
step 18   -  0.859    0.863
step 19   -  0.854    0.859
step 20   -  0.849    0.854
step 21   -  0.843    0.849
step 22   -  0.840    0.847
step 23   -  0.834    0.843
step 24   -  0.830    0.838
=============================
Summary   -  21.623    21.668
V_y Shape is 
(75725, 1)
=========== SHAPE INPUT-OUTPUT MATRIXES =====================
shape x_training : (227214, 420)  y_training: :  (227214, 24)
shape x_test     :  (75724, 420)  y_test      :  (75724, 24)
shape x_val      :  (75724, 420)  y_val       :  (75724, 24)
=============================================================
(420,)
Model: "MLP"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_34 (InputLayer)       [(None, 420)]             0         
                                                                 
 dense_132 (Dense)           (None, 1024)              431104    
                                                                 
 elu_33 (ELU)                (None, 1024)              0         
                                                                 
 dropout_99 (Dropout)        (None, 1024)              0         
                                                                 
 dense_133 (Dense)           (None, 512)               524800    
                                                                 
 dropout_100 (Dropout)       (None, 512)               0         
                                                                 
 dense_134 (Dense)           (None, 512)               262656    
                                                                 
 dropout_101 (Dropout)       (None, 512)               0         
                                                                 
 dense_135 (Dense)           (None, 24)                12312     
                                                                 
=================================================================
Total params: 1,230,872
Trainable params: 1,230,872
Non-trainable params: 0
_________________________________________________________________
Epoch 1/200
222/222 [==============================] - 2s 6ms/step - loss: 0.4430 - val_loss: 0.1120
Epoch 2/200
222/222 [==============================] - 1s 6ms/step - loss: 0.2351 - val_loss: 0.1027
Epoch 3/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1982 - val_loss: 0.0997
Epoch 4/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1802 - val_loss: 0.0979
Epoch 5/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1689 - val_loss: 0.0962
Epoch 6/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1609 - val_loss: 0.0939
Epoch 7/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1546 - val_loss: 0.0942
Epoch 8/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1497 - val_loss: 0.0944
Epoch 9/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1458 - val_loss: 0.0937
Epoch 10/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1426 - val_loss: 0.0916
Epoch 11/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1395 - val_loss: 0.0918
Epoch 12/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1367 - val_loss: 0.0915
Epoch 13/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1344 - val_loss: 0.0905
Epoch 14/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1325 - val_loss: 0.0897
Epoch 15/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1308 - val_loss: 0.0897
Epoch 16/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1290 - val_loss: 0.0931
Epoch 17/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1277 - val_loss: 0.0935
Epoch 18/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1262 - val_loss: 0.0891
Epoch 19/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1249 - val_loss: 0.0889
Epoch 20/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1240 - val_loss: 0.0890
Epoch 21/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1231 - val_loss: 0.0887
Epoch 22/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1221 - val_loss: 0.0889
Epoch 23/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1211 - val_loss: 0.0890
Epoch 24/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1206 - val_loss: 0.0864
Epoch 25/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1200 - val_loss: 0.0881
Epoch 26/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1192 - val_loss: 0.0874
Epoch 27/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1189 - val_loss: 0.0858
Epoch 28/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1182 - val_loss: 0.0874
Epoch 29/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1177 - val_loss: 0.0888
Epoch 30/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1171 - val_loss: 0.0876
Epoch 31/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1166 - val_loss: 0.0868
Epoch 32/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1162 - val_loss: 0.0857
Epoch 33/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1158 - val_loss: 0.0895
Epoch 34/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1154 - val_loss: 0.0852
Epoch 35/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1151 - val_loss: 0.0852
Epoch 36/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1145 - val_loss: 0.0852
Epoch 37/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1142 - val_loss: 0.0880
Epoch 38/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1139 - val_loss: 0.0858
Epoch 39/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1134 - val_loss: 0.0867
Epoch 40/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1129 - val_loss: 0.0848
Epoch 41/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1129 - val_loss: 0.0852
Epoch 42/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1122 - val_loss: 0.0854
Epoch 43/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1119 - val_loss: 0.0842
Epoch 44/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1114 - val_loss: 0.0847
Epoch 45/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1112 - val_loss: 0.0845
Epoch 46/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1107 - val_loss: 0.0837
Epoch 47/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1104 - val_loss: 0.0828
Epoch 48/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1103 - val_loss: 0.0866
Epoch 49/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1096 - val_loss: 0.0835
Epoch 50/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1091 - val_loss: 0.0821
Epoch 51/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1088 - val_loss: 0.0851
Epoch 52/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1083 - val_loss: 0.0830
Epoch 53/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1081 - val_loss: 0.0845
Epoch 54/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1078 - val_loss: 0.0820
Epoch 55/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1074 - val_loss: 0.0832
Epoch 56/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1069 - val_loss: 0.0823
Epoch 57/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1068 - val_loss: 0.0834
Epoch 58/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1064 - val_loss: 0.0825
Epoch 59/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1060 - val_loss: 0.0828
Epoch 60/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1057 - val_loss: 0.0832
Epoch 61/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1055 - val_loss: 0.0839
Epoch 62/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1050 - val_loss: 0.0827
Epoch 63/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1049 - val_loss: 0.0833
Epoch 64/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1046 - val_loss: 0.0826
Val_yp Shape is 
(75724, 24)
Results === Test == Validation ===== 60
=============================
step 1    -  0.991    0.990
step 2    -  0.984    0.982
step 3    -  0.975    0.973
step 4    -  0.965    0.963
step 5    -  0.955    0.953
step 6    -  0.945    0.943
step 7    -  0.936    0.935
step 8    -  0.926    0.925
step 9    -  0.918    0.918
step 10   -  0.910    0.911
step 11   -  0.903    0.904
step 12   -  0.897    0.898
step 13   -  0.890    0.892
step 14   -  0.884    0.885
step 15   -  0.876    0.879
step 16   -  0.871    0.873
step 17   -  0.865    0.869
step 18   -  0.860    0.864
step 19   -  0.855    0.860
step 20   -  0.849    0.855
step 21   -  0.844    0.851
step 22   -  0.839    0.847
step 23   -  0.835    0.843
step 24   -  0.829    0.838
=============================
Summary   -  21.601    21.648
V_y Shape is 
(75724, 1)
=========== SHAPE INPUT-OUTPUT MATRIXES =====================
shape x_training : (227213, 427)  y_training: :  (227213, 24)
shape x_test     :  (75723, 427)  y_test      :  (75723, 24)
shape x_val      :  (75724, 427)  y_val       :  (75724, 24)
=============================================================
(427,)
Model: "MLP"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_35 (InputLayer)       [(None, 427)]             0         
                                                                 
 dense_136 (Dense)           (None, 1024)              438272    
                                                                 
 elu_34 (ELU)                (None, 1024)              0         
                                                                 
 dropout_102 (Dropout)       (None, 1024)              0         
                                                                 
 dense_137 (Dense)           (None, 512)               524800    
                                                                 
 dropout_103 (Dropout)       (None, 512)               0         
                                                                 
 dense_138 (Dense)           (None, 512)               262656    
                                                                 
 dropout_104 (Dropout)       (None, 512)               0         
                                                                 
 dense_139 (Dense)           (None, 24)                12312     
                                                                 
=================================================================
Total params: 1,238,040
Trainable params: 1,238,040
Non-trainable params: 0
_________________________________________________________________
Epoch 1/200
222/222 [==============================] - 2s 7ms/step - loss: 0.4501 - val_loss: 0.1119
Epoch 2/200
222/222 [==============================] - 1s 6ms/step - loss: 0.2353 - val_loss: 0.1034
Epoch 3/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1984 - val_loss: 0.1001
Epoch 4/200
222/222 [==============================] - 1s 5ms/step - loss: 0.1801 - val_loss: 0.0999
Epoch 5/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1689 - val_loss: 0.0943
Epoch 6/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1610 - val_loss: 0.0933
Epoch 7/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1550 - val_loss: 0.0926
Epoch 8/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1498 - val_loss: 0.0925
Epoch 9/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1457 - val_loss: 0.0921
Epoch 10/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1424 - val_loss: 0.0920
Epoch 11/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1393 - val_loss: 0.0930
Epoch 12/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1364 - val_loss: 0.0927
Epoch 13/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1345 - val_loss: 0.0912
Epoch 14/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1326 - val_loss: 0.0917
Epoch 15/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1304 - val_loss: 0.0887
Epoch 16/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1292 - val_loss: 0.0895
Epoch 17/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1275 - val_loss: 0.0878
Epoch 18/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1262 - val_loss: 0.0896
Epoch 19/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1251 - val_loss: 0.0899
Epoch 20/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1239 - val_loss: 0.0895
Epoch 21/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1231 - val_loss: 0.0887
Epoch 22/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1221 - val_loss: 0.0889
Epoch 23/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1211 - val_loss: 0.0876
Epoch 24/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1206 - val_loss: 0.0919
Epoch 25/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1200 - val_loss: 0.0888
Epoch 26/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1191 - val_loss: 0.0883
Epoch 27/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1187 - val_loss: 0.0864
Epoch 28/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1180 - val_loss: 0.0868
Epoch 29/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1176 - val_loss: 0.0875
Epoch 30/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1168 - val_loss: 0.0871
Epoch 31/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1166 - val_loss: 0.0873
Epoch 32/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1161 - val_loss: 0.0875
Epoch 33/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1156 - val_loss: 0.0865
Epoch 34/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1152 - val_loss: 0.0873
Epoch 35/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1149 - val_loss: 0.0874
Epoch 36/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1146 - val_loss: 0.0862
Epoch 37/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1141 - val_loss: 0.0870
Epoch 38/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1136 - val_loss: 0.0862
Epoch 39/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1132 - val_loss: 0.0871
Epoch 40/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1128 - val_loss: 0.0855
Epoch 41/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1126 - val_loss: 0.0846
Epoch 42/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1119 - val_loss: 0.0854
Epoch 43/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1116 - val_loss: 0.0851
Epoch 44/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1114 - val_loss: 0.0852
Epoch 45/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1109 - val_loss: 0.0852
Epoch 46/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1107 - val_loss: 0.0840
Epoch 47/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1102 - val_loss: 0.0855
Epoch 48/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1098 - val_loss: 0.0857
Epoch 49/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1093 - val_loss: 0.0832
Epoch 50/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1090 - val_loss: 0.0830
Epoch 51/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1086 - val_loss: 0.0859
Epoch 52/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1083 - val_loss: 0.0843
Epoch 53/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1081 - val_loss: 0.0826
Epoch 54/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1078 - val_loss: 0.0833
Epoch 55/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1072 - val_loss: 0.0823
Epoch 56/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1067 - val_loss: 0.0833
Epoch 57/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1064 - val_loss: 0.0825
Epoch 58/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1062 - val_loss: 0.0827
Epoch 59/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1060 - val_loss: 0.0823
Epoch 60/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1054 - val_loss: 0.0815
Epoch 61/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1052 - val_loss: 0.0823
Epoch 62/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1051 - val_loss: 0.0815
Epoch 63/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1045 - val_loss: 0.0816
Epoch 64/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1046 - val_loss: 0.0830
Epoch 65/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1041 - val_loss: 0.0800
Epoch 66/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1038 - val_loss: 0.0820
Epoch 67/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1035 - val_loss: 0.0814
Epoch 68/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1034 - val_loss: 0.0817
Epoch 69/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1029 - val_loss: 0.0807
Epoch 70/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1029 - val_loss: 0.0808
Epoch 71/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1025 - val_loss: 0.0827
Epoch 72/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1022 - val_loss: 0.0813
Epoch 73/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1019 - val_loss: 0.0812
Epoch 74/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1017 - val_loss: 0.0813
Epoch 75/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1015 - val_loss: 0.0818
Val_yp Shape is 
(75724, 24)
Results === Test == Validation ===== 61
=============================
step 1    -  0.991    0.990
step 2    -  0.985    0.983
step 3    -  0.976    0.974
step 4    -  0.967    0.965
step 5    -  0.957    0.955
step 6    -  0.947    0.946
step 7    -  0.938    0.937
step 8    -  0.930    0.929
step 9    -  0.922    0.922
step 10   -  0.914    0.915
step 11   -  0.907    0.908
step 12   -  0.900    0.902
step 13   -  0.894    0.896
step 14   -  0.888    0.890
step 15   -  0.881    0.884
step 16   -  0.875    0.878
step 17   -  0.869    0.872
step 18   -  0.863    0.867
step 19   -  0.858    0.863
step 20   -  0.853    0.859
step 21   -  0.848    0.855
step 22   -  0.842    0.850
step 23   -  0.837    0.846
step 24   -  0.832    0.841
=============================
Summary   -  21.672    21.727
V_y Shape is 
(75724, 1)
=========== SHAPE INPUT-OUTPUT MATRIXES =====================
shape x_training : (227212, 434)  y_training: :  (227212, 24)
shape x_test     :  (75723, 434)  y_test      :  (75723, 24)
shape x_val      :  (75723, 434)  y_val       :  (75723, 24)
=============================================================
(434,)
Model: "MLP"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_36 (InputLayer)       [(None, 434)]             0         
                                                                 
 dense_140 (Dense)           (None, 1024)              445440    
                                                                 
 elu_35 (ELU)                (None, 1024)              0         
                                                                 
 dropout_105 (Dropout)       (None, 1024)              0         
                                                                 
 dense_141 (Dense)           (None, 512)               524800    
                                                                 
 dropout_106 (Dropout)       (None, 512)               0         
                                                                 
 dense_142 (Dense)           (None, 512)               262656    
                                                                 
 dropout_107 (Dropout)       (None, 512)               0         
                                                                 
 dense_143 (Dense)           (None, 24)                12312     
                                                                 
=================================================================
Total params: 1,245,208
Trainable params: 1,245,208
Non-trainable params: 0
_________________________________________________________________
Epoch 1/200
222/222 [==============================] - 2s 7ms/step - loss: 0.4457 - val_loss: 0.1125
Epoch 2/200
222/222 [==============================] - 1s 6ms/step - loss: 0.2355 - val_loss: 0.1026
Epoch 3/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1984 - val_loss: 0.1014
Epoch 4/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1801 - val_loss: 0.0988
Epoch 5/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1688 - val_loss: 0.0960
Epoch 6/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1606 - val_loss: 0.0949
Epoch 7/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1545 - val_loss: 0.0962
Epoch 8/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1495 - val_loss: 0.0938
Epoch 9/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1454 - val_loss: 0.0921
Epoch 10/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1421 - val_loss: 0.0931
Epoch 11/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1391 - val_loss: 0.0913
Epoch 12/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1365 - val_loss: 0.0935
Epoch 13/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1342 - val_loss: 0.0914
Epoch 14/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1323 - val_loss: 0.0927
Epoch 15/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1305 - val_loss: 0.0888
Epoch 16/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1288 - val_loss: 0.0906
Epoch 17/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1273 - val_loss: 0.0896
Epoch 18/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1261 - val_loss: 0.0900
Epoch 19/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1246 - val_loss: 0.0872
Epoch 20/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1236 - val_loss: 0.0889
Epoch 21/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1226 - val_loss: 0.0886
Epoch 22/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1217 - val_loss: 0.0888
Epoch 23/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1211 - val_loss: 0.0895
Epoch 24/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1202 - val_loss: 0.0895
Epoch 25/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1195 - val_loss: 0.0869
Epoch 26/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1187 - val_loss: 0.0862
Epoch 27/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1184 - val_loss: 0.0890
Epoch 28/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1178 - val_loss: 0.0871
Epoch 29/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1172 - val_loss: 0.0863
Epoch 30/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1167 - val_loss: 0.0887
Epoch 31/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1163 - val_loss: 0.0878
Epoch 32/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1160 - val_loss: 0.0865
Epoch 33/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1152 - val_loss: 0.0878
Epoch 34/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1151 - val_loss: 0.0858
Epoch 35/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1145 - val_loss: 0.0848
Epoch 36/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1140 - val_loss: 0.0863
Epoch 37/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1137 - val_loss: 0.0859
Epoch 38/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1135 - val_loss: 0.0865
Epoch 39/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1131 - val_loss: 0.0878
Epoch 40/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1125 - val_loss: 0.0868
Epoch 41/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1122 - val_loss: 0.0850
Epoch 42/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1118 - val_loss: 0.0861
Epoch 43/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1116 - val_loss: 0.0859
Epoch 44/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1112 - val_loss: 0.0846
Epoch 45/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1106 - val_loss: 0.0848
Epoch 46/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1103 - val_loss: 0.0837
Epoch 47/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1101 - val_loss: 0.0843
Epoch 48/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1096 - val_loss: 0.0840
Epoch 49/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1092 - val_loss: 0.0848
Epoch 50/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1088 - val_loss: 0.0842
Epoch 51/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1084 - val_loss: 0.0824
Epoch 52/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1081 - val_loss: 0.0849
Epoch 53/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1078 - val_loss: 0.0837
Epoch 54/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1071 - val_loss: 0.0824
Epoch 55/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1069 - val_loss: 0.0833
Epoch 56/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1065 - val_loss: 0.0817
Epoch 57/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1063 - val_loss: 0.0848
Epoch 58/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1059 - val_loss: 0.0815
Epoch 59/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1055 - val_loss: 0.0830
Epoch 60/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1054 - val_loss: 0.0822
Epoch 61/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1049 - val_loss: 0.0829
Epoch 62/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1045 - val_loss: 0.0819
Epoch 63/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1043 - val_loss: 0.0818
Epoch 64/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1041 - val_loss: 0.0824
Epoch 65/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1039 - val_loss: 0.0822
Epoch 66/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1033 - val_loss: 0.0806
Epoch 67/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1031 - val_loss: 0.0824
Epoch 68/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1028 - val_loss: 0.0809
Epoch 69/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1027 - val_loss: 0.0823
Epoch 70/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1022 - val_loss: 0.0802
Epoch 71/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1024 - val_loss: 0.0804
Epoch 72/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1018 - val_loss: 0.0812
Epoch 73/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1015 - val_loss: 0.0800
Epoch 74/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1013 - val_loss: 0.0801
Epoch 75/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1012 - val_loss: 0.0817
Epoch 76/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1008 - val_loss: 0.0821
Epoch 77/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1004 - val_loss: 0.0812
Epoch 78/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1003 - val_loss: 0.0803
Epoch 79/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1002 - val_loss: 0.0805
Epoch 80/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0999 - val_loss: 0.0812
Epoch 81/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0996 - val_loss: 0.0812
Epoch 82/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0998 - val_loss: 0.0818
Epoch 83/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0994 - val_loss: 0.0800
Epoch 84/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0990 - val_loss: 0.0831
Epoch 85/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0990 - val_loss: 0.0809
Epoch 86/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0986 - val_loss: 0.0808
Epoch 87/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0984 - val_loss: 0.0799
Epoch 88/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0983 - val_loss: 0.0802
Epoch 89/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0981 - val_loss: 0.0815
Epoch 90/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0980 - val_loss: 0.0815
Epoch 91/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0977 - val_loss: 0.0805
Epoch 92/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0974 - val_loss: 0.0801
Epoch 93/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0973 - val_loss: 0.0803
Epoch 94/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0970 - val_loss: 0.0815
Epoch 95/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0968 - val_loss: 0.0802
Epoch 96/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0968 - val_loss: 0.0808
Epoch 97/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0966 - val_loss: 0.0805
Val_yp Shape is 
(75723, 24)
Results === Test == Validation ===== 62
=============================
step 1    -  0.991    0.990
step 2    -  0.984    0.982
step 3    -  0.975    0.973
step 4    -  0.965    0.963
step 5    -  0.956    0.954
step 6    -  0.947    0.945
step 7    -  0.938    0.937
step 8    -  0.930    0.929
step 9    -  0.921    0.921
step 10   -  0.914    0.914
step 11   -  0.906    0.907
step 12   -  0.899    0.901
step 13   -  0.891    0.894
step 14   -  0.885    0.888
step 15   -  0.877    0.881
step 16   -  0.870    0.874
step 17   -  0.863    0.867
step 18   -  0.858    0.863
step 19   -  0.854    0.860
step 20   -  0.848    0.854
step 21   -  0.844    0.851
step 22   -  0.840    0.847
step 23   -  0.834    0.842
step 24   -  0.827    0.836
=============================
Summary   -  21.616    21.672
V_y Shape is 
(75723, 1)
=========== SHAPE INPUT-OUTPUT MATRIXES =====================
shape x_training : (227211, 441)  y_training: :  (227211, 24)
shape x_test     :  (75722, 441)  y_test      :  (75722, 24)
shape x_val      :  (75723, 441)  y_val       :  (75723, 24)
=============================================================
(441,)
Model: "MLP"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_37 (InputLayer)       [(None, 441)]             0         
                                                                 
 dense_144 (Dense)           (None, 1024)              452608    
                                                                 
 elu_36 (ELU)                (None, 1024)              0         
                                                                 
 dropout_108 (Dropout)       (None, 1024)              0         
                                                                 
 dense_145 (Dense)           (None, 512)               524800    
                                                                 
 dropout_109 (Dropout)       (None, 512)               0         
                                                                 
 dense_146 (Dense)           (None, 512)               262656    
                                                                 
 dropout_110 (Dropout)       (None, 512)               0         
                                                                 
 dense_147 (Dense)           (None, 24)                12312     
                                                                 
=================================================================
Total params: 1,252,376
Trainable params: 1,252,376
Non-trainable params: 0
_________________________________________________________________
Epoch 1/200
222/222 [==============================] - 2s 8ms/step - loss: 0.4612 - val_loss: 0.1120
Epoch 2/200
222/222 [==============================] - 1s 6ms/step - loss: 0.2393 - val_loss: 0.1027
Epoch 3/200
222/222 [==============================] - 1s 6ms/step - loss: 0.2009 - val_loss: 0.1002
Epoch 4/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1824 - val_loss: 0.0979
Epoch 5/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1704 - val_loss: 0.0975
Epoch 6/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1621 - val_loss: 0.0947
Epoch 7/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1556 - val_loss: 0.0962
Epoch 8/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1506 - val_loss: 0.0922
Epoch 9/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1465 - val_loss: 0.0927
Epoch 10/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1428 - val_loss: 0.0918
Epoch 11/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1398 - val_loss: 0.0921
Epoch 12/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1369 - val_loss: 0.0918
Epoch 13/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1347 - val_loss: 0.0889
Epoch 14/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1329 - val_loss: 0.0926
Epoch 15/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1311 - val_loss: 0.0911
Epoch 16/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1291 - val_loss: 0.0907
Epoch 17/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1277 - val_loss: 0.0886
Epoch 18/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1264 - val_loss: 0.0886
Epoch 19/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1250 - val_loss: 0.0896
Epoch 20/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1238 - val_loss: 0.0887
Epoch 21/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1229 - val_loss: 0.0894
Epoch 22/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1218 - val_loss: 0.0895
Epoch 23/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1211 - val_loss: 0.0875
Epoch 24/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1203 - val_loss: 0.0917
Epoch 25/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1198 - val_loss: 0.0901
Epoch 26/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1190 - val_loss: 0.0907
Epoch 27/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1184 - val_loss: 0.0880
Epoch 28/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1178 - val_loss: 0.0884
Epoch 29/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1172 - val_loss: 0.0897
Epoch 30/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1167 - val_loss: 0.0861
Epoch 31/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1161 - val_loss: 0.0885
Epoch 32/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1159 - val_loss: 0.0860
Epoch 33/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1155 - val_loss: 0.0890
Epoch 34/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1146 - val_loss: 0.0868
Epoch 35/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1143 - val_loss: 0.0880
Epoch 36/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1141 - val_loss: 0.0872
Epoch 37/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1137 - val_loss: 0.0854
Epoch 38/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1132 - val_loss: 0.0851
Epoch 39/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1127 - val_loss: 0.0863
Epoch 40/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1123 - val_loss: 0.0845
Epoch 41/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1120 - val_loss: 0.0860
Epoch 42/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1113 - val_loss: 0.0844
Epoch 43/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1111 - val_loss: 0.0854
Epoch 44/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1107 - val_loss: 0.0838
Epoch 45/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1102 - val_loss: 0.0827
Epoch 46/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1098 - val_loss: 0.0843
Epoch 47/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1096 - val_loss: 0.0827
Epoch 48/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1089 - val_loss: 0.0848
Epoch 49/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1086 - val_loss: 0.0839
Epoch 50/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1083 - val_loss: 0.0836
Epoch 51/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1079 - val_loss: 0.0832
Epoch 52/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1076 - val_loss: 0.0846
Epoch 53/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1071 - val_loss: 0.0841
Epoch 54/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1066 - val_loss: 0.0816
Epoch 55/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1063 - val_loss: 0.0822
Epoch 56/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1061 - val_loss: 0.0848
Epoch 57/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1057 - val_loss: 0.0838
Epoch 58/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1052 - val_loss: 0.0832
Epoch 59/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1052 - val_loss: 0.0823
Epoch 60/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1047 - val_loss: 0.0821
Epoch 61/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1045 - val_loss: 0.0824
Epoch 62/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1041 - val_loss: 0.0813
Epoch 63/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1039 - val_loss: 0.0807
Epoch 64/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1034 - val_loss: 0.0823
Epoch 65/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1033 - val_loss: 0.0806
Epoch 66/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1029 - val_loss: 0.0814
Epoch 67/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1027 - val_loss: 0.0818
Epoch 68/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1025 - val_loss: 0.0798
Epoch 69/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1022 - val_loss: 0.0818
Epoch 70/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1017 - val_loss: 0.0809
Epoch 71/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1018 - val_loss: 0.0817
Epoch 72/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1015 - val_loss: 0.0818
Epoch 73/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1012 - val_loss: 0.0818
Epoch 74/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1009 - val_loss: 0.0823
Epoch 75/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1006 - val_loss: 0.0817
Epoch 76/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1003 - val_loss: 0.0810
Epoch 77/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1002 - val_loss: 0.0812
Epoch 78/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0999 - val_loss: 0.0828
Val_yp Shape is 
(75723, 24)
Results === Test == Validation ===== 63
=============================
step 1    -  0.991    0.990
step 2    -  0.984    0.983
step 3    -  0.976    0.974
step 4    -  0.967    0.964
step 5    -  0.957    0.955
step 6    -  0.947    0.945
step 7    -  0.938    0.937
step 8    -  0.929    0.929
step 9    -  0.922    0.922
step 10   -  0.914    0.915
step 11   -  0.908    0.909
step 12   -  0.901    0.902
step 13   -  0.894    0.896
step 14   -  0.888    0.891
step 15   -  0.883    0.885
step 16   -  0.877    0.881
step 17   -  0.872    0.876
step 18   -  0.867    0.871
step 19   -  0.862    0.867
step 20   -  0.857    0.863
step 21   -  0.853    0.859
step 22   -  0.849    0.856
step 23   -  0.844    0.852
step 24   -  0.839    0.848
=============================
Summary   -  21.721    21.769
V_y Shape is 
(75723, 1)
=========== SHAPE INPUT-OUTPUT MATRIXES =====================
shape x_training : (227210, 448)  y_training: :  (227210, 24)
shape x_test     :  (75722, 448)  y_test      :  (75722, 24)
shape x_val      :  (75722, 448)  y_val       :  (75722, 24)
=============================================================
(448,)
Model: "MLP"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_38 (InputLayer)       [(None, 448)]             0         
                                                                 
 dense_148 (Dense)           (None, 1024)              459776    
                                                                 
 elu_37 (ELU)                (None, 1024)              0         
                                                                 
 dropout_111 (Dropout)       (None, 1024)              0         
                                                                 
 dense_149 (Dense)           (None, 512)               524800    
                                                                 
 dropout_112 (Dropout)       (None, 512)               0         
                                                                 
 dense_150 (Dense)           (None, 512)               262656    
                                                                 
 dropout_113 (Dropout)       (None, 512)               0         
                                                                 
 dense_151 (Dense)           (None, 24)                12312     
                                                                 
=================================================================
Total params: 1,259,544
Trainable params: 1,259,544
Non-trainable params: 0
_________________________________________________________________
Epoch 1/200
222/222 [==============================] - 2s 7ms/step - loss: 0.4595 - val_loss: 0.1112
Epoch 2/200
222/222 [==============================] - 1s 6ms/step - loss: 0.2387 - val_loss: 0.1022
Epoch 3/200
222/222 [==============================] - 1s 6ms/step - loss: 0.2005 - val_loss: 0.1016
Epoch 4/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1821 - val_loss: 0.0999
Epoch 5/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1703 - val_loss: 0.0962
Epoch 6/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1619 - val_loss: 0.0969
Epoch 7/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1555 - val_loss: 0.0955
Epoch 8/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1504 - val_loss: 0.0930
Epoch 9/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1463 - val_loss: 0.0940
Epoch 10/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1428 - val_loss: 0.0920
Epoch 11/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1397 - val_loss: 0.0926
Epoch 12/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1371 - val_loss: 0.0911
Epoch 13/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1345 - val_loss: 0.0912
Epoch 14/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1325 - val_loss: 0.0885
Epoch 15/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1306 - val_loss: 0.0897
Epoch 16/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1288 - val_loss: 0.0928
Epoch 17/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1274 - val_loss: 0.0880
Epoch 18/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1262 - val_loss: 0.0900
Epoch 19/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1247 - val_loss: 0.0902
Epoch 20/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1238 - val_loss: 0.0877
Epoch 21/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1226 - val_loss: 0.0885
Epoch 22/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1218 - val_loss: 0.0893
Epoch 23/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1212 - val_loss: 0.0887
Epoch 24/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1202 - val_loss: 0.0894
Epoch 25/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1194 - val_loss: 0.0878
Epoch 26/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1189 - val_loss: 0.0882
Epoch 27/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1184 - val_loss: 0.0880
Epoch 28/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1176 - val_loss: 0.0894
Epoch 29/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1172 - val_loss: 0.0879
Epoch 30/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1167 - val_loss: 0.0862
Epoch 31/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1161 - val_loss: 0.0865
Epoch 32/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1156 - val_loss: 0.0863
Epoch 33/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1152 - val_loss: 0.0862
Epoch 34/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1149 - val_loss: 0.0854
Epoch 35/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1142 - val_loss: 0.0860
Epoch 36/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1140 - val_loss: 0.0856
Epoch 37/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1133 - val_loss: 0.0858
Epoch 38/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1134 - val_loss: 0.0854
Epoch 39/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1124 - val_loss: 0.0860
Epoch 40/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1123 - val_loss: 0.0860
Epoch 41/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1119 - val_loss: 0.0838
Epoch 42/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1113 - val_loss: 0.0859
Epoch 43/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1112 - val_loss: 0.0855
Epoch 44/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1107 - val_loss: 0.0854
Epoch 45/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1103 - val_loss: 0.0839
Epoch 46/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1099 - val_loss: 0.0830
Epoch 47/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1096 - val_loss: 0.0847
Epoch 48/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1091 - val_loss: 0.0826
Epoch 49/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1087 - val_loss: 0.0842
Epoch 50/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1083 - val_loss: 0.0842
Epoch 51/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1079 - val_loss: 0.0842
Epoch 52/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1075 - val_loss: 0.0820
Epoch 53/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1071 - val_loss: 0.0838
Epoch 54/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1068 - val_loss: 0.0830
Epoch 55/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1064 - val_loss: 0.0807
Epoch 56/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1061 - val_loss: 0.0836
Epoch 57/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1057 - val_loss: 0.0825
Epoch 58/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1054 - val_loss: 0.0847
Epoch 59/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1050 - val_loss: 0.0820
Epoch 60/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1047 - val_loss: 0.0824
Epoch 61/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1044 - val_loss: 0.0820
Epoch 62/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1039 - val_loss: 0.0804
Epoch 63/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1038 - val_loss: 0.0808
Epoch 64/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1034 - val_loss: 0.0817
Epoch 65/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1030 - val_loss: 0.0818
Epoch 66/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1029 - val_loss: 0.0805
Epoch 67/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1027 - val_loss: 0.0813
Epoch 68/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1023 - val_loss: 0.0817
Epoch 69/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1021 - val_loss: 0.0830
Epoch 70/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1017 - val_loss: 0.0815
Epoch 71/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1014 - val_loss: 0.0803
Epoch 72/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1014 - val_loss: 0.0819
Epoch 73/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1008 - val_loss: 0.0792
Epoch 74/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1007 - val_loss: 0.0810
Epoch 75/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1006 - val_loss: 0.0798
Epoch 76/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1003 - val_loss: 0.0806
Epoch 77/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1000 - val_loss: 0.0814
Epoch 78/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0997 - val_loss: 0.0814
Epoch 79/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0997 - val_loss: 0.0802
Epoch 80/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0992 - val_loss: 0.0813
Epoch 81/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0990 - val_loss: 0.0801
Epoch 82/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0988 - val_loss: 0.0797
Epoch 83/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0986 - val_loss: 0.0801
Val_yp Shape is 
(75722, 24)
Results === Test == Validation ===== 64
=============================
step 1    -  0.991    0.990
step 2    -  0.984    0.983
step 3    -  0.976    0.974
step 4    -  0.967    0.964
step 5    -  0.957    0.955
step 6    -  0.948    0.946
step 7    -  0.939    0.938
step 8    -  0.931    0.930
step 9    -  0.922    0.922
step 10   -  0.914    0.915
step 11   -  0.906    0.907
step 12   -  0.897    0.899
step 13   -  0.892    0.894
step 14   -  0.884    0.887
step 15   -  0.878    0.881
step 16   -  0.872    0.876
step 17   -  0.867    0.871
step 18   -  0.859    0.865
step 19   -  0.855    0.861
step 20   -  0.851    0.857
step 21   -  0.846    0.853
step 22   -  0.840    0.848
step 23   -  0.836    0.844
step 24   -  0.831    0.840
=============================
Summary   -  21.645    21.702
V_y Shape is 
(75722, 1)
=========== SHAPE INPUT-OUTPUT MATRIXES =====================
shape x_training : (227209, 455)  y_training: :  (227209, 24)
shape x_test     :  (75721, 455)  y_test      :  (75721, 24)
shape x_val      :  (75722, 455)  y_val       :  (75722, 24)
=============================================================
(455,)
Model: "MLP"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_39 (InputLayer)       [(None, 455)]             0         
                                                                 
 dense_152 (Dense)           (None, 1024)              466944    
                                                                 
 elu_38 (ELU)                (None, 1024)              0         
                                                                 
 dropout_114 (Dropout)       (None, 1024)              0         
                                                                 
 dense_153 (Dense)           (None, 512)               524800    
                                                                 
 dropout_115 (Dropout)       (None, 512)               0         
                                                                 
 dense_154 (Dense)           (None, 512)               262656    
                                                                 
 dropout_116 (Dropout)       (None, 512)               0         
                                                                 
 dense_155 (Dense)           (None, 24)                12312     
                                                                 
=================================================================
Total params: 1,266,712
Trainable params: 1,266,712
Non-trainable params: 0
_________________________________________________________________
Epoch 1/200
222/222 [==============================] - 2s 8ms/step - loss: 0.4555 - val_loss: 0.1133
Epoch 2/200
222/222 [==============================] - 1s 7ms/step - loss: 0.2386 - val_loss: 0.1021
Epoch 3/200
222/222 [==============================] - 1s 7ms/step - loss: 0.2005 - val_loss: 0.1003
Epoch 4/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1820 - val_loss: 0.0985
Epoch 5/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1702 - val_loss: 0.0978
Epoch 6/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1620 - val_loss: 0.0943
Epoch 7/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1552 - val_loss: 0.0959
Epoch 8/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1504 - val_loss: 0.0928
Epoch 9/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1462 - val_loss: 0.0933
Epoch 10/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1425 - val_loss: 0.0916
Epoch 11/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1393 - val_loss: 0.0920
Epoch 12/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1367 - val_loss: 0.0905
Epoch 13/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1343 - val_loss: 0.0918
Epoch 14/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1323 - val_loss: 0.0885
Epoch 15/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1305 - val_loss: 0.0929
Epoch 16/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1285 - val_loss: 0.0903
Epoch 17/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1274 - val_loss: 0.0910
Epoch 18/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1259 - val_loss: 0.0884
Epoch 19/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1246 - val_loss: 0.0891
Epoch 20/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1236 - val_loss: 0.0920
Epoch 21/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1226 - val_loss: 0.0870
Epoch 22/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1215 - val_loss: 0.0891
Epoch 23/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1206 - val_loss: 0.0899
Epoch 24/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1199 - val_loss: 0.0880
Epoch 25/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1192 - val_loss: 0.0899
Epoch 26/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1184 - val_loss: 0.0880
Epoch 27/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1179 - val_loss: 0.0873
Epoch 28/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1173 - val_loss: 0.0873
Epoch 29/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1169 - val_loss: 0.0884
Epoch 30/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1164 - val_loss: 0.0885
Epoch 31/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1159 - val_loss: 0.0867
Epoch 32/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1153 - val_loss: 0.0870
Epoch 33/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1149 - val_loss: 0.0878
Epoch 34/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1146 - val_loss: 0.0862
Epoch 35/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1139 - val_loss: 0.0863
Epoch 36/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1139 - val_loss: 0.0858
Epoch 37/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1133 - val_loss: 0.0846
Epoch 38/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1128 - val_loss: 0.0858
Epoch 39/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1124 - val_loss: 0.0870
Epoch 40/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1121 - val_loss: 0.0845
Epoch 41/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1114 - val_loss: 0.0853
Epoch 42/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1111 - val_loss: 0.0862
Epoch 43/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1107 - val_loss: 0.0855
Epoch 44/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1103 - val_loss: 0.0850
Epoch 45/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1101 - val_loss: 0.0850
Epoch 46/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1092 - val_loss: 0.0842
Epoch 47/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1090 - val_loss: 0.0842
Epoch 48/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1085 - val_loss: 0.0838
Epoch 49/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1084 - val_loss: 0.0860
Epoch 50/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1078 - val_loss: 0.0836
Epoch 51/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1075 - val_loss: 0.0826
Epoch 52/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1070 - val_loss: 0.0831
Epoch 53/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1066 - val_loss: 0.0822
Epoch 54/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1063 - val_loss: 0.0832
Epoch 55/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1058 - val_loss: 0.0822
Epoch 56/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1057 - val_loss: 0.0832
Epoch 57/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1051 - val_loss: 0.0820
Epoch 58/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1050 - val_loss: 0.0816
Epoch 59/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1045 - val_loss: 0.0821
Epoch 60/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1042 - val_loss: 0.0834
Epoch 61/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1040 - val_loss: 0.0821
Epoch 62/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1035 - val_loss: 0.0811
Epoch 63/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1034 - val_loss: 0.0824
Epoch 64/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1031 - val_loss: 0.0825
Epoch 65/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1025 - val_loss: 0.0816
Epoch 66/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1024 - val_loss: 0.0828
Epoch 67/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1022 - val_loss: 0.0804
Epoch 68/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1020 - val_loss: 0.0815
Epoch 69/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1016 - val_loss: 0.0804
Epoch 70/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1014 - val_loss: 0.0814
Epoch 71/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1008 - val_loss: 0.0812
Epoch 72/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1009 - val_loss: 0.0824
Epoch 73/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1005 - val_loss: 0.0808
Epoch 74/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1005 - val_loss: 0.0818
Epoch 75/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1003 - val_loss: 0.0816
Epoch 76/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1000 - val_loss: 0.0811
Epoch 77/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0995 - val_loss: 0.0810
Epoch 78/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0995 - val_loss: 0.0815
Epoch 79/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0994 - val_loss: 0.0809
Val_yp Shape is 
(75722, 24)
Results === Test == Validation ===== 65
=============================
step 1    -  0.991    0.990
step 2    -  0.984    0.983
step 3    -  0.976    0.974
step 4    -  0.967    0.965
step 5    -  0.958    0.956
step 6    -  0.949    0.948
step 7    -  0.941    0.940
step 8    -  0.932    0.932
step 9    -  0.924    0.925
step 10   -  0.916    0.918
step 11   -  0.909    0.911
step 12   -  0.902    0.905
step 13   -  0.895    0.899
step 14   -  0.889    0.893
step 15   -  0.884    0.889
step 16   -  0.879    0.884
step 17   -  0.873    0.879
step 18   -  0.867    0.874
step 19   -  0.861    0.869
step 20   -  0.855    0.863
step 21   -  0.850    0.859
step 22   -  0.845    0.855
step 23   -  0.840    0.851
step 24   -  0.836    0.847
=============================
Summary   -  21.725    21.807
V_y Shape is 
(75722, 1)
=========== SHAPE INPUT-OUTPUT MATRIXES =====================
shape x_training : (227208, 462)  y_training: :  (227208, 24)
shape x_test     :  (75721, 462)  y_test      :  (75721, 24)
shape x_val      :  (75721, 462)  y_val       :  (75721, 24)
=============================================================
(462,)
Model: "MLP"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_40 (InputLayer)       [(None, 462)]             0         
                                                                 
 dense_156 (Dense)           (None, 1024)              474112    
                                                                 
 elu_39 (ELU)                (None, 1024)              0         
                                                                 
 dropout_117 (Dropout)       (None, 1024)              0         
                                                                 
 dense_157 (Dense)           (None, 512)               524800    
                                                                 
 dropout_118 (Dropout)       (None, 512)               0         
                                                                 
 dense_158 (Dense)           (None, 512)               262656    
                                                                 
 dropout_119 (Dropout)       (None, 512)               0         
                                                                 
 dense_159 (Dense)           (None, 24)                12312     
                                                                 
=================================================================
Total params: 1,273,880
Trainable params: 1,273,880
Non-trainable params: 0
_________________________________________________________________
Epoch 1/200
222/222 [==============================] - 2s 8ms/step - loss: 0.4571 - val_loss: 0.1137
Epoch 2/200
222/222 [==============================] - 1s 6ms/step - loss: 0.2385 - val_loss: 0.1054
Epoch 3/200
222/222 [==============================] - 1s 6ms/step - loss: 0.2005 - val_loss: 0.0994
Epoch 4/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1819 - val_loss: 0.0970
Epoch 5/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1699 - val_loss: 0.0959
Epoch 6/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1616 - val_loss: 0.0959
Epoch 7/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1554 - val_loss: 0.0972
Epoch 8/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1503 - val_loss: 0.0923
Epoch 9/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1461 - val_loss: 0.0910
Epoch 10/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1425 - val_loss: 0.0920
Epoch 11/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1392 - val_loss: 0.0917
Epoch 12/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1367 - val_loss: 0.0938
Epoch 13/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1343 - val_loss: 0.0908
Epoch 14/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1322 - val_loss: 0.0893
Epoch 15/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1305 - val_loss: 0.0893
Epoch 16/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1288 - val_loss: 0.0914
Epoch 17/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1271 - val_loss: 0.0908
Epoch 18/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1257 - val_loss: 0.0894
Epoch 19/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1246 - val_loss: 0.0905
Epoch 20/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1234 - val_loss: 0.0890
Epoch 21/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1224 - val_loss: 0.0875
Epoch 22/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1216 - val_loss: 0.0891
Epoch 23/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1208 - val_loss: 0.0877
Epoch 24/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1197 - val_loss: 0.0896
Epoch 25/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1189 - val_loss: 0.0875
Epoch 26/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1186 - val_loss: 0.0865
Epoch 27/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1180 - val_loss: 0.0885
Epoch 28/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1175 - val_loss: 0.0869
Epoch 29/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1168 - val_loss: 0.0865
Epoch 30/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1162 - val_loss: 0.0884
Epoch 31/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1157 - val_loss: 0.0890
Epoch 32/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1153 - val_loss: 0.0865
Epoch 33/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1149 - val_loss: 0.0869
Epoch 34/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1146 - val_loss: 0.0869
Epoch 35/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1139 - val_loss: 0.0870
Epoch 36/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1134 - val_loss: 0.0884
Epoch 37/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1131 - val_loss: 0.0874
Epoch 38/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1126 - val_loss: 0.0839
Epoch 39/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1123 - val_loss: 0.0860
Epoch 40/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1120 - val_loss: 0.0851
Epoch 41/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1113 - val_loss: 0.0848
Epoch 42/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1112 - val_loss: 0.0852
Epoch 43/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1109 - val_loss: 0.0840
Epoch 44/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1102 - val_loss: 0.0841
Epoch 45/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1098 - val_loss: 0.0850
Epoch 46/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1096 - val_loss: 0.0849
Epoch 47/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1093 - val_loss: 0.0846
Epoch 48/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1085 - val_loss: 0.0854
Val_yp Shape is 
(75721, 24)
Results === Test == Validation ===== 66
=============================
step 1    -  0.990    0.989
step 2    -  0.983    0.981
step 3    -  0.974    0.972
step 4    -  0.964    0.961
step 5    -  0.953    0.951
step 6    -  0.943    0.941
step 7    -  0.934    0.932
step 8    -  0.924    0.923
step 9    -  0.915    0.914
step 10   -  0.905    0.906
step 11   -  0.897    0.898
step 12   -  0.890    0.892
step 13   -  0.884    0.885
step 14   -  0.877    0.879
step 15   -  0.871    0.873
step 16   -  0.865    0.867
step 17   -  0.860    0.862
step 18   -  0.854    0.857
step 19   -  0.850    0.853
step 20   -  0.846    0.850
step 21   -  0.843    0.848
step 22   -  0.839    0.844
step 23   -  0.833    0.839
step 24   -  0.829    0.834
=============================
Summary   -  21.522    21.550
V_y Shape is 
(75721, 1)
=========== SHAPE INPUT-OUTPUT MATRIXES =====================
shape x_training : (227207, 469)  y_training: :  (227207, 24)
shape x_test     :  (75720, 469)  y_test      :  (75720, 24)
shape x_val      :  (75721, 469)  y_val       :  (75721, 24)
=============================================================
(469,)
Model: "MLP"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_41 (InputLayer)       [(None, 469)]             0         
                                                                 
 dense_160 (Dense)           (None, 1024)              481280    
                                                                 
 elu_40 (ELU)                (None, 1024)              0         
                                                                 
 dropout_120 (Dropout)       (None, 1024)              0         
                                                                 
 dense_161 (Dense)           (None, 512)               524800    
                                                                 
 dropout_121 (Dropout)       (None, 512)               0         
                                                                 
 dense_162 (Dense)           (None, 512)               262656    
                                                                 
 dropout_122 (Dropout)       (None, 512)               0         
                                                                 
 dense_163 (Dense)           (None, 24)                12312     
                                                                 
=================================================================
Total params: 1,281,048
Trainable params: 1,281,048
Non-trainable params: 0
_________________________________________________________________
Epoch 1/200
222/222 [==============================] - 2s 8ms/step - loss: 0.4586 - val_loss: 0.1133
Epoch 2/200
222/222 [==============================] - 1s 6ms/step - loss: 0.2397 - val_loss: 0.1025
Epoch 3/200
222/222 [==============================] - 1s 6ms/step - loss: 0.2010 - val_loss: 0.1006
Epoch 4/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1821 - val_loss: 0.0966
Epoch 5/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1704 - val_loss: 0.0988
Epoch 6/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1621 - val_loss: 0.0950
Epoch 7/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1556 - val_loss: 0.0934
Epoch 8/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1503 - val_loss: 0.0937
Epoch 9/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1461 - val_loss: 0.0919
Epoch 10/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1423 - val_loss: 0.0919
Epoch 11/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1394 - val_loss: 0.0912
Epoch 12/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1367 - val_loss: 0.0919
Epoch 13/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1344 - val_loss: 0.0902
Epoch 14/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1322 - val_loss: 0.0910
Epoch 15/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1301 - val_loss: 0.0901
Epoch 16/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1286 - val_loss: 0.0890
Epoch 17/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1271 - val_loss: 0.0890
Epoch 18/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1257 - val_loss: 0.0900
Epoch 19/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1243 - val_loss: 0.0901
Epoch 20/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1234 - val_loss: 0.0908
Epoch 21/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1223 - val_loss: 0.0884
Epoch 22/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1212 - val_loss: 0.0877
Epoch 23/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1207 - val_loss: 0.0880
Epoch 24/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1200 - val_loss: 0.0882
Epoch 25/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1190 - val_loss: 0.0871
Epoch 26/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1182 - val_loss: 0.0884
Epoch 27/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1174 - val_loss: 0.0896
Epoch 28/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1171 - val_loss: 0.0875
Epoch 29/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1166 - val_loss: 0.0862
Epoch 30/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1162 - val_loss: 0.0868
Epoch 31/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1157 - val_loss: 0.0864
Epoch 32/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1152 - val_loss: 0.0881
Epoch 33/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1146 - val_loss: 0.0865
Epoch 34/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1142 - val_loss: 0.0855
Epoch 35/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1137 - val_loss: 0.0862
Epoch 36/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1134 - val_loss: 0.0849
Epoch 37/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1129 - val_loss: 0.0860
Epoch 38/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1124 - val_loss: 0.0851
Epoch 39/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1122 - val_loss: 0.0851
Epoch 40/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1118 - val_loss: 0.0843
Epoch 41/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1112 - val_loss: 0.0883
Epoch 42/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1111 - val_loss: 0.0848
Epoch 43/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1106 - val_loss: 0.0837
Epoch 44/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1102 - val_loss: 0.0850
Epoch 45/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1099 - val_loss: 0.0862
Epoch 46/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1095 - val_loss: 0.0838
Epoch 47/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1088 - val_loss: 0.0832
Epoch 48/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1085 - val_loss: 0.0841
Epoch 49/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1082 - val_loss: 0.0842
Epoch 50/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1077 - val_loss: 0.0843
Epoch 51/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1073 - val_loss: 0.0836
Epoch 52/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1070 - val_loss: 0.0831
Epoch 53/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1066 - val_loss: 0.0842
Epoch 54/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1061 - val_loss: 0.0836
Epoch 55/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1057 - val_loss: 0.0826
Epoch 56/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1056 - val_loss: 0.0819
Epoch 57/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1051 - val_loss: 0.0822
Epoch 58/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1046 - val_loss: 0.0822
Epoch 59/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1044 - val_loss: 0.0813
Epoch 60/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1040 - val_loss: 0.0813
Epoch 61/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1037 - val_loss: 0.0807
Epoch 62/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1034 - val_loss: 0.0822
Epoch 63/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1033 - val_loss: 0.0832
Epoch 64/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1027 - val_loss: 0.0826
Epoch 65/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1025 - val_loss: 0.0806
Epoch 66/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1021 - val_loss: 0.0818
Epoch 67/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1019 - val_loss: 0.0820
Epoch 68/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1015 - val_loss: 0.0828
Epoch 69/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1014 - val_loss: 0.0810
Epoch 70/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1010 - val_loss: 0.0810
Epoch 71/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1006 - val_loss: 0.0813
Epoch 72/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1005 - val_loss: 0.0820
Epoch 73/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1004 - val_loss: 0.0810
Epoch 74/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1001 - val_loss: 0.0828
Epoch 75/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0998 - val_loss: 0.0821
Val_yp Shape is 
(75721, 24)
Results === Test == Validation ===== 67
=============================
step 1    -  0.991    0.990
step 2    -  0.984    0.982
step 3    -  0.975    0.973
step 4    -  0.966    0.963
step 5    -  0.956    0.954
step 6    -  0.946    0.944
step 7    -  0.937    0.936
step 8    -  0.928    0.928
step 9    -  0.920    0.920
step 10   -  0.912    0.913
step 11   -  0.905    0.906
step 12   -  0.896    0.899
step 13   -  0.891    0.894
step 14   -  0.885    0.888
step 15   -  0.880    0.884
step 16   -  0.875    0.879
step 17   -  0.870    0.874
step 18   -  0.865    0.870
step 19   -  0.859    0.865
step 20   -  0.855    0.861
step 21   -  0.850    0.857
step 22   -  0.844    0.851
step 23   -  0.839    0.848
step 24   -  0.834    0.844
=============================
Summary   -  21.663    21.724
V_y Shape is 
(75721, 1)
=========== SHAPE INPUT-OUTPUT MATRIXES =====================
shape x_training : (227206, 476)  y_training: :  (227206, 24)
shape x_test     :  (75720, 476)  y_test      :  (75720, 24)
shape x_val      :  (75720, 476)  y_val       :  (75720, 24)
=============================================================
(476,)
Model: "MLP"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_42 (InputLayer)       [(None, 476)]             0         
                                                                 
 dense_164 (Dense)           (None, 1024)              488448    
                                                                 
 elu_41 (ELU)                (None, 1024)              0         
                                                                 
 dropout_123 (Dropout)       (None, 1024)              0         
                                                                 
 dense_165 (Dense)           (None, 512)               524800    
                                                                 
 dropout_124 (Dropout)       (None, 512)               0         
                                                                 
 dense_166 (Dense)           (None, 512)               262656    
                                                                 
 dropout_125 (Dropout)       (None, 512)               0         
                                                                 
 dense_167 (Dense)           (None, 24)                12312     
                                                                 
=================================================================
Total params: 1,288,216
Trainable params: 1,288,216
Non-trainable params: 0
_________________________________________________________________
Epoch 1/200
222/222 [==============================] - 2s 7ms/step - loss: 0.4609 - val_loss: 0.1120
Epoch 2/200
222/222 [==============================] - 1s 6ms/step - loss: 0.2401 - val_loss: 0.1025
Epoch 3/200
222/222 [==============================] - 1s 6ms/step - loss: 0.2016 - val_loss: 0.1008
Epoch 4/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1827 - val_loss: 0.0992
Epoch 5/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1708 - val_loss: 0.0955
Epoch 6/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1622 - val_loss: 0.0925
Epoch 7/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1556 - val_loss: 0.0950
Epoch 8/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1504 - val_loss: 0.0966
Epoch 9/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1461 - val_loss: 0.0912
Epoch 10/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1424 - val_loss: 0.0922
Epoch 11/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1395 - val_loss: 0.0905
Epoch 12/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1366 - val_loss: 0.0908
Epoch 13/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1340 - val_loss: 0.0901
Epoch 14/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1319 - val_loss: 0.0903
Epoch 15/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1302 - val_loss: 0.0896
Epoch 16/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1284 - val_loss: 0.0917
Epoch 17/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1268 - val_loss: 0.0881
Epoch 18/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1255 - val_loss: 0.0887
Epoch 19/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1242 - val_loss: 0.0879
Epoch 20/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1231 - val_loss: 0.0887
Epoch 21/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1220 - val_loss: 0.0888
Epoch 22/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1212 - val_loss: 0.0914
Epoch 23/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1202 - val_loss: 0.0883
Epoch 24/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1193 - val_loss: 0.0872
Epoch 25/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1187 - val_loss: 0.0888
Epoch 26/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1182 - val_loss: 0.0867
Epoch 27/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1174 - val_loss: 0.0869
Epoch 28/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1168 - val_loss: 0.0885
Epoch 29/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1163 - val_loss: 0.0877
Epoch 30/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1158 - val_loss: 0.0864
Epoch 31/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1155 - val_loss: 0.0877
Epoch 32/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1148 - val_loss: 0.0872
Epoch 33/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1144 - val_loss: 0.0861
Epoch 34/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1140 - val_loss: 0.0859
Epoch 35/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1135 - val_loss: 0.0863
Epoch 36/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1132 - val_loss: 0.0864
Epoch 37/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1127 - val_loss: 0.0864
Epoch 38/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1121 - val_loss: 0.0854
Epoch 39/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1118 - val_loss: 0.0851
Epoch 40/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1114 - val_loss: 0.0847
Epoch 41/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1110 - val_loss: 0.0859
Epoch 42/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1107 - val_loss: 0.0865
Epoch 43/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1102 - val_loss: 0.0854
Epoch 44/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1096 - val_loss: 0.0850
Epoch 45/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1093 - val_loss: 0.0861
Epoch 46/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1088 - val_loss: 0.0851
Epoch 47/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1084 - val_loss: 0.0846
Epoch 48/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1083 - val_loss: 0.0842
Epoch 49/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1077 - val_loss: 0.0838
Epoch 50/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1074 - val_loss: 0.0832
Epoch 51/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1067 - val_loss: 0.0851
Epoch 52/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1067 - val_loss: 0.0822
Epoch 53/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1059 - val_loss: 0.0818
Epoch 54/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1058 - val_loss: 0.0831
Epoch 55/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1053 - val_loss: 0.0824
Epoch 56/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1050 - val_loss: 0.0818
Epoch 57/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1047 - val_loss: 0.0816
Epoch 58/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1043 - val_loss: 0.0825
Epoch 59/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1039 - val_loss: 0.0834
Epoch 60/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1034 - val_loss: 0.0823
Epoch 61/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1033 - val_loss: 0.0833
Epoch 62/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1031 - val_loss: 0.0841
Epoch 63/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1027 - val_loss: 0.0838
Epoch 64/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1022 - val_loss: 0.0827
Epoch 65/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1022 - val_loss: 0.0818
Epoch 66/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1018 - val_loss: 0.0818
Epoch 67/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1014 - val_loss: 0.0835
Val_yp Shape is 
(75720, 24)
Results === Test == Validation ===== 68
=============================
step 1    -  0.991    0.990
step 2    -  0.984    0.982
step 3    -  0.975    0.973
step 4    -  0.966    0.964
step 5    -  0.956    0.955
step 6    -  0.947    0.945
step 7    -  0.937    0.936
step 8    -  0.928    0.928
step 9    -  0.919    0.920
step 10   -  0.911    0.913
step 11   -  0.903    0.906
step 12   -  0.896    0.899
step 13   -  0.889    0.892
step 14   -  0.883    0.886
step 15   -  0.877    0.881
step 16   -  0.872    0.876
step 17   -  0.866    0.871
step 18   -  0.860    0.865
step 19   -  0.855    0.861
step 20   -  0.850    0.856
step 21   -  0.845    0.852
step 22   -  0.841    0.848
step 23   -  0.834    0.842
step 24   -  0.828    0.836
=============================
Summary   -  21.614    21.678
V_y Shape is 
(75720, 1)
=========== SHAPE INPUT-OUTPUT MATRIXES =====================
shape x_training : (227205, 483)  y_training: :  (227205, 24)
shape x_test     :  (75719, 483)  y_test      :  (75719, 24)
shape x_val      :  (75720, 483)  y_val       :  (75720, 24)
=============================================================
(483,)
Model: "MLP"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_43 (InputLayer)       [(None, 483)]             0         
                                                                 
 dense_168 (Dense)           (None, 1024)              495616    
                                                                 
 elu_42 (ELU)                (None, 1024)              0         
                                                                 
 dropout_126 (Dropout)       (None, 1024)              0         
                                                                 
 dense_169 (Dense)           (None, 512)               524800    
                                                                 
 dropout_127 (Dropout)       (None, 512)               0         
                                                                 
 dense_170 (Dense)           (None, 512)               262656    
                                                                 
 dropout_128 (Dropout)       (None, 512)               0         
                                                                 
 dense_171 (Dense)           (None, 24)                12312     
                                                                 
=================================================================
Total params: 1,295,384
Trainable params: 1,295,384
Non-trainable params: 0
_________________________________________________________________
Epoch 1/200
222/222 [==============================] - 2s 8ms/step - loss: 0.4691 - val_loss: 0.1142
Epoch 2/200
222/222 [==============================] - 1s 6ms/step - loss: 0.2428 - val_loss: 0.1053
Epoch 3/200
222/222 [==============================] - 1s 6ms/step - loss: 0.2032 - val_loss: 0.1003
Epoch 4/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1840 - val_loss: 0.0988
Epoch 5/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1720 - val_loss: 0.0972
Epoch 6/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1631 - val_loss: 0.0963
Epoch 7/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1560 - val_loss: 0.0955
Epoch 8/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1510 - val_loss: 0.0962
Epoch 9/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1468 - val_loss: 0.0930
Epoch 10/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1428 - val_loss: 0.0943
Epoch 11/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1398 - val_loss: 0.0905
Epoch 12/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1369 - val_loss: 0.0918
Epoch 13/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1344 - val_loss: 0.0923
Epoch 14/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1321 - val_loss: 0.0904
Epoch 15/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1304 - val_loss: 0.0910
Epoch 16/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1287 - val_loss: 0.0919
Epoch 17/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1270 - val_loss: 0.0901
Epoch 18/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1256 - val_loss: 0.0902
Epoch 19/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1246 - val_loss: 0.0899
Epoch 20/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1232 - val_loss: 0.0885
Epoch 21/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1225 - val_loss: 0.0930
Epoch 22/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1211 - val_loss: 0.0903
Epoch 23/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1203 - val_loss: 0.0902
Epoch 24/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1196 - val_loss: 0.0886
Epoch 25/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1190 - val_loss: 0.0891
Epoch 26/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1182 - val_loss: 0.0879
Epoch 27/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1174 - val_loss: 0.0885
Epoch 28/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1169 - val_loss: 0.0870
Epoch 29/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1167 - val_loss: 0.0861
Epoch 30/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1158 - val_loss: 0.0872
Epoch 31/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1153 - val_loss: 0.0854
Epoch 32/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1147 - val_loss: 0.0878
Epoch 33/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1144 - val_loss: 0.0875
Epoch 34/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1139 - val_loss: 0.0857
Epoch 35/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1135 - val_loss: 0.0862
Epoch 36/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1130 - val_loss: 0.0870
Epoch 37/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1124 - val_loss: 0.0854
Epoch 38/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1121 - val_loss: 0.0871
Epoch 39/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1117 - val_loss: 0.0858
Epoch 40/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1111 - val_loss: 0.0857
Epoch 41/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1111 - val_loss: 0.0854
Val_yp Shape is 
(75720, 24)
Results === Test == Validation ===== 69
=============================
step 1    -  0.990    0.989
step 2    -  0.983    0.982
step 3    -  0.974    0.973
step 4    -  0.965    0.963
step 5    -  0.955    0.953
step 6    -  0.945    0.943
step 7    -  0.935    0.934
step 8    -  0.926    0.925
step 9    -  0.917    0.917
step 10   -  0.909    0.909
step 11   -  0.900    0.901
step 12   -  0.892    0.893
step 13   -  0.885    0.887
step 14   -  0.878    0.881
step 15   -  0.872    0.875
step 16   -  0.866    0.870
step 17   -  0.861    0.864
step 18   -  0.856    0.859
step 19   -  0.852    0.856
step 20   -  0.848    0.852
step 21   -  0.843    0.847
step 22   -  0.838    0.844
step 23   -  0.833    0.839
step 24   -  0.828    0.835
=============================
Summary   -  21.549    21.592
V_y Shape is 
(75720, 1)
=========== SHAPE INPUT-OUTPUT MATRIXES =====================
shape x_training : (227204, 490)  y_training: :  (227204, 24)
shape x_test     :  (75719, 490)  y_test      :  (75719, 24)
shape x_val      :  (75719, 490)  y_val       :  (75719, 24)
=============================================================
(490,)
Model: "MLP"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_44 (InputLayer)       [(None, 490)]             0         
                                                                 
 dense_172 (Dense)           (None, 1024)              502784    
                                                                 
 elu_43 (ELU)                (None, 1024)              0         
                                                                 
 dropout_129 (Dropout)       (None, 1024)              0         
                                                                 
 dense_173 (Dense)           (None, 512)               524800    
                                                                 
 dropout_130 (Dropout)       (None, 512)               0         
                                                                 
 dense_174 (Dense)           (None, 512)               262656    
                                                                 
 dropout_131 (Dropout)       (None, 512)               0         
                                                                 
 dense_175 (Dense)           (None, 24)                12312     
                                                                 
=================================================================
Total params: 1,302,552
Trainable params: 1,302,552
Non-trainable params: 0
_________________________________________________________________
Epoch 1/200
222/222 [==============================] - 2s 8ms/step - loss: 0.4674 - val_loss: 0.1164
Epoch 2/200
222/222 [==============================] - 1s 7ms/step - loss: 0.2417 - val_loss: 0.1047
Epoch 3/200
222/222 [==============================] - 1s 7ms/step - loss: 0.2028 - val_loss: 0.1027
Epoch 4/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1836 - val_loss: 0.0994
Epoch 5/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1711 - val_loss: 0.0968
Epoch 6/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1627 - val_loss: 0.1014
Epoch 7/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1557 - val_loss: 0.0963
Epoch 8/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1507 - val_loss: 0.0929
Epoch 9/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1462 - val_loss: 0.0929
Epoch 10/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1424 - val_loss: 0.0936
Epoch 11/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1393 - val_loss: 0.0947
Epoch 12/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1365 - val_loss: 0.0910
Epoch 13/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1340 - val_loss: 0.0926
Epoch 14/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1319 - val_loss: 0.0922
Epoch 15/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1301 - val_loss: 0.0905
Epoch 16/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1282 - val_loss: 0.0906
Epoch 17/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1270 - val_loss: 0.0882
Epoch 18/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1254 - val_loss: 0.0883
Epoch 19/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1243 - val_loss: 0.0895
Epoch 20/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1229 - val_loss: 0.0882
Epoch 21/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1219 - val_loss: 0.0909
Epoch 22/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1211 - val_loss: 0.0885
Epoch 23/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1202 - val_loss: 0.0888
Epoch 24/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1192 - val_loss: 0.0902
Epoch 25/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1187 - val_loss: 0.0887
Epoch 26/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1180 - val_loss: 0.0873
Epoch 27/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1173 - val_loss: 0.0884
Epoch 28/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1167 - val_loss: 0.0868
Epoch 29/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1160 - val_loss: 0.0887
Epoch 30/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1158 - val_loss: 0.0873
Epoch 31/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1152 - val_loss: 0.0877
Epoch 32/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1147 - val_loss: 0.0861
Epoch 33/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1143 - val_loss: 0.0858
Epoch 34/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1138 - val_loss: 0.0871
Epoch 35/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1133 - val_loss: 0.0856
Epoch 36/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1130 - val_loss: 0.0890
Epoch 37/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1126 - val_loss: 0.0855
Epoch 38/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1120 - val_loss: 0.0851
Epoch 39/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1117 - val_loss: 0.0865
Epoch 40/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1114 - val_loss: 0.0870
Epoch 41/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1109 - val_loss: 0.0862
Epoch 42/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1102 - val_loss: 0.0844
Epoch 43/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1100 - val_loss: 0.0842
Epoch 44/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1095 - val_loss: 0.0851
Epoch 45/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1091 - val_loss: 0.0833
Epoch 46/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1087 - val_loss: 0.0850
Epoch 47/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1082 - val_loss: 0.0845
Epoch 48/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1080 - val_loss: 0.0846
Epoch 49/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1074 - val_loss: 0.0839
Epoch 50/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1069 - val_loss: 0.0833
Epoch 51/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1066 - val_loss: 0.0846
Epoch 52/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1064 - val_loss: 0.0830
Epoch 53/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1058 - val_loss: 0.0838
Epoch 54/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1054 - val_loss: 0.0851
Epoch 55/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1052 - val_loss: 0.0836
Epoch 56/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1047 - val_loss: 0.0837
Epoch 57/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1042 - val_loss: 0.0832
Epoch 58/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1038 - val_loss: 0.0840
Epoch 59/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1035 - val_loss: 0.0822
Epoch 60/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1031 - val_loss: 0.0836
Epoch 61/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1028 - val_loss: 0.0827
Epoch 62/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1027 - val_loss: 0.0824
Epoch 63/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1024 - val_loss: 0.0814
Epoch 64/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1020 - val_loss: 0.0821
Epoch 65/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1016 - val_loss: 0.0833
Epoch 66/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1014 - val_loss: 0.0828
Epoch 67/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1011 - val_loss: 0.0805
Epoch 68/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1008 - val_loss: 0.0813
Epoch 69/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1005 - val_loss: 0.0833
Epoch 70/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1000 - val_loss: 0.0842
Epoch 71/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0998 - val_loss: 0.0821
Epoch 72/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0997 - val_loss: 0.0813
Epoch 73/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0991 - val_loss: 0.0840
Epoch 74/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0989 - val_loss: 0.0816
Epoch 75/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0987 - val_loss: 0.0823
Epoch 76/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0986 - val_loss: 0.0818
Epoch 77/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0981 - val_loss: 0.0823
Val_yp Shape is 
(75719, 24)
Results === Test == Validation ===== 70
=============================
step 1    -  0.991    0.990
step 2    -  0.984    0.982
step 3    -  0.975    0.973
step 4    -  0.966    0.964
step 5    -  0.957    0.955
step 6    -  0.947    0.946
step 7    -  0.938    0.937
step 8    -  0.929    0.930
step 9    -  0.922    0.922
step 10   -  0.914    0.916
step 11   -  0.906    0.909
step 12   -  0.899    0.902
step 13   -  0.893    0.896
step 14   -  0.887    0.890
step 15   -  0.880    0.884
step 16   -  0.874    0.878
step 17   -  0.868    0.873
step 18   -  0.863    0.868
step 19   -  0.858    0.864
step 20   -  0.853    0.859
step 21   -  0.848    0.854
step 22   -  0.842    0.850
step 23   -  0.838    0.846
step 24   -  0.833    0.842
=============================
Summary   -  21.664    21.730
V_y Shape is 
(75719, 1)
=========== SHAPE INPUT-OUTPUT MATRIXES =====================
shape x_training : (227203, 497)  y_training: :  (227203, 24)
shape x_test     :  (75718, 497)  y_test      :  (75718, 24)
shape x_val      :  (75719, 497)  y_val       :  (75719, 24)
=============================================================
(497,)
Model: "MLP"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_45 (InputLayer)       [(None, 497)]             0         
                                                                 
 dense_176 (Dense)           (None, 1024)              509952    
                                                                 
 elu_44 (ELU)                (None, 1024)              0         
                                                                 
 dropout_132 (Dropout)       (None, 1024)              0         
                                                                 
 dense_177 (Dense)           (None, 512)               524800    
                                                                 
 dropout_133 (Dropout)       (None, 512)               0         
                                                                 
 dense_178 (Dense)           (None, 512)               262656    
                                                                 
 dropout_134 (Dropout)       (None, 512)               0         
                                                                 
 dense_179 (Dense)           (None, 24)                12312     
                                                                 
=================================================================
Total params: 1,309,720
Trainable params: 1,309,720
Non-trainable params: 0
_________________________________________________________________
Epoch 1/200
222/222 [==============================] - 2s 7ms/step - loss: 0.4744 - val_loss: 0.1146
Epoch 2/200
222/222 [==============================] - 1s 6ms/step - loss: 0.2435 - val_loss: 0.1063
Epoch 3/200
222/222 [==============================] - 1s 6ms/step - loss: 0.2039 - val_loss: 0.1012
Epoch 4/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1843 - val_loss: 0.0989
Epoch 5/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1721 - val_loss: 0.0976
Epoch 6/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1631 - val_loss: 0.0966
Epoch 7/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1565 - val_loss: 0.0937
Epoch 8/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1509 - val_loss: 0.0928
Epoch 9/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1466 - val_loss: 0.0963
Epoch 10/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1429 - val_loss: 0.0913
Epoch 11/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1397 - val_loss: 0.0949
Epoch 12/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1370 - val_loss: 0.0940
Epoch 13/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1344 - val_loss: 0.0912
Epoch 14/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1321 - val_loss: 0.0935
Epoch 15/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1303 - val_loss: 0.0892
Epoch 16/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1286 - val_loss: 0.0893
Epoch 17/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1269 - val_loss: 0.0888
Epoch 18/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1255 - val_loss: 0.0896
Epoch 19/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1243 - val_loss: 0.0902
Epoch 20/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1232 - val_loss: 0.0890
Epoch 21/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1220 - val_loss: 0.0904
Epoch 22/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1210 - val_loss: 0.0898
Epoch 23/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1201 - val_loss: 0.0884
Epoch 24/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1192 - val_loss: 0.0869
Epoch 25/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1188 - val_loss: 0.0895
Epoch 26/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1177 - val_loss: 0.0870
Epoch 27/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1173 - val_loss: 0.0871
Epoch 28/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1169 - val_loss: 0.0885
Epoch 29/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1162 - val_loss: 0.0889
Epoch 30/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1157 - val_loss: 0.0868
Epoch 31/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1150 - val_loss: 0.0847
Epoch 32/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1146 - val_loss: 0.0875
Epoch 33/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1141 - val_loss: 0.0880
Epoch 34/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1135 - val_loss: 0.0869
Epoch 35/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1131 - val_loss: 0.0871
Epoch 36/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1126 - val_loss: 0.0850
Epoch 37/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1123 - val_loss: 0.0863
Epoch 38/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1120 - val_loss: 0.0853
Epoch 39/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1115 - val_loss: 0.0859
Epoch 40/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1109 - val_loss: 0.0853
Epoch 41/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1105 - val_loss: 0.0850
Val_yp Shape is 
(75719, 24)
Results === Test == Validation ===== 71
=============================
step 1    -  0.990    0.989
step 2    -  0.983    0.981
step 3    -  0.973    0.971
step 4    -  0.963    0.961
step 5    -  0.953    0.951
step 6    -  0.943    0.941
step 7    -  0.932    0.931
step 8    -  0.922    0.922
step 9    -  0.914    0.914
step 10   -  0.905    0.906
step 11   -  0.897    0.899
step 12   -  0.890    0.892
step 13   -  0.883    0.885
step 14   -  0.877    0.879
step 15   -  0.872    0.874
step 16   -  0.866    0.869
step 17   -  0.863    0.866
step 18   -  0.859    0.862
step 19   -  0.856    0.860
step 20   -  0.852    0.856
step 21   -  0.848    0.853
step 22   -  0.844    0.850
step 23   -  0.839    0.846
step 24   -  0.835    0.842
=============================
Summary   -  21.557    21.600
V_y Shape is 
(75719, 1)
=========== SHAPE INPUT-OUTPUT MATRIXES =====================
shape x_training : (227202, 504)  y_training: :  (227202, 24)
shape x_test     :  (75718, 504)  y_test      :  (75718, 24)
shape x_val      :  (75718, 504)  y_val       :  (75718, 24)
=============================================================
(504,)
Model: "MLP"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_46 (InputLayer)       [(None, 504)]             0         
                                                                 
 dense_180 (Dense)           (None, 1024)              517120    
                                                                 
 elu_45 (ELU)                (None, 1024)              0         
                                                                 
 dropout_135 (Dropout)       (None, 1024)              0         
                                                                 
 dense_181 (Dense)           (None, 512)               524800    
                                                                 
 dropout_136 (Dropout)       (None, 512)               0         
                                                                 
 dense_182 (Dense)           (None, 512)               262656    
                                                                 
 dropout_137 (Dropout)       (None, 512)               0         
                                                                 
 dense_183 (Dense)           (None, 24)                12312     
                                                                 
=================================================================
Total params: 1,316,888
Trainable params: 1,316,888
Non-trainable params: 0
_________________________________________________________________
Epoch 1/200
222/222 [==============================] - 2s 8ms/step - loss: 0.4684 - val_loss: 0.1145
Epoch 2/200
222/222 [==============================] - 1s 6ms/step - loss: 0.2428 - val_loss: 0.1030
Epoch 3/200
222/222 [==============================] - 1s 6ms/step - loss: 0.2029 - val_loss: 0.1013
Epoch 4/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1836 - val_loss: 0.1000
Epoch 5/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1716 - val_loss: 0.0969
Epoch 6/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1626 - val_loss: 0.0949
Epoch 7/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1559 - val_loss: 0.0957
Epoch 8/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1504 - val_loss: 0.0935
Epoch 9/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1462 - val_loss: 0.0937
Epoch 10/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1421 - val_loss: 0.0939
Epoch 11/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1388 - val_loss: 0.0924
Epoch 12/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1364 - val_loss: 0.0925
Epoch 13/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1339 - val_loss: 0.0942
Epoch 14/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1318 - val_loss: 0.0901
Epoch 15/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1301 - val_loss: 0.0925
Epoch 16/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1278 - val_loss: 0.0889
Epoch 17/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1263 - val_loss: 0.0890
Epoch 18/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1251 - val_loss: 0.0898
Epoch 19/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1237 - val_loss: 0.0901
Epoch 20/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1226 - val_loss: 0.0906
Epoch 21/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1216 - val_loss: 0.0896
Epoch 22/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1204 - val_loss: 0.0888
Epoch 23/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1197 - val_loss: 0.0885
Epoch 24/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1189 - val_loss: 0.0872
Epoch 25/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1183 - val_loss: 0.0881
Epoch 26/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1176 - val_loss: 0.0885
Epoch 27/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1167 - val_loss: 0.0880
Epoch 28/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1162 - val_loss: 0.0870
Epoch 29/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1156 - val_loss: 0.0866
Epoch 30/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1152 - val_loss: 0.0865
Epoch 31/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1149 - val_loss: 0.0852
Epoch 32/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1141 - val_loss: 0.0872
Epoch 33/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1136 - val_loss: 0.0864
Epoch 34/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1130 - val_loss: 0.0872
Epoch 35/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1129 - val_loss: 0.0882
Epoch 36/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1124 - val_loss: 0.0859
Epoch 37/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1119 - val_loss: 0.0847
Epoch 38/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1115 - val_loss: 0.0850
Epoch 39/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1109 - val_loss: 0.0847
Epoch 40/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1104 - val_loss: 0.0847
Epoch 41/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1101 - val_loss: 0.0841
Epoch 42/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1097 - val_loss: 0.0849
Epoch 43/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1091 - val_loss: 0.0839
Epoch 44/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1086 - val_loss: 0.0840
Epoch 45/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1082 - val_loss: 0.0851
Epoch 46/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1079 - val_loss: 0.0841
Epoch 47/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1075 - val_loss: 0.0844
Epoch 48/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1069 - val_loss: 0.0836
Epoch 49/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1065 - val_loss: 0.0834
Epoch 50/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1062 - val_loss: 0.0832
Epoch 51/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1059 - val_loss: 0.0834
Epoch 52/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1053 - val_loss: 0.0834
Epoch 53/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1050 - val_loss: 0.0837
Epoch 54/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1044 - val_loss: 0.0809
Epoch 55/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1043 - val_loss: 0.0821
Epoch 56/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1039 - val_loss: 0.0813
Epoch 57/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1037 - val_loss: 0.0819
Epoch 58/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1030 - val_loss: 0.0821
Epoch 59/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1027 - val_loss: 0.0832
Epoch 60/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1023 - val_loss: 0.0816
Epoch 61/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1022 - val_loss: 0.0806
Epoch 62/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1017 - val_loss: 0.0832
Epoch 63/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1015 - val_loss: 0.0832
Epoch 64/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1010 - val_loss: 0.0830
Epoch 65/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1007 - val_loss: 0.0809
Epoch 66/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1004 - val_loss: 0.0818
Epoch 67/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1001 - val_loss: 0.0820
Epoch 68/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0999 - val_loss: 0.0819
Epoch 69/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0995 - val_loss: 0.0808
Epoch 70/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0990 - val_loss: 0.0823
Epoch 71/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0990 - val_loss: 0.0806
Val_yp Shape is 
(75718, 24)
Results === Test == Validation ===== 72
=============================
step 1    -  0.991    0.990
step 2    -  0.984    0.983
step 3    -  0.976    0.974
step 4    -  0.967    0.965
step 5    -  0.957    0.956
step 6    -  0.948    0.947
step 7    -  0.939    0.939
step 8    -  0.930    0.930
step 9    -  0.922    0.922
step 10   -  0.913    0.915
step 11   -  0.905    0.908
step 12   -  0.898    0.901
step 13   -  0.891    0.894
step 14   -  0.884    0.888
step 15   -  0.877    0.881
step 16   -  0.872    0.876
step 17   -  0.866    0.871
step 18   -  0.862    0.867
step 19   -  0.856    0.862
step 20   -  0.852    0.858
step 21   -  0.847    0.854
step 22   -  0.842    0.849
step 23   -  0.837    0.845
step 24   -  0.832    0.841
=============================
Summary   -  21.651    21.715
V_y Shape is 
(75718, 1)
=========== SHAPE INPUT-OUTPUT MATRIXES =====================
shape x_training : (227201, 511)  y_training: :  (227201, 24)
shape x_test     :  (75717, 511)  y_test      :  (75717, 24)
shape x_val      :  (75718, 511)  y_val       :  (75718, 24)
=============================================================
(511,)
Model: "MLP"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_47 (InputLayer)       [(None, 511)]             0         
                                                                 
 dense_184 (Dense)           (None, 1024)              524288    
                                                                 
 elu_46 (ELU)                (None, 1024)              0         
                                                                 
 dropout_138 (Dropout)       (None, 1024)              0         
                                                                 
 dense_185 (Dense)           (None, 512)               524800    
                                                                 
 dropout_139 (Dropout)       (None, 512)               0         
                                                                 
 dense_186 (Dense)           (None, 512)               262656    
                                                                 
 dropout_140 (Dropout)       (None, 512)               0         
                                                                 
 dense_187 (Dense)           (None, 24)                12312     
                                                                 
=================================================================
Total params: 1,324,056
Trainable params: 1,324,056
Non-trainable params: 0
_________________________________________________________________
Epoch 1/200
222/222 [==============================] - 2s 8ms/step - loss: 0.4638 - val_loss: 0.1132
Epoch 2/200
222/222 [==============================] - 1s 6ms/step - loss: 0.2418 - val_loss: 0.1035
Epoch 3/200
222/222 [==============================] - 1s 6ms/step - loss: 0.2028 - val_loss: 0.1002
Epoch 4/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1833 - val_loss: 0.0987
Epoch 5/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1711 - val_loss: 0.0956
Epoch 6/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1624 - val_loss: 0.0950
Epoch 7/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1557 - val_loss: 0.0952
Epoch 8/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1500 - val_loss: 0.0942
Epoch 9/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1460 - val_loss: 0.0927
Epoch 10/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1420 - val_loss: 0.0924
Epoch 11/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1389 - val_loss: 0.0916
Epoch 12/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1360 - val_loss: 0.0925
Epoch 13/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1336 - val_loss: 0.0919
Epoch 14/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1312 - val_loss: 0.0905
Epoch 15/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1296 - val_loss: 0.0906
Epoch 16/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1276 - val_loss: 0.0878
Epoch 17/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1263 - val_loss: 0.0916
Epoch 18/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1246 - val_loss: 0.0872
Epoch 19/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1236 - val_loss: 0.0878
Epoch 20/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1224 - val_loss: 0.0894
Epoch 21/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1212 - val_loss: 0.0900
Epoch 22/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1205 - val_loss: 0.0887
Epoch 23/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1194 - val_loss: 0.0886
Epoch 24/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1187 - val_loss: 0.0887
Epoch 25/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1180 - val_loss: 0.0869
Epoch 26/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1172 - val_loss: 0.0870
Epoch 27/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1166 - val_loss: 0.0869
Epoch 28/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1159 - val_loss: 0.0867
Epoch 29/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1154 - val_loss: 0.0872
Epoch 30/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1151 - val_loss: 0.0871
Epoch 31/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1144 - val_loss: 0.0853
Epoch 32/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1139 - val_loss: 0.0853
Epoch 33/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1133 - val_loss: 0.0862
Epoch 34/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1130 - val_loss: 0.0859
Epoch 35/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1125 - val_loss: 0.0856
Epoch 36/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1121 - val_loss: 0.0861
Epoch 37/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1117 - val_loss: 0.0865
Epoch 38/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1114 - val_loss: 0.0856
Epoch 39/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1108 - val_loss: 0.0856
Epoch 40/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1105 - val_loss: 0.0850
Epoch 41/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1099 - val_loss: 0.0847
Epoch 42/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1096 - val_loss: 0.0839
Epoch 43/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1089 - val_loss: 0.0854
Epoch 44/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1085 - val_loss: 0.0851
Epoch 45/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1083 - val_loss: 0.0850
Epoch 46/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1076 - val_loss: 0.0839
Epoch 47/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1074 - val_loss: 0.0833
Epoch 48/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1070 - val_loss: 0.0842
Epoch 49/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1066 - val_loss: 0.0838
Epoch 50/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1060 - val_loss: 0.0829
Epoch 51/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1056 - val_loss: 0.0840
Epoch 52/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1050 - val_loss: 0.0840
Epoch 53/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1049 - val_loss: 0.0816
Epoch 54/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1043 - val_loss: 0.0823
Epoch 55/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1041 - val_loss: 0.0822
Epoch 56/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1038 - val_loss: 0.0841
Epoch 57/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1033 - val_loss: 0.0835
Epoch 58/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1030 - val_loss: 0.0826
Epoch 59/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1028 - val_loss: 0.0835
Epoch 60/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1022 - val_loss: 0.0827
Epoch 61/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1020 - val_loss: 0.0820
Epoch 62/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1015 - val_loss: 0.0807
Epoch 63/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1013 - val_loss: 0.0807
Epoch 64/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1012 - val_loss: 0.0817
Epoch 65/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1006 - val_loss: 0.0835
Epoch 66/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1003 - val_loss: 0.0823
Epoch 67/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0998 - val_loss: 0.0823
Epoch 68/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0999 - val_loss: 0.0825
Epoch 69/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0995 - val_loss: 0.0809
Epoch 70/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0990 - val_loss: 0.0823
Epoch 71/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0987 - val_loss: 0.0816
Epoch 72/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0984 - val_loss: 0.0806
Epoch 73/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0984 - val_loss: 0.0803
Epoch 74/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0981 - val_loss: 0.0836
Epoch 75/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0978 - val_loss: 0.0808
Epoch 76/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0976 - val_loss: 0.0808
Epoch 77/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0974 - val_loss: 0.0816
Epoch 78/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0970 - val_loss: 0.0809
Epoch 79/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0967 - val_loss: 0.0812
Epoch 80/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0964 - val_loss: 0.0799
Epoch 81/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0960 - val_loss: 0.0813
Epoch 82/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0961 - val_loss: 0.0805
Epoch 83/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0956 - val_loss: 0.0813
Epoch 84/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0954 - val_loss: 0.0820
Epoch 85/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0953 - val_loss: 0.0802
Epoch 86/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0949 - val_loss: 0.0828
Epoch 87/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0947 - val_loss: 0.0813
Epoch 88/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0946 - val_loss: 0.0804
Epoch 89/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0944 - val_loss: 0.0806
Epoch 90/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0940 - val_loss: 0.0809
Val_yp Shape is 
(75718, 24)
Results === Test == Validation ===== 73
=============================
step 1    -  0.991    0.990
step 2    -  0.984    0.983
step 3    -  0.976    0.974
step 4    -  0.967    0.965
step 5    -  0.958    0.956
step 6    -  0.949    0.948
step 7    -  0.940    0.940
step 8    -  0.931    0.932
step 9    -  0.923    0.924
step 10   -  0.915    0.917
step 11   -  0.908    0.911
step 12   -  0.902    0.905
step 13   -  0.895    0.899
step 14   -  0.888    0.892
step 15   -  0.882    0.886
step 16   -  0.876    0.881
step 17   -  0.871    0.876
step 18   -  0.865    0.870
step 19   -  0.860    0.866
step 20   -  0.856    0.862
step 21   -  0.851    0.858
step 22   -  0.846    0.854
step 23   -  0.841    0.849
step 24   -  0.836    0.845
=============================
Summary   -  21.712    21.786
V_y Shape is 
(75718, 1)
=========== SHAPE INPUT-OUTPUT MATRIXES =====================
shape x_training : (227200, 518)  y_training: :  (227200, 24)
shape x_test     :  (75717, 518)  y_test      :  (75717, 24)
shape x_val      :  (75717, 518)  y_val       :  (75717, 24)
=============================================================
(518,)
Model: "MLP"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_48 (InputLayer)       [(None, 518)]             0         
                                                                 
 dense_188 (Dense)           (None, 1024)              531456    
                                                                 
 elu_47 (ELU)                (None, 1024)              0         
                                                                 
 dropout_141 (Dropout)       (None, 1024)              0         
                                                                 
 dense_189 (Dense)           (None, 512)               524800    
                                                                 
 dropout_142 (Dropout)       (None, 512)               0         
                                                                 
 dense_190 (Dense)           (None, 512)               262656    
                                                                 
 dropout_143 (Dropout)       (None, 512)               0         
                                                                 
 dense_191 (Dense)           (None, 24)                12312     
                                                                 
=================================================================
Total params: 1,331,224
Trainable params: 1,331,224
Non-trainable params: 0
_________________________________________________________________
Epoch 1/200
222/222 [==============================] - 2s 8ms/step - loss: 0.4784 - val_loss: 0.1152
Epoch 2/200
222/222 [==============================] - 1s 6ms/step - loss: 0.2452 - val_loss: 0.1048
Epoch 3/200
222/222 [==============================] - 1s 6ms/step - loss: 0.2047 - val_loss: 0.1021
Epoch 4/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1847 - val_loss: 0.0974
Epoch 5/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1725 - val_loss: 0.0974
Epoch 6/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1635 - val_loss: 0.0965
Epoch 7/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1565 - val_loss: 0.0929
Epoch 8/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1512 - val_loss: 0.0943
Epoch 9/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1465 - val_loss: 0.0930
Epoch 10/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1428 - val_loss: 0.0919
Epoch 11/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1394 - val_loss: 0.0902
Epoch 12/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1367 - val_loss: 0.0896
Epoch 13/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1343 - val_loss: 0.0917
Epoch 14/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1319 - val_loss: 0.0889
Epoch 15/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1300 - val_loss: 0.0900
Epoch 16/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1282 - val_loss: 0.0902
Epoch 17/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1266 - val_loss: 0.0891
Epoch 18/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1249 - val_loss: 0.0881
Epoch 19/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1237 - val_loss: 0.0916
Epoch 20/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1224 - val_loss: 0.0889
Epoch 21/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1216 - val_loss: 0.0884
Epoch 22/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1205 - val_loss: 0.0867
Epoch 23/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1195 - val_loss: 0.0875
Epoch 24/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1189 - val_loss: 0.0883
Epoch 25/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1179 - val_loss: 0.0879
Epoch 26/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1176 - val_loss: 0.0868
Epoch 27/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1168 - val_loss: 0.0867
Epoch 28/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1160 - val_loss: 0.0878
Epoch 29/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1156 - val_loss: 0.0881
Epoch 30/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1147 - val_loss: 0.0871
Epoch 31/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1145 - val_loss: 0.0862
Epoch 32/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1139 - val_loss: 0.0856
Epoch 33/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1134 - val_loss: 0.0873
Epoch 34/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1129 - val_loss: 0.0864
Epoch 35/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1125 - val_loss: 0.0880
Epoch 36/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1118 - val_loss: 0.0858
Epoch 37/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1115 - val_loss: 0.0865
Epoch 38/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1112 - val_loss: 0.0855
Epoch 39/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1108 - val_loss: 0.0862
Epoch 40/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1103 - val_loss: 0.0852
Epoch 41/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1098 - val_loss: 0.0847
Epoch 42/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1093 - val_loss: 0.0829
Epoch 43/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1088 - val_loss: 0.0844
Epoch 44/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1086 - val_loss: 0.0845
Epoch 45/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1081 - val_loss: 0.0833
Epoch 46/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1077 - val_loss: 0.0860
Epoch 47/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1074 - val_loss: 0.0839
Epoch 48/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1068 - val_loss: 0.0842
Epoch 49/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1064 - val_loss: 0.0840
Epoch 50/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1059 - val_loss: 0.0830
Epoch 51/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1052 - val_loss: 0.0845
Epoch 52/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1051 - val_loss: 0.0835
Val_yp Shape is 
(75717, 24)
Results === Test == Validation ===== 74
=============================
step 1    -  0.990    0.989
step 2    -  0.983    0.981
step 3    -  0.974    0.972
step 4    -  0.965    0.963
step 5    -  0.955    0.953
step 6    -  0.945    0.943
step 7    -  0.935    0.934
step 8    -  0.926    0.925
step 9    -  0.918    0.917
step 10   -  0.910    0.910
step 11   -  0.903    0.903
step 12   -  0.895    0.896
step 13   -  0.889    0.890
step 14   -  0.883    0.884
step 15   -  0.877    0.879
step 16   -  0.871    0.873
step 17   -  0.865    0.867
step 18   -  0.860    0.863
step 19   -  0.854    0.857
step 20   -  0.848    0.852
step 21   -  0.842    0.847
step 22   -  0.837    0.843
step 23   -  0.833    0.839
step 24   -  0.827    0.833
=============================
Summary   -  21.583    21.614
V_y Shape is 
(75717, 1)
=========== SHAPE INPUT-OUTPUT MATRIXES =====================
shape x_training : (227199, 525)  y_training: :  (227199, 24)
shape x_test     :  (75716, 525)  y_test      :  (75716, 24)
shape x_val      :  (75717, 525)  y_val       :  (75717, 24)
=============================================================
(525,)
Model: "MLP"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_49 (InputLayer)       [(None, 525)]             0         
                                                                 
 dense_192 (Dense)           (None, 1024)              538624    
                                                                 
 elu_48 (ELU)                (None, 1024)              0         
                                                                 
 dropout_144 (Dropout)       (None, 1024)              0         
                                                                 
 dense_193 (Dense)           (None, 512)               524800    
                                                                 
 dropout_145 (Dropout)       (None, 512)               0         
                                                                 
 dense_194 (Dense)           (None, 512)               262656    
                                                                 
 dropout_146 (Dropout)       (None, 512)               0         
                                                                 
 dense_195 (Dense)           (None, 24)                12312     
                                                                 
=================================================================
Total params: 1,338,392
Trainable params: 1,338,392
Non-trainable params: 0
_________________________________________________________________
Epoch 1/200
222/222 [==============================] - 2s 7ms/step - loss: 0.4671 - val_loss: 0.1161
Epoch 2/200
222/222 [==============================] - 1s 6ms/step - loss: 0.2429 - val_loss: 0.1020
Epoch 3/200
222/222 [==============================] - 1s 6ms/step - loss: 0.2031 - val_loss: 0.0993
Epoch 4/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1840 - val_loss: 0.0985
Epoch 5/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1713 - val_loss: 0.0965
Epoch 6/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1628 - val_loss: 0.0963
Epoch 7/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1556 - val_loss: 0.0952
Epoch 8/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1502 - val_loss: 0.0941
Epoch 9/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1457 - val_loss: 0.0920
Epoch 10/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1418 - val_loss: 0.0928
Epoch 11/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1385 - val_loss: 0.0917
Epoch 12/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1360 - val_loss: 0.0900
Epoch 13/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1333 - val_loss: 0.0929
Epoch 14/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1313 - val_loss: 0.0899
Epoch 15/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1292 - val_loss: 0.0911
Epoch 16/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1276 - val_loss: 0.0896
Epoch 17/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1257 - val_loss: 0.0905
Epoch 18/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1246 - val_loss: 0.0903
Epoch 19/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1233 - val_loss: 0.0875
Epoch 20/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1220 - val_loss: 0.0902
Epoch 21/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1209 - val_loss: 0.0896
Epoch 22/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1202 - val_loss: 0.0869
Epoch 23/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1191 - val_loss: 0.0889
Epoch 24/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1183 - val_loss: 0.0882
Epoch 25/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1176 - val_loss: 0.0878
Epoch 26/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1171 - val_loss: 0.0880
Epoch 27/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1163 - val_loss: 0.0883
Epoch 28/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1155 - val_loss: 0.0880
Epoch 29/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1152 - val_loss: 0.0900
Epoch 30/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1145 - val_loss: 0.0873
Epoch 31/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1140 - val_loss: 0.0871
Epoch 32/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1134 - val_loss: 0.0875
Val_yp Shape is 
(75717, 24)
Results === Test == Validation ===== 75
=============================
step 1    -  0.989    0.989
step 2    -  0.982    0.981
step 3    -  0.973    0.972
step 4    -  0.963    0.962
step 5    -  0.953    0.952
step 6    -  0.943    0.942
step 7    -  0.933    0.933
step 8    -  0.924    0.924
step 9    -  0.915    0.916
step 10   -  0.907    0.908
step 11   -  0.899    0.900
step 12   -  0.891    0.893
step 13   -  0.884    0.886
step 14   -  0.877    0.880
step 15   -  0.870    0.873
step 16   -  0.864    0.868
step 17   -  0.860    0.864
step 18   -  0.855    0.859
step 19   -  0.850    0.856
step 20   -  0.847    0.853
step 21   -  0.843    0.849
step 22   -  0.839    0.846
step 23   -  0.835    0.842
step 24   -  0.830    0.838
=============================
Summary   -  21.527    21.587
V_y Shape is 
(75717, 1)
=========== SHAPE INPUT-OUTPUT MATRIXES =====================
shape x_training : (227198, 532)  y_training: :  (227198, 24)
shape x_test     :  (75716, 532)  y_test      :  (75716, 24)
shape x_val      :  (75716, 532)  y_val       :  (75716, 24)
=============================================================
(532,)
Model: "MLP"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_50 (InputLayer)       [(None, 532)]             0         
                                                                 
 dense_196 (Dense)           (None, 1024)              545792    
                                                                 
 elu_49 (ELU)                (None, 1024)              0         
                                                                 
 dropout_147 (Dropout)       (None, 1024)              0         
                                                                 
 dense_197 (Dense)           (None, 512)               524800    
                                                                 
 dropout_148 (Dropout)       (None, 512)               0         
                                                                 
 dense_198 (Dense)           (None, 512)               262656    
                                                                 
 dropout_149 (Dropout)       (None, 512)               0         
                                                                 
 dense_199 (Dense)           (None, 24)                12312     
                                                                 
=================================================================
Total params: 1,345,560
Trainable params: 1,345,560
Non-trainable params: 0
_________________________________________________________________
Epoch 1/200
222/222 [==============================] - 2s 7ms/step - loss: 0.4728 - val_loss: 0.1124
Epoch 2/200
222/222 [==============================] - 1s 6ms/step - loss: 0.2446 - val_loss: 0.1024
Epoch 3/200
222/222 [==============================] - 1s 6ms/step - loss: 0.2040 - val_loss: 0.0990
Epoch 4/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1845 - val_loss: 0.0986
Epoch 5/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1718 - val_loss: 0.0954
Epoch 6/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1632 - val_loss: 0.0961
Epoch 7/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1560 - val_loss: 0.0932
Epoch 8/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1504 - val_loss: 0.0945
Epoch 9/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1460 - val_loss: 0.0922
Epoch 10/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1420 - val_loss: 0.0906
Epoch 11/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1388 - val_loss: 0.0928
Epoch 12/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1359 - val_loss: 0.0914
Epoch 13/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1337 - val_loss: 0.0900
Epoch 14/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1310 - val_loss: 0.0898
Epoch 15/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1294 - val_loss: 0.0890
Epoch 16/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1274 - val_loss: 0.0913
Epoch 17/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1259 - val_loss: 0.0905
Epoch 18/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1245 - val_loss: 0.0904
Epoch 19/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1230 - val_loss: 0.0871
Epoch 20/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1222 - val_loss: 0.0878
Epoch 21/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1210 - val_loss: 0.0879
Epoch 22/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1200 - val_loss: 0.0898
Epoch 23/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1194 - val_loss: 0.0895
Epoch 24/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1182 - val_loss: 0.0870
Epoch 25/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1175 - val_loss: 0.0872
Epoch 26/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1169 - val_loss: 0.0872
Epoch 27/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1161 - val_loss: 0.0855
Epoch 28/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1156 - val_loss: 0.0873
Epoch 29/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1150 - val_loss: 0.0886
Epoch 30/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1146 - val_loss: 0.0872
Epoch 31/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1140 - val_loss: 0.0876
Epoch 32/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1136 - val_loss: 0.0863
Epoch 33/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1131 - val_loss: 0.0876
Epoch 34/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1124 - val_loss: 0.0860
Epoch 35/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1120 - val_loss: 0.0854
Epoch 36/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1118 - val_loss: 0.0860
Epoch 37/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1112 - val_loss: 0.0876
Epoch 38/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1109 - val_loss: 0.0860
Epoch 39/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1102 - val_loss: 0.0844
Epoch 40/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1099 - val_loss: 0.0878
Epoch 41/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1095 - val_loss: 0.0845
Epoch 42/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1089 - val_loss: 0.0868
Epoch 43/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1086 - val_loss: 0.0863
Epoch 44/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1080 - val_loss: 0.0847
Epoch 45/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1076 - val_loss: 0.0856
Epoch 46/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1072 - val_loss: 0.0848
Epoch 47/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1067 - val_loss: 0.0845
Epoch 48/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1064 - val_loss: 0.0839
Epoch 49/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1060 - val_loss: 0.0841
Epoch 50/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1056 - val_loss: 0.0843
Epoch 51/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1050 - val_loss: 0.0851
Epoch 52/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1046 - val_loss: 0.0831
Epoch 53/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1044 - val_loss: 0.0851
Epoch 54/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1042 - val_loss: 0.0842
Epoch 55/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1033 - val_loss: 0.0815
Epoch 56/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1029 - val_loss: 0.0838
Epoch 57/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1028 - val_loss: 0.0839
Epoch 58/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1022 - val_loss: 0.0824
Epoch 59/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1020 - val_loss: 0.0828
Epoch 60/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1015 - val_loss: 0.0819
Epoch 61/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1013 - val_loss: 0.0807
Epoch 62/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1008 - val_loss: 0.0854
Epoch 63/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1005 - val_loss: 0.0816
Epoch 64/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1001 - val_loss: 0.0822
Epoch 65/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0998 - val_loss: 0.0833
Epoch 66/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0997 - val_loss: 0.0816
Epoch 67/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0993 - val_loss: 0.0803
Epoch 68/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0990 - val_loss: 0.0823
Epoch 69/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0985 - val_loss: 0.0820
Epoch 70/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0984 - val_loss: 0.0815
Epoch 71/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0981 - val_loss: 0.0818
Epoch 72/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0978 - val_loss: 0.0814
Epoch 73/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0973 - val_loss: 0.0815
Epoch 74/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0971 - val_loss: 0.0818
Epoch 75/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0968 - val_loss: 0.0810
Epoch 76/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0966 - val_loss: 0.0814
Epoch 77/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0965 - val_loss: 0.0803
Epoch 78/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0960 - val_loss: 0.0819
Epoch 79/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0957 - val_loss: 0.0808
Epoch 80/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0956 - val_loss: 0.0800
Epoch 81/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0953 - val_loss: 0.0813
Epoch 82/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0952 - val_loss: 0.0815
Epoch 83/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0947 - val_loss: 0.0827
Epoch 84/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0947 - val_loss: 0.0812
Epoch 85/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0944 - val_loss: 0.0815
Epoch 86/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0940 - val_loss: 0.0811
Epoch 87/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0938 - val_loss: 0.0821
Epoch 88/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0935 - val_loss: 0.0817
Epoch 89/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0932 - val_loss: 0.0808
Epoch 90/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0931 - val_loss: 0.0810
Val_yp Shape is 
(75716, 24)
Results === Test == Validation ===== 76
=============================
step 1    -  0.991    0.990
step 2    -  0.984    0.982
step 3    -  0.975    0.973
step 4    -  0.965    0.963
step 5    -  0.955    0.953
step 6    -  0.946    0.945
step 7    -  0.937    0.936
step 8    -  0.928    0.927
step 9    -  0.919    0.919
step 10   -  0.911    0.912
step 11   -  0.902    0.904
step 12   -  0.895    0.897
step 13   -  0.887    0.889
step 14   -  0.880    0.882
step 15   -  0.874    0.877
step 16   -  0.868    0.872
step 17   -  0.863    0.867
step 18   -  0.859    0.864
step 19   -  0.856    0.861
step 20   -  0.852    0.857
step 21   -  0.849    0.855
step 22   -  0.844    0.851
step 23   -  0.839    0.847
step 24   -  0.834    0.843
=============================
Summary   -  21.615    21.666
V_y Shape is 
(75716, 1)
=========== SHAPE INPUT-OUTPUT MATRIXES =====================
shape x_training : (227197, 539)  y_training: :  (227197, 24)
shape x_test     :  (75715, 539)  y_test      :  (75715, 24)
shape x_val      :  (75716, 539)  y_val       :  (75716, 24)
=============================================================
(539,)
Model: "MLP"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_51 (InputLayer)       [(None, 539)]             0         
                                                                 
 dense_200 (Dense)           (None, 1024)              552960    
                                                                 
 elu_50 (ELU)                (None, 1024)              0         
                                                                 
 dropout_150 (Dropout)       (None, 1024)              0         
                                                                 
 dense_201 (Dense)           (None, 512)               524800    
                                                                 
 dropout_151 (Dropout)       (None, 512)               0         
                                                                 
 dense_202 (Dense)           (None, 512)               262656    
                                                                 
 dropout_152 (Dropout)       (None, 512)               0         
                                                                 
 dense_203 (Dense)           (None, 24)                12312     
                                                                 
=================================================================
Total params: 1,352,728
Trainable params: 1,352,728
Non-trainable params: 0
_________________________________________________________________
Epoch 1/200
222/222 [==============================] - 2s 8ms/step - loss: 0.4784 - val_loss: 0.1133
Epoch 2/200
222/222 [==============================] - 1s 6ms/step - loss: 0.2456 - val_loss: 0.1023
Epoch 3/200
222/222 [==============================] - 1s 7ms/step - loss: 0.2044 - val_loss: 0.1000
Epoch 4/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1843 - val_loss: 0.0961
Epoch 5/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1718 - val_loss: 0.0981
Epoch 6/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1629 - val_loss: 0.0932
Epoch 7/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1559 - val_loss: 0.0940
Epoch 8/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1505 - val_loss: 0.0932
Epoch 9/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1455 - val_loss: 0.0929
Epoch 10/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1420 - val_loss: 0.0916
Epoch 11/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1387 - val_loss: 0.0948
Epoch 12/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1361 - val_loss: 0.0937
Epoch 13/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1333 - val_loss: 0.0923
Epoch 14/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1311 - val_loss: 0.0889
Epoch 15/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1290 - val_loss: 0.0931
Epoch 16/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1271 - val_loss: 0.0894
Epoch 17/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1257 - val_loss: 0.0880
Epoch 18/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1244 - val_loss: 0.0914
Epoch 19/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1231 - val_loss: 0.0909
Epoch 20/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1218 - val_loss: 0.0877
Epoch 21/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1207 - val_loss: 0.0876
Epoch 22/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1197 - val_loss: 0.0891
Epoch 23/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1188 - val_loss: 0.0886
Epoch 24/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1181 - val_loss: 0.0894
Epoch 25/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1172 - val_loss: 0.0902
Epoch 26/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1165 - val_loss: 0.0875
Epoch 27/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1158 - val_loss: 0.0876
Epoch 28/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1155 - val_loss: 0.0875
Epoch 29/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1148 - val_loss: 0.0864
Epoch 30/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1142 - val_loss: 0.0854
Epoch 31/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1138 - val_loss: 0.0853
Epoch 32/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1134 - val_loss: 0.0888
Epoch 33/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1128 - val_loss: 0.0858
Epoch 34/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1124 - val_loss: 0.0860
Epoch 35/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1119 - val_loss: 0.0883
Epoch 36/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1113 - val_loss: 0.0865
Epoch 37/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1108 - val_loss: 0.0857
Epoch 38/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1105 - val_loss: 0.0852
Epoch 39/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1102 - val_loss: 0.0862
Epoch 40/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1097 - val_loss: 0.0864
Epoch 41/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1090 - val_loss: 0.0845
Epoch 42/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1088 - val_loss: 0.0844
Epoch 43/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1082 - val_loss: 0.0856
Epoch 44/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1078 - val_loss: 0.0851
Epoch 45/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1075 - val_loss: 0.0849
Epoch 46/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1068 - val_loss: 0.0840
Epoch 47/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1067 - val_loss: 0.0846
Epoch 48/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1062 - val_loss: 0.0844
Epoch 49/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1059 - val_loss: 0.0856
Epoch 50/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1053 - val_loss: 0.0837
Epoch 51/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1049 - val_loss: 0.0829
Epoch 52/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1044 - val_loss: 0.0825
Epoch 53/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1041 - val_loss: 0.0817
Epoch 54/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1038 - val_loss: 0.0831
Epoch 55/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1034 - val_loss: 0.0821
Epoch 56/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1029 - val_loss: 0.0824
Epoch 57/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1025 - val_loss: 0.0828
Epoch 58/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1019 - val_loss: 0.0824
Epoch 59/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1018 - val_loss: 0.0827
Epoch 60/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1016 - val_loss: 0.0811
Epoch 61/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1011 - val_loss: 0.0822
Epoch 62/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1005 - val_loss: 0.0816
Epoch 63/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1001 - val_loss: 0.0840
Epoch 64/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1000 - val_loss: 0.0812
Epoch 65/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0994 - val_loss: 0.0803
Epoch 66/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0994 - val_loss: 0.0821
Epoch 67/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0989 - val_loss: 0.0813
Epoch 68/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0988 - val_loss: 0.0814
Epoch 69/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0984 - val_loss: 0.0820
Epoch 70/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0981 - val_loss: 0.0815
Epoch 71/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0979 - val_loss: 0.0813
Epoch 72/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0973 - val_loss: 0.0817
Epoch 73/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0972 - val_loss: 0.0828
Epoch 74/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0970 - val_loss: 0.0810
Epoch 75/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0967 - val_loss: 0.0830
Val_yp Shape is 
(75716, 24)
Results === Test == Validation ===== 77
=============================
step 1    -  0.990    0.989
step 2    -  0.984    0.982
step 3    -  0.976    0.974
step 4    -  0.967    0.965
step 5    -  0.957    0.956
step 6    -  0.948    0.947
step 7    -  0.938    0.938
step 8    -  0.929    0.929
step 9    -  0.919    0.919
step 10   -  0.910    0.911
step 11   -  0.902    0.904
step 12   -  0.895    0.897
step 13   -  0.889    0.892
step 14   -  0.883    0.886
step 15   -  0.878    0.881
step 16   -  0.873    0.877
step 17   -  0.870    0.874
step 18   -  0.866    0.871
step 19   -  0.862    0.868
step 20   -  0.858    0.864
step 21   -  0.853    0.859
step 22   -  0.848    0.855
step 23   -  0.842    0.850
step 24   -  0.837    0.845
=============================
Summary   -  21.673    21.733
V_y Shape is 
(75716, 1)
=========== SHAPE INPUT-OUTPUT MATRIXES =====================
shape x_training : (227196, 546)  y_training: :  (227196, 24)
shape x_test     :  (75715, 546)  y_test      :  (75715, 24)
shape x_val      :  (75715, 546)  y_val       :  (75715, 24)
=============================================================
(546,)
Model: "MLP"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_52 (InputLayer)       [(None, 546)]             0         
                                                                 
 dense_204 (Dense)           (None, 1024)              560128    
                                                                 
 elu_51 (ELU)                (None, 1024)              0         
                                                                 
 dropout_153 (Dropout)       (None, 1024)              0         
                                                                 
 dense_205 (Dense)           (None, 512)               524800    
                                                                 
 dropout_154 (Dropout)       (None, 512)               0         
                                                                 
 dense_206 (Dense)           (None, 512)               262656    
                                                                 
 dropout_155 (Dropout)       (None, 512)               0         
                                                                 
 dense_207 (Dense)           (None, 24)                12312     
                                                                 
=================================================================
Total params: 1,359,896
Trainable params: 1,359,896
Non-trainable params: 0
_________________________________________________________________
Epoch 1/200
222/222 [==============================] - 2s 7ms/step - loss: 0.4748 - val_loss: 0.1124
Epoch 2/200
222/222 [==============================] - 1s 6ms/step - loss: 0.2453 - val_loss: 0.1048
Epoch 3/200
222/222 [==============================] - 1s 6ms/step - loss: 0.2042 - val_loss: 0.1000
Epoch 4/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1841 - val_loss: 0.0985
Epoch 5/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1715 - val_loss: 0.0952
Epoch 6/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1627 - val_loss: 0.0947
Epoch 7/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1556 - val_loss: 0.0960
Epoch 8/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1503 - val_loss: 0.0972
Epoch 9/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1456 - val_loss: 0.0937
Epoch 10/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1418 - val_loss: 0.0927
Epoch 11/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1386 - val_loss: 0.0926
Epoch 12/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1355 - val_loss: 0.0907
Epoch 13/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1332 - val_loss: 0.0902
Epoch 14/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1309 - val_loss: 0.0913
Epoch 15/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1289 - val_loss: 0.0891
Epoch 16/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1272 - val_loss: 0.0904
Epoch 17/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1254 - val_loss: 0.0928
Epoch 18/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1239 - val_loss: 0.0897
Epoch 19/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1230 - val_loss: 0.0894
Epoch 20/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1215 - val_loss: 0.0891
Epoch 21/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1205 - val_loss: 0.0870
Epoch 22/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1194 - val_loss: 0.0875
Epoch 23/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1187 - val_loss: 0.0870
Epoch 24/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1177 - val_loss: 0.0881
Epoch 25/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1169 - val_loss: 0.0877
Epoch 26/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1163 - val_loss: 0.0875
Epoch 27/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1156 - val_loss: 0.0871
Epoch 28/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1151 - val_loss: 0.0866
Epoch 29/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1147 - val_loss: 0.0869
Epoch 30/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1139 - val_loss: 0.0884
Epoch 31/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1135 - val_loss: 0.0864
Epoch 32/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1130 - val_loss: 0.0877
Epoch 33/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1124 - val_loss: 0.0880
Epoch 34/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1121 - val_loss: 0.0865
Epoch 35/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1115 - val_loss: 0.0863
Epoch 36/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1111 - val_loss: 0.0861
Epoch 37/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1106 - val_loss: 0.0866
Epoch 38/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1101 - val_loss: 0.0856
Epoch 39/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1096 - val_loss: 0.0856
Epoch 40/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1092 - val_loss: 0.0870
Epoch 41/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1089 - val_loss: 0.0838
Epoch 42/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1083 - val_loss: 0.0849
Epoch 43/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1079 - val_loss: 0.0844
Epoch 44/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1073 - val_loss: 0.0839
Epoch 45/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1067 - val_loss: 0.0851
Epoch 46/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1064 - val_loss: 0.0834
Epoch 47/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1059 - val_loss: 0.0842
Epoch 48/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1055 - val_loss: 0.0848
Epoch 49/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1052 - val_loss: 0.0839
Epoch 50/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1046 - val_loss: 0.0849
Epoch 51/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1043 - val_loss: 0.0855
Epoch 52/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1040 - val_loss: 0.0833
Epoch 53/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1033 - val_loss: 0.0826
Epoch 54/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1032 - val_loss: 0.0825
Epoch 55/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1028 - val_loss: 0.0837
Epoch 56/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1024 - val_loss: 0.0836
Epoch 57/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1020 - val_loss: 0.0819
Epoch 58/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1013 - val_loss: 0.0827
Epoch 59/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1009 - val_loss: 0.0822
Epoch 60/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1009 - val_loss: 0.0837
Epoch 61/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1005 - val_loss: 0.0815
Epoch 62/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1000 - val_loss: 0.0821
Epoch 63/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0998 - val_loss: 0.0822
Epoch 64/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0994 - val_loss: 0.0821
Epoch 65/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0992 - val_loss: 0.0818
Epoch 66/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0988 - val_loss: 0.0819
Epoch 67/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0983 - val_loss: 0.0829
Epoch 68/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0980 - val_loss: 0.0811
Epoch 69/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0978 - val_loss: 0.0828
Epoch 70/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0974 - val_loss: 0.0808
Epoch 71/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0972 - val_loss: 0.0820
Epoch 72/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0969 - val_loss: 0.0804
Epoch 73/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0968 - val_loss: 0.0834
Epoch 74/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0964 - val_loss: 0.0815
Epoch 75/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0959 - val_loss: 0.0813
Epoch 76/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0959 - val_loss: 0.0821
Epoch 77/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0955 - val_loss: 0.0821
Epoch 78/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0952 - val_loss: 0.0815
Epoch 79/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0952 - val_loss: 0.0821
Epoch 80/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0947 - val_loss: 0.0814
Epoch 81/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0946 - val_loss: 0.0809
Epoch 82/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0942 - val_loss: 0.0844
Val_yp Shape is 
(75715, 24)
Results === Test == Validation ===== 78
=============================
step 1    -  0.990    0.989
step 2    -  0.983    0.981
step 3    -  0.975    0.973
step 4    -  0.966    0.964
step 5    -  0.957    0.955
step 6    -  0.948    0.946
step 7    -  0.939    0.937
step 8    -  0.930    0.929
step 9    -  0.922    0.922
step 10   -  0.915    0.914
step 11   -  0.907    0.907
step 12   -  0.901    0.901
step 13   -  0.895    0.895
step 14   -  0.889    0.890
step 15   -  0.884    0.885
step 16   -  0.878    0.880
step 17   -  0.873    0.874
step 18   -  0.868    0.871
step 19   -  0.863    0.866
step 20   -  0.859    0.862
step 21   -  0.855    0.859
step 22   -  0.850    0.856
step 23   -  0.846    0.852
step 24   -  0.841    0.847
=============================
Summary   -  21.734    21.756
V_y Shape is 
(75715, 1)
=========== SHAPE INPUT-OUTPUT MATRIXES =====================
shape x_training : (227195, 553)  y_training: :  (227195, 24)
shape x_test     :  (75714, 553)  y_test      :  (75714, 24)
shape x_val      :  (75715, 553)  y_val       :  (75715, 24)
=============================================================
(553,)
Model: "MLP"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_53 (InputLayer)       [(None, 553)]             0         
                                                                 
 dense_208 (Dense)           (None, 1024)              567296    
                                                                 
 elu_52 (ELU)                (None, 1024)              0         
                                                                 
 dropout_156 (Dropout)       (None, 1024)              0         
                                                                 
 dense_209 (Dense)           (None, 512)               524800    
                                                                 
 dropout_157 (Dropout)       (None, 512)               0         
                                                                 
 dense_210 (Dense)           (None, 512)               262656    
                                                                 
 dropout_158 (Dropout)       (None, 512)               0         
                                                                 
 dense_211 (Dense)           (None, 24)                12312     
                                                                 
=================================================================
Total params: 1,367,064
Trainable params: 1,367,064
Non-trainable params: 0
_________________________________________________________________
Epoch 1/200
222/222 [==============================] - 2s 7ms/step - loss: 0.4775 - val_loss: 0.1122
Epoch 2/200
222/222 [==============================] - 1s 6ms/step - loss: 0.2464 - val_loss: 0.1045
Epoch 3/200
222/222 [==============================] - 1s 6ms/step - loss: 0.2051 - val_loss: 0.0991
Epoch 4/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1853 - val_loss: 0.0992
Epoch 5/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1724 - val_loss: 0.0960
Epoch 6/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1631 - val_loss: 0.0957
Epoch 7/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1562 - val_loss: 0.0928
Epoch 8/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1506 - val_loss: 0.0916
Epoch 9/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1457 - val_loss: 0.0924
Epoch 10/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1418 - val_loss: 0.0919
Epoch 11/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1387 - val_loss: 0.0926
Epoch 12/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1357 - val_loss: 0.0904
Epoch 13/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1333 - val_loss: 0.0915
Epoch 14/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1311 - val_loss: 0.0916
Epoch 15/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1290 - val_loss: 0.0918
Epoch 16/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1270 - val_loss: 0.0889
Epoch 17/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1257 - val_loss: 0.0892
Epoch 18/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1241 - val_loss: 0.0887
Epoch 19/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1228 - val_loss: 0.0899
Epoch 20/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1215 - val_loss: 0.0893
Epoch 21/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1204 - val_loss: 0.0870
Epoch 22/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1196 - val_loss: 0.0889
Epoch 23/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1186 - val_loss: 0.0879
Epoch 24/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1178 - val_loss: 0.0903
Epoch 25/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1170 - val_loss: 0.0878
Epoch 26/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1163 - val_loss: 0.0872
Epoch 27/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1159 - val_loss: 0.0888
Epoch 28/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1151 - val_loss: 0.0862
Epoch 29/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1145 - val_loss: 0.0866
Epoch 30/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1140 - val_loss: 0.0894
Epoch 31/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1135 - val_loss: 0.0863
Epoch 32/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1128 - val_loss: 0.0866
Epoch 33/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1125 - val_loss: 0.0874
Epoch 34/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1120 - val_loss: 0.0859
Epoch 35/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1114 - val_loss: 0.0854
Epoch 36/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1110 - val_loss: 0.0865
Epoch 37/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1103 - val_loss: 0.0878
Epoch 38/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1100 - val_loss: 0.0853
Epoch 39/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1096 - val_loss: 0.0860
Epoch 40/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1091 - val_loss: 0.0853
Epoch 41/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1087 - val_loss: 0.0846
Epoch 42/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1081 - val_loss: 0.0850
Epoch 43/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1077 - val_loss: 0.0835
Epoch 44/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1073 - val_loss: 0.0839
Epoch 45/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1067 - val_loss: 0.0841
Epoch 46/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1062 - val_loss: 0.0847
Epoch 47/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1057 - val_loss: 0.0865
Epoch 48/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1054 - val_loss: 0.0854
Epoch 49/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1049 - val_loss: 0.0839
Epoch 50/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1045 - val_loss: 0.0839
Epoch 51/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1041 - val_loss: 0.0836
Epoch 52/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1038 - val_loss: 0.0841
Epoch 53/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1032 - val_loss: 0.0844
Val_yp Shape is 
(75715, 24)
Results === Test == Validation ===== 79
=============================
step 1    -  0.990    0.989
step 2    -  0.983    0.982
step 3    -  0.975    0.973
step 4    -  0.965    0.963
step 5    -  0.956    0.953
step 6    -  0.946    0.944
step 7    -  0.937    0.935
step 8    -  0.928    0.927
step 9    -  0.919    0.919
step 10   -  0.911    0.911
step 11   -  0.903    0.904
step 12   -  0.896    0.897
step 13   -  0.889    0.890
step 14   -  0.883    0.885
step 15   -  0.878    0.880
step 16   -  0.873    0.875
step 17   -  0.868    0.871
step 18   -  0.864    0.867
step 19   -  0.859    0.863
step 20   -  0.854    0.859
step 21   -  0.850    0.856
step 22   -  0.846    0.852
step 23   -  0.842    0.849
step 24   -  0.836    0.844
=============================
Summary   -  21.650    21.686
V_y Shape is 
(75715, 1)
=========== SHAPE INPUT-OUTPUT MATRIXES =====================
shape x_training : (227194, 560)  y_training: :  (227194, 24)
shape x_test     :  (75714, 560)  y_test      :  (75714, 24)
shape x_val      :  (75714, 560)  y_val       :  (75714, 24)
=============================================================
(560,)
Model: "MLP"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_54 (InputLayer)       [(None, 560)]             0         
                                                                 
 dense_212 (Dense)           (None, 1024)              574464    
                                                                 
 elu_53 (ELU)                (None, 1024)              0         
                                                                 
 dropout_159 (Dropout)       (None, 1024)              0         
                                                                 
 dense_213 (Dense)           (None, 512)               524800    
                                                                 
 dropout_160 (Dropout)       (None, 512)               0         
                                                                 
 dense_214 (Dense)           (None, 512)               262656    
                                                                 
 dropout_161 (Dropout)       (None, 512)               0         
                                                                 
 dense_215 (Dense)           (None, 24)                12312     
                                                                 
=================================================================
Total params: 1,374,232
Trainable params: 1,374,232
Non-trainable params: 0
_________________________________________________________________
Epoch 1/200
222/222 [==============================] - 2s 7ms/step - loss: 0.4819 - val_loss: 0.1122
Epoch 2/200
222/222 [==============================] - 1s 6ms/step - loss: 0.2467 - val_loss: 0.1060
Epoch 3/200
222/222 [==============================] - 1s 6ms/step - loss: 0.2056 - val_loss: 0.1049
Epoch 4/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1855 - val_loss: 0.0996
Epoch 5/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1725 - val_loss: 0.0964
Epoch 6/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1634 - val_loss: 0.0951
Epoch 7/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1562 - val_loss: 0.0940
Epoch 8/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1507 - val_loss: 0.0916
Epoch 9/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1462 - val_loss: 0.0925
Epoch 10/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1422 - val_loss: 0.0920
Epoch 11/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1389 - val_loss: 0.0932
Epoch 12/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1359 - val_loss: 0.0911
Epoch 13/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1332 - val_loss: 0.0948
Epoch 14/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1311 - val_loss: 0.0905
Epoch 15/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1290 - val_loss: 0.0913
Epoch 16/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1273 - val_loss: 0.0888
Epoch 17/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1255 - val_loss: 0.0886
Epoch 18/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1245 - val_loss: 0.0904
Epoch 19/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1227 - val_loss: 0.0898
Epoch 20/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1214 - val_loss: 0.0890
Epoch 21/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1205 - val_loss: 0.0927
Epoch 22/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1195 - val_loss: 0.0887
Epoch 23/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1186 - val_loss: 0.0895
Epoch 24/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1180 - val_loss: 0.0887
Epoch 25/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1169 - val_loss: 0.0887
Epoch 26/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1162 - val_loss: 0.0878
Epoch 27/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1157 - val_loss: 0.0884
Epoch 28/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1148 - val_loss: 0.0886
Epoch 29/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1143 - val_loss: 0.0881
Epoch 30/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1139 - val_loss: 0.0871
Epoch 31/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1134 - val_loss: 0.0887
Epoch 32/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1129 - val_loss: 0.0878
Epoch 33/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1123 - val_loss: 0.0873
Epoch 34/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1120 - val_loss: 0.0856
Epoch 35/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1113 - val_loss: 0.0857
Epoch 36/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1108 - val_loss: 0.0869
Epoch 37/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1104 - val_loss: 0.0859
Epoch 38/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1100 - val_loss: 0.0867
Epoch 39/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1094 - val_loss: 0.0873
Epoch 40/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1091 - val_loss: 0.0848
Epoch 41/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1086 - val_loss: 0.0842
Epoch 42/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1082 - val_loss: 0.0835
Epoch 43/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1076 - val_loss: 0.0868
Epoch 44/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1072 - val_loss: 0.0863
Epoch 45/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1068 - val_loss: 0.0854
Epoch 46/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1063 - val_loss: 0.0840
Epoch 47/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1056 - val_loss: 0.0838
Epoch 48/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1052 - val_loss: 0.0831
Epoch 49/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1049 - val_loss: 0.0848
Epoch 50/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1043 - val_loss: 0.0843
Epoch 51/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1041 - val_loss: 0.0825
Epoch 52/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1035 - val_loss: 0.0844
Epoch 53/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1029 - val_loss: 0.0831
Epoch 54/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1026 - val_loss: 0.0821
Epoch 55/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1023 - val_loss: 0.0834
Epoch 56/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1019 - val_loss: 0.0841
Epoch 57/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1014 - val_loss: 0.0823
Epoch 58/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1011 - val_loss: 0.0824
Epoch 59/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1006 - val_loss: 0.0817
Epoch 60/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1003 - val_loss: 0.0826
Epoch 61/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1001 - val_loss: 0.0835
Epoch 62/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0997 - val_loss: 0.0845
Epoch 63/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0991 - val_loss: 0.0809
Epoch 64/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0989 - val_loss: 0.0837
Epoch 65/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0984 - val_loss: 0.0841
Epoch 66/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0983 - val_loss: 0.0823
Epoch 67/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0978 - val_loss: 0.0821
Epoch 68/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0976 - val_loss: 0.0828
Epoch 69/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0975 - val_loss: 0.0820
Epoch 70/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0970 - val_loss: 0.0809
Epoch 71/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0966 - val_loss: 0.0820
Epoch 72/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0963 - val_loss: 0.0816
Epoch 73/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0962 - val_loss: 0.0815
Epoch 74/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0961 - val_loss: 0.0828
Epoch 75/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0956 - val_loss: 0.0835
Epoch 76/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0953 - val_loss: 0.0827
Epoch 77/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0953 - val_loss: 0.0829
Epoch 78/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0949 - val_loss: 0.0811
Epoch 79/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0944 - val_loss: 0.0826
Epoch 80/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0942 - val_loss: 0.0817
Val_yp Shape is 
(75714, 24)
Results === Test == Validation ===== 80
=============================
step 1    -  0.991    0.990
step 2    -  0.984    0.983
step 3    -  0.976    0.974
step 4    -  0.967    0.965
step 5    -  0.957    0.956
step 6    -  0.948    0.946
step 7    -  0.939    0.938
step 8    -  0.930    0.930
step 9    -  0.922    0.923
step 10   -  0.914    0.916
step 11   -  0.907    0.909
step 12   -  0.900    0.902
step 13   -  0.893    0.896
step 14   -  0.886    0.889
step 15   -  0.880    0.884
step 16   -  0.874    0.877
step 17   -  0.868    0.872
step 18   -  0.862    0.866
step 19   -  0.855    0.860
step 20   -  0.848    0.853
step 21   -  0.843    0.849
step 22   -  0.839    0.846
step 23   -  0.833    0.841
step 24   -  0.826    0.835
=============================
Summary   -  21.644    21.701
V_y Shape is 
(75714, 1)
=========== SHAPE INPUT-OUTPUT MATRIXES =====================
shape x_training : (227193, 567)  y_training: :  (227193, 24)
shape x_test     :  (75713, 567)  y_test      :  (75713, 24)
shape x_val      :  (75714, 567)  y_val       :  (75714, 24)
=============================================================
(567,)
Model: "MLP"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_55 (InputLayer)       [(None, 567)]             0         
                                                                 
 dense_216 (Dense)           (None, 1024)              581632    
                                                                 
 elu_54 (ELU)                (None, 1024)              0         
                                                                 
 dropout_162 (Dropout)       (None, 1024)              0         
                                                                 
 dense_217 (Dense)           (None, 512)               524800    
                                                                 
 dropout_163 (Dropout)       (None, 512)               0         
                                                                 
 dense_218 (Dense)           (None, 512)               262656    
                                                                 
 dropout_164 (Dropout)       (None, 512)               0         
                                                                 
 dense_219 (Dense)           (None, 24)                12312     
                                                                 
=================================================================
Total params: 1,381,400
Trainable params: 1,381,400
Non-trainable params: 0
_________________________________________________________________
Epoch 1/200
222/222 [==============================] - 2s 7ms/step - loss: 0.4773 - val_loss: 0.1153
Epoch 2/200
222/222 [==============================] - 1s 6ms/step - loss: 0.2450 - val_loss: 0.1033
Epoch 3/200
222/222 [==============================] - 1s 6ms/step - loss: 0.2051 - val_loss: 0.1016
Epoch 4/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1847 - val_loss: 0.0998
Epoch 5/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1722 - val_loss: 0.0993
Epoch 6/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1632 - val_loss: 0.0964
Epoch 7/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1561 - val_loss: 0.0954
Epoch 8/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1506 - val_loss: 0.0924
Epoch 9/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1456 - val_loss: 0.0931
Epoch 10/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1420 - val_loss: 0.0944
Epoch 11/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1386 - val_loss: 0.0938
Epoch 12/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1354 - val_loss: 0.0915
Epoch 13/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1329 - val_loss: 0.0905
Epoch 14/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1306 - val_loss: 0.0886
Epoch 15/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1288 - val_loss: 0.0888
Epoch 16/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1270 - val_loss: 0.0916
Epoch 17/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1251 - val_loss: 0.0886
Epoch 18/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1240 - val_loss: 0.0904
Epoch 19/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1224 - val_loss: 0.0901
Epoch 20/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1213 - val_loss: 0.0908
Epoch 21/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1203 - val_loss: 0.0884
Epoch 22/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1193 - val_loss: 0.0900
Epoch 23/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1183 - val_loss: 0.0884
Epoch 24/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1176 - val_loss: 0.0861
Epoch 25/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1166 - val_loss: 0.0870
Epoch 26/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1159 - val_loss: 0.0881
Epoch 27/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1153 - val_loss: 0.0893
Epoch 28/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1147 - val_loss: 0.0860
Epoch 29/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1140 - val_loss: 0.0873
Epoch 30/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1136 - val_loss: 0.0860
Epoch 31/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1128 - val_loss: 0.0856
Epoch 32/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1128 - val_loss: 0.0854
Epoch 33/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1120 - val_loss: 0.0877
Epoch 34/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1117 - val_loss: 0.0858
Epoch 35/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1110 - val_loss: 0.0853
Epoch 36/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1104 - val_loss: 0.0860
Epoch 37/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1102 - val_loss: 0.0853
Epoch 38/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1096 - val_loss: 0.0863
Epoch 39/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1089 - val_loss: 0.0852
Epoch 40/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1087 - val_loss: 0.0864
Epoch 41/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1081 - val_loss: 0.0868
Epoch 42/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1076 - val_loss: 0.0855
Epoch 43/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1071 - val_loss: 0.0871
Epoch 44/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1067 - val_loss: 0.0862
Epoch 45/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1064 - val_loss: 0.0863
Epoch 46/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1059 - val_loss: 0.0841
Epoch 47/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1053 - val_loss: 0.0833
Epoch 48/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1049 - val_loss: 0.0837
Epoch 49/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1044 - val_loss: 0.0832
Epoch 50/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1042 - val_loss: 0.0835
Epoch 51/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1035 - val_loss: 0.0863
Epoch 52/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1031 - val_loss: 0.0835
Epoch 53/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1029 - val_loss: 0.0835
Epoch 54/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1022 - val_loss: 0.0835
Epoch 55/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1020 - val_loss: 0.0831
Epoch 56/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1015 - val_loss: 0.0830
Epoch 57/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1012 - val_loss: 0.0839
Epoch 58/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1008 - val_loss: 0.0834
Epoch 59/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1003 - val_loss: 0.0824
Epoch 60/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1002 - val_loss: 0.0828
Epoch 61/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0995 - val_loss: 0.0827
Epoch 62/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0992 - val_loss: 0.0833
Epoch 63/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0989 - val_loss: 0.0838
Epoch 64/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0986 - val_loss: 0.0823
Epoch 65/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0982 - val_loss: 0.0817
Epoch 66/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0978 - val_loss: 0.0830
Epoch 67/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0976 - val_loss: 0.0825
Epoch 68/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0974 - val_loss: 0.0820
Epoch 69/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0968 - val_loss: 0.0822
Epoch 70/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0966 - val_loss: 0.0827
Epoch 71/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0964 - val_loss: 0.0813
Epoch 72/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0959 - val_loss: 0.0808
Epoch 73/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0956 - val_loss: 0.0835
Epoch 74/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0952 - val_loss: 0.0809
Epoch 75/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0951 - val_loss: 0.0822
Epoch 76/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0949 - val_loss: 0.0822
Epoch 77/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0946 - val_loss: 0.0815
Epoch 78/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0943 - val_loss: 0.0834
Epoch 79/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0941 - val_loss: 0.0831
Epoch 80/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0938 - val_loss: 0.0827
Epoch 81/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0935 - val_loss: 0.0808
Epoch 82/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0931 - val_loss: 0.0820
Val_yp Shape is 
(75714, 24)
Results === Test == Validation ===== 81
=============================
step 1    -  0.991    0.990
step 2    -  0.984    0.982
step 3    -  0.975    0.973
step 4    -  0.965    0.963
step 5    -  0.956    0.954
step 6    -  0.946    0.944
step 7    -  0.936    0.935
step 8    -  0.928    0.927
step 9    -  0.919    0.919
step 10   -  0.912    0.912
step 11   -  0.905    0.906
step 12   -  0.899    0.900
step 13   -  0.893    0.895
step 14   -  0.887    0.889
step 15   -  0.882    0.884
step 16   -  0.876    0.879
step 17   -  0.870    0.873
step 18   -  0.864    0.868
step 19   -  0.858    0.862
step 20   -  0.852    0.858
step 21   -  0.846    0.852
step 22   -  0.841    0.847
step 23   -  0.836    0.844
step 24   -  0.830    0.838
=============================
Summary   -  21.650    21.695
V_y Shape is 
(75714, 1)
=========== SHAPE INPUT-OUTPUT MATRIXES =====================
shape x_training : (227192, 574)  y_training: :  (227192, 24)
shape x_test     :  (75713, 574)  y_test      :  (75713, 24)
shape x_val      :  (75713, 574)  y_val       :  (75713, 24)
=============================================================
(574,)
Model: "MLP"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_56 (InputLayer)       [(None, 574)]             0         
                                                                 
 dense_220 (Dense)           (None, 1024)              588800    
                                                                 
 elu_55 (ELU)                (None, 1024)              0         
                                                                 
 dropout_165 (Dropout)       (None, 1024)              0         
                                                                 
 dense_221 (Dense)           (None, 512)               524800    
                                                                 
 dropout_166 (Dropout)       (None, 512)               0         
                                                                 
 dense_222 (Dense)           (None, 512)               262656    
                                                                 
 dropout_167 (Dropout)       (None, 512)               0         
                                                                 
 dense_223 (Dense)           (None, 24)                12312     
                                                                 
=================================================================
Total params: 1,388,568
Trainable params: 1,388,568
Non-trainable params: 0
_________________________________________________________________
Epoch 1/200
222/222 [==============================] - 3s 8ms/step - loss: 0.4794 - val_loss: 0.1127
Epoch 2/200
222/222 [==============================] - 1s 6ms/step - loss: 0.2469 - val_loss: 0.1031
Epoch 3/200
222/222 [==============================] - 1s 6ms/step - loss: 0.2053 - val_loss: 0.0994
Epoch 4/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1856 - val_loss: 0.0977
Epoch 5/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1727 - val_loss: 0.0943
Epoch 6/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1635 - val_loss: 0.0954
Epoch 7/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1567 - val_loss: 0.0959
Epoch 8/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1506 - val_loss: 0.0955
Epoch 9/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1461 - val_loss: 0.0918
Epoch 10/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1418 - val_loss: 0.0915
Epoch 11/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1385 - val_loss: 0.0917
Epoch 12/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1355 - val_loss: 0.0896
Epoch 13/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1329 - val_loss: 0.0909
Epoch 14/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1305 - val_loss: 0.0887
Epoch 15/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1284 - val_loss: 0.0896
Epoch 16/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1268 - val_loss: 0.0894
Epoch 17/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1250 - val_loss: 0.0880
Epoch 18/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1237 - val_loss: 0.0895
Epoch 19/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1222 - val_loss: 0.0902
Epoch 20/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1212 - val_loss: 0.0906
Epoch 21/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1199 - val_loss: 0.0870
Epoch 22/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1189 - val_loss: 0.0888
Epoch 23/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1181 - val_loss: 0.0881
Epoch 24/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1170 - val_loss: 0.0869
Epoch 25/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1165 - val_loss: 0.0881
Epoch 26/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1156 - val_loss: 0.0868
Epoch 27/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1151 - val_loss: 0.0870
Epoch 28/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1144 - val_loss: 0.0869
Epoch 29/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1138 - val_loss: 0.0868
Epoch 30/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1132 - val_loss: 0.0877
Epoch 31/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1126 - val_loss: 0.0864
Epoch 32/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1122 - val_loss: 0.0865
Epoch 33/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1118 - val_loss: 0.0861
Epoch 34/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1112 - val_loss: 0.0867
Epoch 35/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1109 - val_loss: 0.0861
Epoch 36/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1103 - val_loss: 0.0862
Epoch 37/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1099 - val_loss: 0.0868
Epoch 38/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1093 - val_loss: 0.0851
Epoch 39/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1087 - val_loss: 0.0845
Epoch 40/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1083 - val_loss: 0.0845
Epoch 41/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1078 - val_loss: 0.0860
Epoch 42/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1073 - val_loss: 0.0839
Epoch 43/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1072 - val_loss: 0.0836
Epoch 44/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1067 - val_loss: 0.0891
Epoch 45/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1061 - val_loss: 0.0867
Epoch 46/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1052 - val_loss: 0.0852
Epoch 47/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1050 - val_loss: 0.0837
Epoch 48/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1046 - val_loss: 0.0859
Epoch 49/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1040 - val_loss: 0.0839
Epoch 50/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1036 - val_loss: 0.0847
Epoch 51/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1031 - val_loss: 0.0837
Epoch 52/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1027 - val_loss: 0.0829
Epoch 53/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1023 - val_loss: 0.0842
Epoch 54/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1018 - val_loss: 0.0836
Epoch 55/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1013 - val_loss: 0.0816
Epoch 56/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1010 - val_loss: 0.0829
Epoch 57/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1007 - val_loss: 0.0825
Epoch 58/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1002 - val_loss: 0.0824
Epoch 59/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0997 - val_loss: 0.0826
Epoch 60/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0999 - val_loss: 0.0832
Epoch 61/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0990 - val_loss: 0.0846
Epoch 62/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0990 - val_loss: 0.0817
Epoch 63/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0985 - val_loss: 0.0832
Epoch 64/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0980 - val_loss: 0.0816
Epoch 65/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0978 - val_loss: 0.0813
Epoch 66/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0973 - val_loss: 0.0820
Epoch 67/200
222/222 [==============================] - 2s 7ms/step - loss: 0.0971 - val_loss: 0.0817
Epoch 68/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0967 - val_loss: 0.0811
Epoch 69/200
222/222 [==============================] - 2s 7ms/step - loss: 0.0965 - val_loss: 0.0811
Epoch 70/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0963 - val_loss: 0.0820
Epoch 71/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0958 - val_loss: 0.0818
Epoch 72/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0954 - val_loss: 0.0823
Epoch 73/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0952 - val_loss: 0.0836
Epoch 74/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0951 - val_loss: 0.0834
Epoch 75/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0950 - val_loss: 0.0810
Epoch 76/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0944 - val_loss: 0.0831
Epoch 77/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0943 - val_loss: 0.0832
Epoch 78/200
222/222 [==============================] - 2s 7ms/step - loss: 0.0938 - val_loss: 0.0829
Epoch 79/200
222/222 [==============================] - 2s 7ms/step - loss: 0.0935 - val_loss: 0.0813
Epoch 80/200
222/222 [==============================] - 2s 7ms/step - loss: 0.0930 - val_loss: 0.0809
Epoch 81/200
222/222 [==============================] - 2s 7ms/step - loss: 0.0928 - val_loss: 0.0817
Epoch 82/200
222/222 [==============================] - 2s 7ms/step - loss: 0.0928 - val_loss: 0.0818
Epoch 83/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0926 - val_loss: 0.0815
Epoch 84/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0922 - val_loss: 0.0821
Epoch 85/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0919 - val_loss: 0.0803
Epoch 86/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0917 - val_loss: 0.0805
Epoch 87/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0916 - val_loss: 0.0815
Epoch 88/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0914 - val_loss: 0.0816
Epoch 89/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0910 - val_loss: 0.0819
Epoch 90/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0909 - val_loss: 0.0811
Epoch 91/200
222/222 [==============================] - 2s 7ms/step - loss: 0.0904 - val_loss: 0.0809
Epoch 92/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0905 - val_loss: 0.0818
Epoch 93/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0900 - val_loss: 0.0817
Epoch 94/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0898 - val_loss: 0.0814
Epoch 95/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0897 - val_loss: 0.0809
Val_yp Shape is 
(75713, 24)
Results === Test == Validation ===== 82
=============================
step 1    -  0.990    0.988
step 2    -  0.983    0.981
step 3    -  0.974    0.972
step 4    -  0.965    0.962
step 5    -  0.954    0.952
step 6    -  0.944    0.943
step 7    -  0.936    0.935
step 8    -  0.928    0.927
step 9    -  0.920    0.920
step 10   -  0.914    0.915
step 11   -  0.907    0.908
step 12   -  0.900    0.901
step 13   -  0.894    0.896
step 14   -  0.888    0.891
step 15   -  0.883    0.886
step 16   -  0.877    0.880
step 17   -  0.870    0.874
step 18   -  0.865    0.869
step 19   -  0.859    0.864
step 20   -  0.855    0.860
step 21   -  0.849    0.855
step 22   -  0.842    0.849
step 23   -  0.837    0.845
step 24   -  0.832    0.840
=============================
Summary   -  21.663    21.715
V_y Shape is 
(75713, 1)
=========== SHAPE INPUT-OUTPUT MATRIXES =====================
shape x_training : (227191, 581)  y_training: :  (227191, 24)
shape x_test     :  (75712, 581)  y_test      :  (75712, 24)
shape x_val      :  (75713, 581)  y_val       :  (75713, 24)
=============================================================
(581,)
Model: "MLP"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_57 (InputLayer)       [(None, 581)]             0         
                                                                 
 dense_224 (Dense)           (None, 1024)              595968    
                                                                 
 elu_56 (ELU)                (None, 1024)              0         
                                                                 
 dropout_168 (Dropout)       (None, 1024)              0         
                                                                 
 dense_225 (Dense)           (None, 512)               524800    
                                                                 
 dropout_169 (Dropout)       (None, 512)               0         
                                                                 
 dense_226 (Dense)           (None, 512)               262656    
                                                                 
 dropout_170 (Dropout)       (None, 512)               0         
                                                                 
 dense_227 (Dense)           (None, 24)                12312     
                                                                 
=================================================================
Total params: 1,395,736
Trainable params: 1,395,736
Non-trainable params: 0
_________________________________________________________________
Epoch 1/200
222/222 [==============================] - 2s 7ms/step - loss: 0.4831 - val_loss: 0.1130
Epoch 2/200
222/222 [==============================] - 1s 6ms/step - loss: 0.2487 - val_loss: 0.1053
Epoch 3/200
222/222 [==============================] - 1s 6ms/step - loss: 0.2067 - val_loss: 0.1012
Epoch 4/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1863 - val_loss: 0.0988
Epoch 5/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1732 - val_loss: 0.0968
Epoch 6/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1637 - val_loss: 0.0951
Epoch 7/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1567 - val_loss: 0.0953
Epoch 8/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1507 - val_loss: 0.0914
Epoch 9/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1458 - val_loss: 0.0925
Epoch 10/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1421 - val_loss: 0.0931
Epoch 11/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1384 - val_loss: 0.0904
Epoch 12/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1355 - val_loss: 0.0931
Epoch 13/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1329 - val_loss: 0.0905
Epoch 14/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1305 - val_loss: 0.0922
Epoch 15/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1286 - val_loss: 0.0898
Epoch 16/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1267 - val_loss: 0.0907
Epoch 17/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1251 - val_loss: 0.0883
Epoch 18/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1235 - val_loss: 0.0893
Epoch 19/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1223 - val_loss: 0.0908
Epoch 20/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1210 - val_loss: 0.0887
Epoch 21/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1200 - val_loss: 0.0909
Epoch 22/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1191 - val_loss: 0.0871
Epoch 23/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1179 - val_loss: 0.0877
Epoch 24/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1170 - val_loss: 0.0887
Epoch 25/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1163 - val_loss: 0.0889
Epoch 26/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1159 - val_loss: 0.0875
Epoch 27/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1150 - val_loss: 0.0868
Epoch 28/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1145 - val_loss: 0.0881
Epoch 29/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1138 - val_loss: 0.0862
Epoch 30/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1134 - val_loss: 0.0876
Epoch 31/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1128 - val_loss: 0.0865
Epoch 32/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1123 - val_loss: 0.0863
Epoch 33/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1119 - val_loss: 0.0859
Epoch 34/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1113 - val_loss: 0.0874
Epoch 35/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1106 - val_loss: 0.0869
Epoch 36/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1103 - val_loss: 0.0862
Epoch 37/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1099 - val_loss: 0.0876
Epoch 38/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1092 - val_loss: 0.0859
Epoch 39/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1091 - val_loss: 0.0856
Epoch 40/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1083 - val_loss: 0.0847
Epoch 41/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1080 - val_loss: 0.0855
Epoch 42/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1075 - val_loss: 0.0853
Epoch 43/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1070 - val_loss: 0.0839
Epoch 44/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1065 - val_loss: 0.0844
Epoch 45/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1060 - val_loss: 0.0859
Epoch 46/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1059 - val_loss: 0.0839
Epoch 47/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1054 - val_loss: 0.0855
Epoch 48/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1047 - val_loss: 0.0859
Epoch 49/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1044 - val_loss: 0.0843
Epoch 50/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1037 - val_loss: 0.0841
Epoch 51/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1033 - val_loss: 0.0834
Epoch 52/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1031 - val_loss: 0.0848
Epoch 53/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1026 - val_loss: 0.0835
Epoch 54/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1021 - val_loss: 0.0861
Epoch 55/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1018 - val_loss: 0.0835
Epoch 56/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1013 - val_loss: 0.0831
Epoch 57/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1010 - val_loss: 0.0840
Epoch 58/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1005 - val_loss: 0.0836
Epoch 59/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1001 - val_loss: 0.0824
Epoch 60/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0996 - val_loss: 0.0828
Epoch 61/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0995 - val_loss: 0.0819
Epoch 62/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0989 - val_loss: 0.0829
Epoch 63/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0986 - val_loss: 0.0818
Epoch 64/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0982 - val_loss: 0.0825
Epoch 65/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0979 - val_loss: 0.0834
Epoch 66/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0977 - val_loss: 0.0831
Epoch 67/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0975 - val_loss: 0.0828
Epoch 68/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0971 - val_loss: 0.0853
Epoch 69/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0967 - val_loss: 0.0813
Epoch 70/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0964 - val_loss: 0.0830
Epoch 71/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0958 - val_loss: 0.0814
Epoch 72/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0958 - val_loss: 0.0823
Epoch 73/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0954 - val_loss: 0.0828
Epoch 74/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0950 - val_loss: 0.0833
Epoch 75/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0950 - val_loss: 0.0819
Epoch 76/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0945 - val_loss: 0.0828
Epoch 77/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0944 - val_loss: 0.0821
Epoch 78/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0939 - val_loss: 0.0822
Epoch 79/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0938 - val_loss: 0.0844
Val_yp Shape is 
(75713, 24)
Results === Test == Validation ===== 83
=============================
step 1    -  0.990    0.990
step 2    -  0.984    0.982
step 3    -  0.975    0.974
step 4    -  0.966    0.964
step 5    -  0.957    0.955
step 6    -  0.947    0.946
step 7    -  0.938    0.937
step 8    -  0.930    0.929
step 9    -  0.921    0.921
step 10   -  0.914    0.914
step 11   -  0.906    0.907
step 12   -  0.899    0.901
step 13   -  0.892    0.894
step 14   -  0.885    0.887
step 15   -  0.878    0.880
step 16   -  0.872    0.875
step 17   -  0.867    0.870
step 18   -  0.862    0.865
step 19   -  0.856    0.860
step 20   -  0.852    0.856
step 21   -  0.847    0.852
step 22   -  0.843    0.848
step 23   -  0.837    0.843
step 24   -  0.831    0.838
=============================
Summary   -  21.651    21.688
V_y Shape is 
(75713, 1)
=========== SHAPE INPUT-OUTPUT MATRIXES =====================
shape x_training : (227190, 588)  y_training: :  (227190, 24)
shape x_test     :  (75712, 588)  y_test      :  (75712, 24)
shape x_val      :  (75712, 588)  y_val       :  (75712, 24)
=============================================================
(588,)
Model: "MLP"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_58 (InputLayer)       [(None, 588)]             0         
                                                                 
 dense_228 (Dense)           (None, 1024)              603136    
                                                                 
 elu_57 (ELU)                (None, 1024)              0         
                                                                 
 dropout_171 (Dropout)       (None, 1024)              0         
                                                                 
 dense_229 (Dense)           (None, 512)               524800    
                                                                 
 dropout_172 (Dropout)       (None, 512)               0         
                                                                 
 dense_230 (Dense)           (None, 512)               262656    
                                                                 
 dropout_173 (Dropout)       (None, 512)               0         
                                                                 
 dense_231 (Dense)           (None, 24)                12312     
                                                                 
=================================================================
Total params: 1,402,904
Trainable params: 1,402,904
Non-trainable params: 0
_________________________________________________________________
Epoch 1/200
222/222 [==============================] - 2s 7ms/step - loss: 0.4837 - val_loss: 0.1132
Epoch 2/200
222/222 [==============================] - 1s 6ms/step - loss: 0.2484 - val_loss: 0.1047
Epoch 3/200
222/222 [==============================] - 1s 6ms/step - loss: 0.2066 - val_loss: 0.0995
Epoch 4/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1861 - val_loss: 0.0985
Epoch 5/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1729 - val_loss: 0.0970
Epoch 6/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1636 - val_loss: 0.0972
Epoch 7/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1564 - val_loss: 0.0950
Epoch 8/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1507 - val_loss: 0.0935
Epoch 9/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1462 - val_loss: 0.0955
Epoch 10/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1421 - val_loss: 0.0910
Epoch 11/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1385 - val_loss: 0.0915
Epoch 12/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1353 - val_loss: 0.0911
Epoch 13/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1329 - val_loss: 0.0909
Epoch 14/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1306 - val_loss: 0.0922
Epoch 15/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1285 - val_loss: 0.0902
Epoch 16/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1267 - val_loss: 0.0897
Epoch 17/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1249 - val_loss: 0.0891
Epoch 18/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1236 - val_loss: 0.0901
Epoch 19/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1220 - val_loss: 0.0898
Epoch 20/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1211 - val_loss: 0.0881
Epoch 21/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1198 - val_loss: 0.0888
Epoch 22/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1189 - val_loss: 0.0917
Epoch 23/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1180 - val_loss: 0.0865
Epoch 24/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1168 - val_loss: 0.0887
Epoch 25/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1164 - val_loss: 0.0895
Epoch 26/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1156 - val_loss: 0.0875
Epoch 27/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1149 - val_loss: 0.0878
Epoch 28/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1142 - val_loss: 0.0868
Epoch 29/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1139 - val_loss: 0.0871
Epoch 30/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1133 - val_loss: 0.0874
Epoch 31/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1128 - val_loss: 0.0881
Epoch 32/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1120 - val_loss: 0.0867
Epoch 33/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1114 - val_loss: 0.0873
Val_yp Shape is 
(75712, 24)
Results === Test == Validation ===== 84
=============================
step 1    -  0.989    0.988
step 2    -  0.982    0.980
step 3    -  0.973    0.971
step 4    -  0.963    0.961
step 5    -  0.953    0.952
step 6    -  0.944    0.942
step 7    -  0.935    0.934
step 8    -  0.926    0.926
step 9    -  0.918    0.918
step 10   -  0.910    0.911
step 11   -  0.903    0.904
step 12   -  0.896    0.898
step 13   -  0.890    0.892
step 14   -  0.884    0.886
step 15   -  0.877    0.880
step 16   -  0.871    0.874
step 17   -  0.865    0.868
step 18   -  0.859    0.863
step 19   -  0.853    0.858
step 20   -  0.848    0.853
step 21   -  0.843    0.849
step 22   -  0.839    0.845
step 23   -  0.835    0.841
step 24   -  0.828    0.836
=============================
Summary   -  21.583    21.629
V_y Shape is 
(75712, 1)
=========== SHAPE INPUT-OUTPUT MATRIXES =====================
shape x_training : (227189, 595)  y_training: :  (227189, 24)
shape x_test     :  (75711, 595)  y_test      :  (75711, 24)
shape x_val      :  (75712, 595)  y_val       :  (75712, 24)
=============================================================
(595,)
Model: "MLP"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_59 (InputLayer)       [(None, 595)]             0         
                                                                 
 dense_232 (Dense)           (None, 1024)              610304    
                                                                 
 elu_58 (ELU)                (None, 1024)              0         
                                                                 
 dropout_174 (Dropout)       (None, 1024)              0         
                                                                 
 dense_233 (Dense)           (None, 512)               524800    
                                                                 
 dropout_175 (Dropout)       (None, 512)               0         
                                                                 
 dense_234 (Dense)           (None, 512)               262656    
                                                                 
 dropout_176 (Dropout)       (None, 512)               0         
                                                                 
 dense_235 (Dense)           (None, 24)                12312     
                                                                 
=================================================================
Total params: 1,410,072
Trainable params: 1,410,072
Non-trainable params: 0
_________________________________________________________________
Epoch 1/200
222/222 [==============================] - 2s 8ms/step - loss: 0.4743 - val_loss: 0.1148
Epoch 2/200
222/222 [==============================] - 1s 6ms/step - loss: 0.2448 - val_loss: 0.1048
Epoch 3/200
222/222 [==============================] - 1s 6ms/step - loss: 0.2045 - val_loss: 0.1020
Epoch 4/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1847 - val_loss: 0.0997
Epoch 5/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1720 - val_loss: 0.0985
Epoch 6/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1626 - val_loss: 0.0972
Epoch 7/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1555 - val_loss: 0.0958
Epoch 8/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1502 - val_loss: 0.0922
Epoch 9/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1454 - val_loss: 0.0939
Epoch 10/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1415 - val_loss: 0.0932
Epoch 11/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1380 - val_loss: 0.0915
Epoch 12/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1347 - val_loss: 0.0926
Epoch 13/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1323 - val_loss: 0.0929
Epoch 14/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1301 - val_loss: 0.0916
Epoch 15/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1279 - val_loss: 0.0893
Epoch 16/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1262 - val_loss: 0.0885
Epoch 17/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1246 - val_loss: 0.0923
Epoch 18/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1231 - val_loss: 0.0906
Epoch 19/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1218 - val_loss: 0.0884
Epoch 20/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1206 - val_loss: 0.0907
Epoch 21/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1196 - val_loss: 0.0897
Epoch 22/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1185 - val_loss: 0.0891
Epoch 23/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1176 - val_loss: 0.0883
Epoch 24/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1167 - val_loss: 0.0889
Epoch 25/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1159 - val_loss: 0.0864
Epoch 26/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1152 - val_loss: 0.0903
Epoch 27/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1147 - val_loss: 0.0878
Epoch 28/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1140 - val_loss: 0.0882
Epoch 29/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1133 - val_loss: 0.0862
Epoch 30/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1129 - val_loss: 0.0891
Epoch 31/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1125 - val_loss: 0.0864
Epoch 32/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1116 - val_loss: 0.0863
Epoch 33/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1113 - val_loss: 0.0886
Epoch 34/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1109 - val_loss: 0.0867
Epoch 35/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1102 - val_loss: 0.0859
Epoch 36/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1101 - val_loss: 0.0857
Epoch 37/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1092 - val_loss: 0.0873
Epoch 38/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1086 - val_loss: 0.0868
Epoch 39/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1083 - val_loss: 0.0861
Epoch 40/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1078 - val_loss: 0.0886
Epoch 41/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1073 - val_loss: 0.0868
Epoch 42/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1067 - val_loss: 0.0873
Epoch 43/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1061 - val_loss: 0.0850
Epoch 44/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1058 - val_loss: 0.0849
Epoch 45/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1052 - val_loss: 0.0834
Epoch 46/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1050 - val_loss: 0.0844
Epoch 47/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1041 - val_loss: 0.0847
Epoch 48/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1039 - val_loss: 0.0840
Epoch 49/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1036 - val_loss: 0.0835
Epoch 50/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1030 - val_loss: 0.0856
Epoch 51/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1025 - val_loss: 0.0840
Epoch 52/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1021 - val_loss: 0.0844
Epoch 53/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1016 - val_loss: 0.0838
Epoch 54/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1011 - val_loss: 0.0830
Epoch 55/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1008 - val_loss: 0.0831
Epoch 56/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1006 - val_loss: 0.0850
Epoch 57/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1002 - val_loss: 0.0839
Epoch 58/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0996 - val_loss: 0.0839
Epoch 59/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0990 - val_loss: 0.0832
Epoch 60/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0990 - val_loss: 0.0839
Epoch 61/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0983 - val_loss: 0.0832
Epoch 62/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0980 - val_loss: 0.0816
Epoch 63/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0976 - val_loss: 0.0830
Epoch 64/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0973 - val_loss: 0.0830
Epoch 65/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0969 - val_loss: 0.0828
Epoch 66/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0964 - val_loss: 0.0846
Epoch 67/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0962 - val_loss: 0.0841
Epoch 68/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0960 - val_loss: 0.0835
Epoch 69/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0957 - val_loss: 0.0828
Epoch 70/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0955 - val_loss: 0.0821
Epoch 71/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0949 - val_loss: 0.0827
Epoch 72/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0945 - val_loss: 0.0816
Epoch 73/200
222/222 [==============================] - 2s 7ms/step - loss: 0.0946 - val_loss: 0.0835
Epoch 74/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0942 - val_loss: 0.0824
Epoch 75/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0939 - val_loss: 0.0827
Epoch 76/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0936 - val_loss: 0.0817
Epoch 77/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0933 - val_loss: 0.0830
Epoch 78/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0931 - val_loss: 0.0819
Epoch 79/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0928 - val_loss: 0.0839
Epoch 80/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0927 - val_loss: 0.0815
Epoch 81/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0922 - val_loss: 0.0826
Epoch 82/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0921 - val_loss: 0.0820
Epoch 83/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0916 - val_loss: 0.0818
Epoch 84/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0914 - val_loss: 0.0817
Epoch 85/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0909 - val_loss: 0.0826
Epoch 86/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0910 - val_loss: 0.0824
Epoch 87/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0905 - val_loss: 0.0816
Epoch 88/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0903 - val_loss: 0.0826
Epoch 89/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0902 - val_loss: 0.0818
Epoch 90/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0900 - val_loss: 0.0826
Val_yp Shape is 
(75712, 24)
Results === Test == Validation ===== 85
=============================
step 1    -  0.990    0.989
step 2    -  0.984    0.982
step 3    -  0.975    0.973
step 4    -  0.966    0.964
step 5    -  0.957    0.954
step 6    -  0.948    0.945
step 7    -  0.938    0.937
step 8    -  0.929    0.928
step 9    -  0.920    0.920
step 10   -  0.912    0.912
step 11   -  0.905    0.905
step 12   -  0.899    0.900
step 13   -  0.893    0.894
step 14   -  0.887    0.889
step 15   -  0.881    0.883
step 16   -  0.876    0.878
step 17   -  0.871    0.874
step 18   -  0.866    0.869
step 19   -  0.862    0.865
step 20   -  0.857    0.861
step 21   -  0.853    0.858
step 22   -  0.847    0.853
step 23   -  0.842    0.848
step 24   -  0.836    0.843
=============================
Summary   -  21.696    21.723
V_y Shape is 
(75712, 1)
=========== SHAPE INPUT-OUTPUT MATRIXES =====================
shape x_training : (227188, 602)  y_training: :  (227188, 24)
shape x_test     :  (75711, 602)  y_test      :  (75711, 24)
shape x_val      :  (75711, 602)  y_val       :  (75711, 24)
=============================================================
(602,)
Model: "MLP"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_60 (InputLayer)       [(None, 602)]             0         
                                                                 
 dense_236 (Dense)           (None, 1024)              617472    
                                                                 
 elu_59 (ELU)                (None, 1024)              0         
                                                                 
 dropout_177 (Dropout)       (None, 1024)              0         
                                                                 
 dense_237 (Dense)           (None, 512)               524800    
                                                                 
 dropout_178 (Dropout)       (None, 512)               0         
                                                                 
 dense_238 (Dense)           (None, 512)               262656    
                                                                 
 dropout_179 (Dropout)       (None, 512)               0         
                                                                 
 dense_239 (Dense)           (None, 24)                12312     
                                                                 
=================================================================
Total params: 1,417,240
Trainable params: 1,417,240
Non-trainable params: 0
_________________________________________________________________
Epoch 1/200
222/222 [==============================] - 2s 8ms/step - loss: 0.4857 - val_loss: 0.1161
Epoch 2/200
222/222 [==============================] - 1s 6ms/step - loss: 0.2487 - val_loss: 0.1056
Epoch 3/200
222/222 [==============================] - 1s 6ms/step - loss: 0.2070 - val_loss: 0.0991
Epoch 4/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1864 - val_loss: 0.0967
Epoch 5/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1734 - val_loss: 0.0983
Epoch 6/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1637 - val_loss: 0.0952
Epoch 7/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1567 - val_loss: 0.0938
Epoch 8/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1507 - val_loss: 0.0937
Epoch 9/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1461 - val_loss: 0.0917
Epoch 10/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1421 - val_loss: 0.0935
Epoch 11/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1382 - val_loss: 0.0923
Epoch 12/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1353 - val_loss: 0.0903
Epoch 13/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1326 - val_loss: 0.0927
Epoch 14/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1301 - val_loss: 0.0931
Epoch 15/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1282 - val_loss: 0.0906
Epoch 16/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1262 - val_loss: 0.0896
Epoch 17/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1243 - val_loss: 0.0919
Epoch 18/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1231 - val_loss: 0.0901
Epoch 19/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1217 - val_loss: 0.0895
Epoch 20/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1205 - val_loss: 0.0901
Epoch 21/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1196 - val_loss: 0.0890
Epoch 22/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1185 - val_loss: 0.0881
Epoch 23/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1176 - val_loss: 0.0881
Epoch 24/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1168 - val_loss: 0.0867
Epoch 25/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1160 - val_loss: 0.0877
Epoch 26/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1152 - val_loss: 0.0880
Epoch 27/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1146 - val_loss: 0.0872
Epoch 28/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1139 - val_loss: 0.0887
Epoch 29/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1133 - val_loss: 0.0874
Epoch 30/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1127 - val_loss: 0.0863
Epoch 31/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1120 - val_loss: 0.0887
Epoch 32/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1115 - val_loss: 0.0864
Epoch 33/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1112 - val_loss: 0.0860
Epoch 34/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1104 - val_loss: 0.0854
Epoch 35/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1100 - val_loss: 0.0865
Epoch 36/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1095 - val_loss: 0.0855
Epoch 37/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1089 - val_loss: 0.0852
Epoch 38/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1085 - val_loss: 0.0874
Epoch 39/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1079 - val_loss: 0.0868
Epoch 40/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1074 - val_loss: 0.0857
Epoch 41/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1071 - val_loss: 0.0855
Epoch 42/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1066 - val_loss: 0.0841
Epoch 43/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1058 - val_loss: 0.0872
Epoch 44/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1055 - val_loss: 0.0840
Epoch 45/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1051 - val_loss: 0.0854
Epoch 46/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1042 - val_loss: 0.0861
Epoch 47/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1042 - val_loss: 0.0841
Epoch 48/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1035 - val_loss: 0.0838
Epoch 49/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1031 - val_loss: 0.0841
Epoch 50/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1027 - val_loss: 0.0836
Epoch 51/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1022 - val_loss: 0.0845
Epoch 52/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1017 - val_loss: 0.0825
Epoch 53/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1012 - val_loss: 0.0835
Epoch 54/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1007 - val_loss: 0.0841
Epoch 55/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1004 - val_loss: 0.0841
Epoch 56/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1000 - val_loss: 0.0836
Epoch 57/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0998 - val_loss: 0.0835
Epoch 58/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0993 - val_loss: 0.0842
Epoch 59/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0990 - val_loss: 0.0836
Epoch 60/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0985 - val_loss: 0.0827
Epoch 61/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0981 - val_loss: 0.0847
Epoch 62/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0980 - val_loss: 0.0817
Epoch 63/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0973 - val_loss: 0.0850
Epoch 64/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0970 - val_loss: 0.0820
Epoch 65/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0968 - val_loss: 0.0846
Epoch 66/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0964 - val_loss: 0.0828
Epoch 67/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0960 - val_loss: 0.0821
Epoch 68/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0958 - val_loss: 0.0831
Epoch 69/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0953 - val_loss: 0.0822
Epoch 70/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0951 - val_loss: 0.0843
Epoch 71/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0947 - val_loss: 0.0820
Epoch 72/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0945 - val_loss: 0.0812
Epoch 73/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0942 - val_loss: 0.0822
Epoch 74/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0939 - val_loss: 0.0835
Epoch 75/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0936 - val_loss: 0.0834
Epoch 76/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0933 - val_loss: 0.0819
Epoch 77/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0929 - val_loss: 0.0838
Epoch 78/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0929 - val_loss: 0.0829
Epoch 79/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0926 - val_loss: 0.0822
Epoch 80/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0922 - val_loss: 0.0842
Epoch 81/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0920 - val_loss: 0.0820
Epoch 82/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0917 - val_loss: 0.0827
Val_yp Shape is 
(75711, 24)
Results === Test == Validation ===== 86
=============================
step 1    -  0.990    0.989
step 2    -  0.983    0.982
step 3    -  0.975    0.973
step 4    -  0.966    0.964
step 5    -  0.957    0.955
step 6    -  0.947    0.946
step 7    -  0.938    0.937
step 8    -  0.929    0.929
step 9    -  0.920    0.920
step 10   -  0.912    0.913
step 11   -  0.904    0.906
step 12   -  0.897    0.900
step 13   -  0.891    0.895
step 14   -  0.884    0.888
step 15   -  0.879    0.883
step 16   -  0.872    0.877
step 17   -  0.866    0.871
step 18   -  0.861    0.866
step 19   -  0.855    0.861
step 20   -  0.851    0.858
step 21   -  0.846    0.854
step 22   -  0.843    0.852
step 23   -  0.838    0.847
step 24   -  0.833    0.842
=============================
Summary   -  21.636    21.706
V_y Shape is 
(75711, 1)
=========== SHAPE INPUT-OUTPUT MATRIXES =====================
shape x_training : (227187, 609)  y_training: :  (227187, 24)
shape x_test     :  (75710, 609)  y_test      :  (75710, 24)
shape x_val      :  (75711, 609)  y_val       :  (75711, 24)
=============================================================
(609,)
Model: "MLP"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_61 (InputLayer)       [(None, 609)]             0         
                                                                 
 dense_240 (Dense)           (None, 1024)              624640    
                                                                 
 elu_60 (ELU)                (None, 1024)              0         
                                                                 
 dropout_180 (Dropout)       (None, 1024)              0         
                                                                 
 dense_241 (Dense)           (None, 512)               524800    
                                                                 
 dropout_181 (Dropout)       (None, 512)               0         
                                                                 
 dense_242 (Dense)           (None, 512)               262656    
                                                                 
 dropout_182 (Dropout)       (None, 512)               0         
                                                                 
 dense_243 (Dense)           (None, 24)                12312     
                                                                 
=================================================================
Total params: 1,424,408
Trainable params: 1,424,408
Non-trainable params: 0
_________________________________________________________________
Epoch 1/200
222/222 [==============================] - 2s 8ms/step - loss: 0.4866 - val_loss: 0.1144
Epoch 2/200
222/222 [==============================] - 1s 6ms/step - loss: 0.2496 - val_loss: 0.1044
Epoch 3/200
222/222 [==============================] - 1s 6ms/step - loss: 0.2076 - val_loss: 0.1024
Epoch 4/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1870 - val_loss: 0.0973
Epoch 5/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1736 - val_loss: 0.0996
Epoch 6/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1641 - val_loss: 0.0944
Epoch 7/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1566 - val_loss: 0.0964
Epoch 8/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1507 - val_loss: 0.0923
Epoch 9/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1460 - val_loss: 0.0934
Epoch 10/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1417 - val_loss: 0.0924
Epoch 11/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1381 - val_loss: 0.0927
Epoch 12/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1354 - val_loss: 0.0918
Epoch 13/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1327 - val_loss: 0.0916
Epoch 14/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1301 - val_loss: 0.0903
Epoch 15/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1280 - val_loss: 0.0909
Epoch 16/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1263 - val_loss: 0.0902
Epoch 17/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1246 - val_loss: 0.0920
Epoch 18/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1232 - val_loss: 0.0892
Epoch 19/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1216 - val_loss: 0.0878
Epoch 20/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1205 - val_loss: 0.0898
Epoch 21/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1192 - val_loss: 0.0886
Epoch 22/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1182 - val_loss: 0.0899
Epoch 23/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1173 - val_loss: 0.0871
Epoch 24/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1165 - val_loss: 0.0879
Epoch 25/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1156 - val_loss: 0.0877
Epoch 26/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1152 - val_loss: 0.0898
Epoch 27/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1144 - val_loss: 0.0856
Epoch 28/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1136 - val_loss: 0.0875
Epoch 29/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1130 - val_loss: 0.0910
Epoch 30/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1127 - val_loss: 0.0878
Epoch 31/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1119 - val_loss: 0.0856
Epoch 32/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1114 - val_loss: 0.0863
Epoch 33/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1110 - val_loss: 0.0854
Epoch 34/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1103 - val_loss: 0.0883
Epoch 35/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1097 - val_loss: 0.0868
Epoch 36/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1093 - val_loss: 0.0850
Epoch 37/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1089 - val_loss: 0.0856
Epoch 38/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1082 - val_loss: 0.0854
Epoch 39/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1078 - val_loss: 0.0850
Epoch 40/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1070 - val_loss: 0.0859
Epoch 41/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1066 - val_loss: 0.0862
Epoch 42/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1062 - val_loss: 0.0861
Epoch 43/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1057 - val_loss: 0.0859
Epoch 44/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1051 - val_loss: 0.0846
Epoch 45/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1048 - val_loss: 0.0838
Epoch 46/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1042 - val_loss: 0.0841
Epoch 47/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1037 - val_loss: 0.0849
Epoch 48/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1032 - val_loss: 0.0829
Epoch 49/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1027 - val_loss: 0.0837
Epoch 50/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1022 - val_loss: 0.0852
Epoch 51/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1016 - val_loss: 0.0859
Epoch 52/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1013 - val_loss: 0.0843
Epoch 53/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1010 - val_loss: 0.0844
Epoch 54/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1004 - val_loss: 0.0843
Epoch 55/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1001 - val_loss: 0.0842
Epoch 56/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0996 - val_loss: 0.0823
Epoch 57/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0992 - val_loss: 0.0830
Epoch 58/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0989 - val_loss: 0.0824
Epoch 59/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0984 - val_loss: 0.0829
Epoch 60/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0982 - val_loss: 0.0830
Epoch 61/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0976 - val_loss: 0.0835
Epoch 62/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0971 - val_loss: 0.0847
Epoch 63/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0970 - val_loss: 0.0827
Epoch 64/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0966 - val_loss: 0.0832
Epoch 65/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0964 - val_loss: 0.0825
Epoch 66/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0959 - val_loss: 0.0835
Val_yp Shape is 
(75711, 24)
Results === Test == Validation ===== 87
=============================
step 1    -  0.989    0.988
step 2    -  0.982    0.980
step 3    -  0.972    0.970
step 4    -  0.963    0.960
step 5    -  0.953    0.950
step 6    -  0.943    0.941
step 7    -  0.933    0.931
step 8    -  0.924    0.923
step 9    -  0.915    0.914
step 10   -  0.906    0.905
step 11   -  0.898    0.897
step 12   -  0.890    0.890
step 13   -  0.884    0.885
step 14   -  0.879    0.879
step 15   -  0.874    0.875
step 16   -  0.869    0.871
step 17   -  0.864    0.866
step 18   -  0.860    0.862
step 19   -  0.855    0.858
step 20   -  0.851    0.854
step 21   -  0.846    0.849
step 22   -  0.840    0.844
step 23   -  0.836    0.841
step 24   -  0.832    0.837
=============================
Summary   -  21.560    21.570
V_y Shape is 
(75711, 1)
=========== SHAPE INPUT-OUTPUT MATRIXES =====================
shape x_training : (227186, 616)  y_training: :  (227186, 24)
shape x_test     :  (75710, 616)  y_test      :  (75710, 24)
shape x_val      :  (75710, 616)  y_val       :  (75710, 24)
=============================================================
(616,)
Model: "MLP"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_62 (InputLayer)       [(None, 616)]             0         
                                                                 
 dense_244 (Dense)           (None, 1024)              631808    
                                                                 
 elu_61 (ELU)                (None, 1024)              0         
                                                                 
 dropout_183 (Dropout)       (None, 1024)              0         
                                                                 
 dense_245 (Dense)           (None, 512)               524800    
                                                                 
 dropout_184 (Dropout)       (None, 512)               0         
                                                                 
 dense_246 (Dense)           (None, 512)               262656    
                                                                 
 dropout_185 (Dropout)       (None, 512)               0         
                                                                 
 dense_247 (Dense)           (None, 24)                12312     
                                                                 
=================================================================
Total params: 1,431,576
Trainable params: 1,431,576
Non-trainable params: 0
_________________________________________________________________
Epoch 1/200
222/222 [==============================] - 2s 8ms/step - loss: 0.4859 - val_loss: 0.1131
Epoch 2/200
222/222 [==============================] - 1s 6ms/step - loss: 0.2491 - val_loss: 0.1035
Epoch 3/200
222/222 [==============================] - 1s 7ms/step - loss: 0.2072 - val_loss: 0.0990
Epoch 4/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1868 - val_loss: 0.0974
Epoch 5/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1735 - val_loss: 0.0954
Epoch 6/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1637 - val_loss: 0.0962
Epoch 7/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1564 - val_loss: 0.0961
Epoch 8/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1504 - val_loss: 0.0945
Epoch 9/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1456 - val_loss: 0.0912
Epoch 10/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1416 - val_loss: 0.0916
Epoch 11/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1382 - val_loss: 0.0928
Epoch 12/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1351 - val_loss: 0.0919
Epoch 13/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1327 - val_loss: 0.0892
Epoch 14/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1300 - val_loss: 0.0899
Epoch 15/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1283 - val_loss: 0.0900
Epoch 16/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1260 - val_loss: 0.0901
Epoch 17/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1244 - val_loss: 0.0892
Epoch 18/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1232 - val_loss: 0.0885
Epoch 19/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1215 - val_loss: 0.0865
Epoch 20/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1204 - val_loss: 0.0887
Epoch 21/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1192 - val_loss: 0.0879
Epoch 22/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1182 - val_loss: 0.0883
Epoch 23/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1173 - val_loss: 0.0873
Epoch 24/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1163 - val_loss: 0.0878
Epoch 25/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1155 - val_loss: 0.0897
Epoch 26/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1149 - val_loss: 0.0870
Epoch 27/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1142 - val_loss: 0.0869
Epoch 28/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1135 - val_loss: 0.0879
Epoch 29/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1130 - val_loss: 0.0870
Val_yp Shape is 
(75710, 24)
Results === Test == Validation ===== 88
=============================
step 1    -  0.989    0.988
step 2    -  0.982    0.980
step 3    -  0.973    0.971
step 4    -  0.963    0.961
step 5    -  0.953    0.951
step 6    -  0.942    0.941
step 7    -  0.932    0.931
step 8    -  0.923    0.922
step 9    -  0.914    0.914
step 10   -  0.906    0.907
step 11   -  0.898    0.899
step 12   -  0.891    0.893
step 13   -  0.884    0.886
step 14   -  0.879    0.882
step 15   -  0.873    0.877
step 16   -  0.868    0.872
step 17   -  0.863    0.867
step 18   -  0.858    0.863
step 19   -  0.854    0.859
step 20   -  0.850    0.856
step 21   -  0.845    0.851
step 22   -  0.840    0.847
step 23   -  0.835    0.842
step 24   -  0.829    0.837
=============================
Summary   -  21.543    21.599
V_y Shape is 
(75710, 1)
=========== SHAPE INPUT-OUTPUT MATRIXES =====================
shape x_training : (227185, 623)  y_training: :  (227185, 24)
shape x_test     :  (75709, 623)  y_test      :  (75709, 24)
shape x_val      :  (75710, 623)  y_val       :  (75710, 24)
=============================================================
(623,)
Model: "MLP"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_63 (InputLayer)       [(None, 623)]             0         
                                                                 
 dense_248 (Dense)           (None, 1024)              638976    
                                                                 
 elu_62 (ELU)                (None, 1024)              0         
                                                                 
 dropout_186 (Dropout)       (None, 1024)              0         
                                                                 
 dense_249 (Dense)           (None, 512)               524800    
                                                                 
 dropout_187 (Dropout)       (None, 512)               0         
                                                                 
 dense_250 (Dense)           (None, 512)               262656    
                                                                 
 dropout_188 (Dropout)       (None, 512)               0         
                                                                 
 dense_251 (Dense)           (None, 24)                12312     
                                                                 
=================================================================
Total params: 1,438,744
Trainable params: 1,438,744
Non-trainable params: 0
_________________________________________________________________
Epoch 1/200
222/222 [==============================] - 2s 7ms/step - loss: 0.5009 - val_loss: 0.1146
Epoch 2/200
222/222 [==============================] - 1s 6ms/step - loss: 0.2518 - val_loss: 0.1066
Epoch 3/200
222/222 [==============================] - 1s 7ms/step - loss: 0.2090 - val_loss: 0.1016
Epoch 4/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1875 - val_loss: 0.0975
Epoch 5/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1745 - val_loss: 0.0958
Epoch 6/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1646 - val_loss: 0.0964
Epoch 7/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1571 - val_loss: 0.0929
Epoch 8/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1512 - val_loss: 0.0948
Epoch 9/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1460 - val_loss: 0.0947
Epoch 10/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1422 - val_loss: 0.0928
Epoch 11/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1388 - val_loss: 0.0944
Epoch 12/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1355 - val_loss: 0.0900
Epoch 13/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1327 - val_loss: 0.0917
Epoch 14/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1306 - val_loss: 0.0918
Epoch 15/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1282 - val_loss: 0.0895
Epoch 16/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1264 - val_loss: 0.0893
Epoch 17/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1246 - val_loss: 0.0921
Epoch 18/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1233 - val_loss: 0.0927
Epoch 19/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1216 - val_loss: 0.0881
Epoch 20/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1206 - val_loss: 0.0876
Epoch 21/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1192 - val_loss: 0.0896
Epoch 22/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1181 - val_loss: 0.0890
Epoch 23/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1173 - val_loss: 0.0883
Epoch 24/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1164 - val_loss: 0.0903
Epoch 25/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1160 - val_loss: 0.0878
Epoch 26/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1150 - val_loss: 0.0870
Epoch 27/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1144 - val_loss: 0.0871
Epoch 28/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1137 - val_loss: 0.0865
Epoch 29/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1129 - val_loss: 0.0875
Epoch 30/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1125 - val_loss: 0.0862
Epoch 31/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1118 - val_loss: 0.0870
Epoch 32/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1114 - val_loss: 0.0868
Epoch 33/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1109 - val_loss: 0.0872
Epoch 34/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1104 - val_loss: 0.0873
Epoch 35/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1100 - val_loss: 0.0874
Epoch 36/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1094 - val_loss: 0.0867
Epoch 37/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1088 - val_loss: 0.0875
Epoch 38/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1085 - val_loss: 0.0881
Epoch 39/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1078 - val_loss: 0.0852
Epoch 40/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1074 - val_loss: 0.0862
Epoch 41/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1070 - val_loss: 0.0850
Epoch 42/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1066 - val_loss: 0.0854
Epoch 43/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1059 - val_loss: 0.0871
Epoch 44/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1054 - val_loss: 0.0855
Epoch 45/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1049 - val_loss: 0.0858
Epoch 46/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1045 - val_loss: 0.0842
Epoch 47/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1040 - val_loss: 0.0867
Epoch 48/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1037 - val_loss: 0.0844
Epoch 49/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1030 - val_loss: 0.0841
Epoch 50/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1025 - val_loss: 0.0844
Epoch 51/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1022 - val_loss: 0.0842
Epoch 52/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1019 - val_loss: 0.0849
Epoch 53/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1012 - val_loss: 0.0833
Epoch 54/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1008 - val_loss: 0.0847
Epoch 55/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1004 - val_loss: 0.0839
Epoch 56/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0998 - val_loss: 0.0843
Epoch 57/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0995 - val_loss: 0.0833
Epoch 58/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0991 - val_loss: 0.0835
Epoch 59/200
222/222 [==============================] - 2s 7ms/step - loss: 0.0985 - val_loss: 0.0835
Epoch 60/200
222/222 [==============================] - 2s 7ms/step - loss: 0.0982 - val_loss: 0.0846
Epoch 61/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0979 - val_loss: 0.0847
Epoch 62/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0972 - val_loss: 0.0851
Epoch 63/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0974 - val_loss: 0.0838
Epoch 64/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0967 - val_loss: 0.0859
Epoch 65/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0964 - val_loss: 0.0839
Epoch 66/200
222/222 [==============================] - 2s 7ms/step - loss: 0.0961 - val_loss: 0.0821
Epoch 67/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0955 - val_loss: 0.0834
Epoch 68/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0953 - val_loss: 0.0830
Epoch 69/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0948 - val_loss: 0.0831
Epoch 70/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0945 - val_loss: 0.0838
Epoch 71/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0944 - val_loss: 0.0832
Epoch 72/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0940 - val_loss: 0.0835
Epoch 73/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0935 - val_loss: 0.0821
Epoch 74/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0933 - val_loss: 0.0836
Epoch 75/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0929 - val_loss: 0.0838
Epoch 76/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0924 - val_loss: 0.0833
Epoch 77/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0924 - val_loss: 0.0831
Epoch 78/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0921 - val_loss: 0.0840
Epoch 79/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0918 - val_loss: 0.0832
Epoch 80/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0914 - val_loss: 0.0834
Epoch 81/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0913 - val_loss: 0.0842
Epoch 82/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0910 - val_loss: 0.0843
Epoch 83/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0908 - val_loss: 0.0824
Val_yp Shape is 
(75710, 24)
Results === Test == Validation ===== 89
=============================
step 1    -  0.990    0.989
step 2    -  0.984    0.982
step 3    -  0.976    0.974
step 4    -  0.967    0.965
step 5    -  0.958    0.956
step 6    -  0.949    0.947
step 7    -  0.940    0.939
step 8    -  0.931    0.930
step 9    -  0.923    0.923
step 10   -  0.915    0.916
step 11   -  0.907    0.909
step 12   -  0.899    0.901
step 13   -  0.892    0.895
step 14   -  0.886    0.889
step 15   -  0.880    0.883
step 16   -  0.874    0.878
step 17   -  0.868    0.872
step 18   -  0.862    0.868
step 19   -  0.858    0.864
step 20   -  0.853    0.860
step 21   -  0.848    0.855
step 22   -  0.844    0.852
step 23   -  0.840    0.848
step 24   -  0.836    0.845
=============================
Summary   -  21.681    21.738
V_y Shape is 
(75710, 1)
=========== SHAPE INPUT-OUTPUT MATRIXES =====================
shape x_training : (227184, 630)  y_training: :  (227184, 24)
shape x_test     :  (75709, 630)  y_test      :  (75709, 24)
shape x_val      :  (75709, 630)  y_val       :  (75709, 24)
=============================================================
(630,)
Model: "MLP"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_64 (InputLayer)       [(None, 630)]             0         
                                                                 
 dense_252 (Dense)           (None, 1024)              646144    
                                                                 
 elu_63 (ELU)                (None, 1024)              0         
                                                                 
 dropout_189 (Dropout)       (None, 1024)              0         
                                                                 
 dense_253 (Dense)           (None, 512)               524800    
                                                                 
 dropout_190 (Dropout)       (None, 512)               0         
                                                                 
 dense_254 (Dense)           (None, 512)               262656    
                                                                 
 dropout_191 (Dropout)       (None, 512)               0         
                                                                 
 dense_255 (Dense)           (None, 24)                12312     
                                                                 
=================================================================
Total params: 1,445,912
Trainable params: 1,445,912
Non-trainable params: 0
_________________________________________________________________
Epoch 1/200
222/222 [==============================] - 3s 9ms/step - loss: 0.4971 - val_loss: 0.1144
Epoch 2/200
222/222 [==============================] - 1s 7ms/step - loss: 0.2512 - val_loss: 0.1043
Epoch 3/200
222/222 [==============================] - 1s 7ms/step - loss: 0.2087 - val_loss: 0.1016
Epoch 4/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1875 - val_loss: 0.0970
Epoch 5/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1745 - val_loss: 0.0961
Epoch 6/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1645 - val_loss: 0.0953
Epoch 7/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1573 - val_loss: 0.0964
Epoch 8/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1515 - val_loss: 0.0969
Epoch 9/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1463 - val_loss: 0.0913
Epoch 10/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1422 - val_loss: 0.0914
Epoch 11/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1385 - val_loss: 0.0940
Epoch 12/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1353 - val_loss: 0.0927
Epoch 13/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1328 - val_loss: 0.0919
Epoch 14/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1304 - val_loss: 0.0892
Epoch 15/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1282 - val_loss: 0.0889
Epoch 16/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1261 - val_loss: 0.0917
Epoch 17/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1243 - val_loss: 0.0941
Epoch 18/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1232 - val_loss: 0.0881
Epoch 19/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1216 - val_loss: 0.0906
Epoch 20/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1203 - val_loss: 0.0879
Epoch 21/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1191 - val_loss: 0.0885
Epoch 22/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1179 - val_loss: 0.0905
Epoch 23/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1171 - val_loss: 0.0892
Epoch 24/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1163 - val_loss: 0.0894
Epoch 25/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1155 - val_loss: 0.0868
Epoch 26/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1149 - val_loss: 0.0900
Epoch 27/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1139 - val_loss: 0.0868
Epoch 28/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1136 - val_loss: 0.0900
Epoch 29/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1128 - val_loss: 0.0870
Epoch 30/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1121 - val_loss: 0.0878
Epoch 31/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1119 - val_loss: 0.0858
Epoch 32/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1112 - val_loss: 0.0878
Epoch 33/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1108 - val_loss: 0.0865
Epoch 34/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1102 - val_loss: 0.0882
Epoch 35/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1095 - val_loss: 0.0884
Epoch 36/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1090 - val_loss: 0.0885
Epoch 37/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1086 - val_loss: 0.0875
Epoch 38/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1081 - val_loss: 0.0894
Epoch 39/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1075 - val_loss: 0.0859
Epoch 40/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1071 - val_loss: 0.0858
Epoch 41/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1065 - val_loss: 0.0883
Epoch 42/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1059 - val_loss: 0.0851
Epoch 43/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1053 - val_loss: 0.0858
Epoch 44/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1050 - val_loss: 0.0866
Epoch 45/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1044 - val_loss: 0.0850
Epoch 46/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1039 - val_loss: 0.0851
Epoch 47/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1035 - val_loss: 0.0833
Epoch 48/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1030 - val_loss: 0.0857
Epoch 49/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1024 - val_loss: 0.0856
Epoch 50/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1019 - val_loss: 0.0864
Epoch 51/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1017 - val_loss: 0.0842
Epoch 52/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1009 - val_loss: 0.0845
Epoch 53/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1006 - val_loss: 0.0851
Epoch 54/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1001 - val_loss: 0.0863
Epoch 55/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0997 - val_loss: 0.0830
Epoch 56/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0993 - val_loss: 0.0853
Epoch 57/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0986 - val_loss: 0.0833
Epoch 58/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0984 - val_loss: 0.0842
Epoch 59/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0982 - val_loss: 0.0841
Epoch 60/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0974 - val_loss: 0.0851
Epoch 61/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0974 - val_loss: 0.0851
Epoch 62/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0970 - val_loss: 0.0831
Epoch 63/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0964 - val_loss: 0.0838
Epoch 64/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0959 - val_loss: 0.0821
Epoch 65/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0957 - val_loss: 0.0828
Epoch 66/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0953 - val_loss: 0.0826
Epoch 67/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0950 - val_loss: 0.0820
Epoch 68/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0947 - val_loss: 0.0843
Epoch 69/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0942 - val_loss: 0.0846
Epoch 70/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0941 - val_loss: 0.0845
Epoch 71/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0936 - val_loss: 0.0828
Epoch 72/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0932 - val_loss: 0.0823
Epoch 73/200
222/222 [==============================] - 2s 7ms/step - loss: 0.0930 - val_loss: 0.0830
Epoch 74/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0927 - val_loss: 0.0835
Epoch 75/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0926 - val_loss: 0.0836
Epoch 76/200
222/222 [==============================] - 2s 7ms/step - loss: 0.0919 - val_loss: 0.0848
Epoch 77/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0916 - val_loss: 0.0824
Val_yp Shape is 
(75709, 24)
Results === Test == Validation ===== 90
=============================
step 1    -  0.990    0.989
step 2    -  0.983    0.981
step 3    -  0.974    0.972
step 4    -  0.965    0.963
step 5    -  0.956    0.954
step 6    -  0.947    0.945
step 7    -  0.938    0.936
step 8    -  0.928    0.927
step 9    -  0.920    0.919
step 10   -  0.912    0.912
step 11   -  0.905    0.905
step 12   -  0.898    0.898
step 13   -  0.891    0.892
step 14   -  0.884    0.885
step 15   -  0.876    0.878
step 16   -  0.869    0.871
step 17   -  0.862    0.866
step 18   -  0.856    0.860
step 19   -  0.850    0.855
step 20   -  0.844    0.850
step 21   -  0.840    0.847
step 22   -  0.834    0.841
step 23   -  0.828    0.836
step 24   -  0.821    0.830
=============================
Summary   -  21.569    21.612
V_y Shape is 
(75709, 1)
=========== SHAPE INPUT-OUTPUT MATRIXES =====================
shape x_training : (227183, 637)  y_training: :  (227183, 24)
shape x_test     :  (75708, 637)  y_test      :  (75708, 24)
shape x_val      :  (75709, 637)  y_val       :  (75709, 24)
=============================================================
(637,)
Model: "MLP"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_65 (InputLayer)       [(None, 637)]             0         
                                                                 
 dense_256 (Dense)           (None, 1024)              653312    
                                                                 
 elu_64 (ELU)                (None, 1024)              0         
                                                                 
 dropout_192 (Dropout)       (None, 1024)              0         
                                                                 
 dense_257 (Dense)           (None, 512)               524800    
                                                                 
 dropout_193 (Dropout)       (None, 512)               0         
                                                                 
 dense_258 (Dense)           (None, 512)               262656    
                                                                 
 dropout_194 (Dropout)       (None, 512)               0         
                                                                 
 dense_259 (Dense)           (None, 24)                12312     
                                                                 
=================================================================
Total params: 1,453,080
Trainable params: 1,453,080
Non-trainable params: 0
_________________________________________________________________
Epoch 1/200
222/222 [==============================] - 2s 7ms/step - loss: 0.4947 - val_loss: 0.1160
Epoch 2/200
222/222 [==============================] - 1s 6ms/step - loss: 0.2525 - val_loss: 0.1048
Epoch 3/200
222/222 [==============================] - 1s 7ms/step - loss: 0.2091 - val_loss: 0.1001
Epoch 4/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1880 - val_loss: 0.0998
Epoch 5/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1742 - val_loss: 0.0976
Epoch 6/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1645 - val_loss: 0.0987
Epoch 7/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1571 - val_loss: 0.0972
Epoch 8/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1512 - val_loss: 0.0939
Epoch 9/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1462 - val_loss: 0.0931
Epoch 10/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1423 - val_loss: 0.0966
Epoch 11/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1385 - val_loss: 0.0917
Epoch 12/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1350 - val_loss: 0.0912
Epoch 13/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1323 - val_loss: 0.0947
Epoch 14/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1302 - val_loss: 0.0900
Epoch 15/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1282 - val_loss: 0.0913
Epoch 16/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1262 - val_loss: 0.0902
Epoch 17/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1244 - val_loss: 0.0914
Epoch 18/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1228 - val_loss: 0.0906
Epoch 19/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1214 - val_loss: 0.0905
Epoch 20/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1202 - val_loss: 0.0888
Epoch 21/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1189 - val_loss: 0.0903
Epoch 22/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1178 - val_loss: 0.0905
Epoch 23/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1170 - val_loss: 0.0879
Epoch 24/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1164 - val_loss: 0.0899
Epoch 25/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1154 - val_loss: 0.0879
Epoch 26/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1146 - val_loss: 0.0870
Epoch 27/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1140 - val_loss: 0.0895
Epoch 28/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1133 - val_loss: 0.0880
Epoch 29/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1127 - val_loss: 0.0871
Epoch 30/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1119 - val_loss: 0.0892
Epoch 31/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1116 - val_loss: 0.0869
Epoch 32/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1109 - val_loss: 0.0876
Epoch 33/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1103 - val_loss: 0.0873
Epoch 34/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1098 - val_loss: 0.0872
Epoch 35/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1093 - val_loss: 0.0865
Epoch 36/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1086 - val_loss: 0.0864
Epoch 37/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1081 - val_loss: 0.0882
Epoch 38/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1077 - val_loss: 0.0866
Epoch 39/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1073 - val_loss: 0.0854
Epoch 40/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1066 - val_loss: 0.0860
Epoch 41/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1060 - val_loss: 0.0861
Epoch 42/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1057 - val_loss: 0.0855
Epoch 43/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1051 - val_loss: 0.0863
Epoch 44/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1046 - val_loss: 0.0839
Epoch 45/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1042 - val_loss: 0.0851
Epoch 46/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1035 - val_loss: 0.0866
Epoch 47/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1031 - val_loss: 0.0858
Epoch 48/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1026 - val_loss: 0.0852
Epoch 49/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1021 - val_loss: 0.0827
Epoch 50/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1014 - val_loss: 0.0839
Epoch 51/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1013 - val_loss: 0.0865
Epoch 52/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1007 - val_loss: 0.0836
Epoch 53/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0999 - val_loss: 0.0832
Epoch 54/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0997 - val_loss: 0.0835
Epoch 55/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0992 - val_loss: 0.0842
Epoch 56/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0990 - val_loss: 0.0835
Epoch 57/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0985 - val_loss: 0.0854
Epoch 58/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0982 - val_loss: 0.0854
Epoch 59/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0978 - val_loss: 0.0843
Val_yp Shape is 
(75709, 24)
Results === Test == Validation ===== 91
=============================
step 1    -  0.990    0.989
step 2    -  0.983    0.982
step 3    -  0.974    0.973
step 4    -  0.965    0.963
step 5    -  0.955    0.953
step 6    -  0.945    0.944
step 7    -  0.935    0.934
step 8    -  0.926    0.925
step 9    -  0.917    0.917
step 10   -  0.908    0.909
step 11   -  0.900    0.901
step 12   -  0.891    0.893
step 13   -  0.884    0.886
step 14   -  0.876    0.879
step 15   -  0.869    0.872
step 16   -  0.863    0.867
step 17   -  0.858    0.862
step 18   -  0.852    0.856
step 19   -  0.846    0.851
step 20   -  0.841    0.847
step 21   -  0.838    0.845
step 22   -  0.833    0.840
step 23   -  0.826    0.834
step 24   -  0.820    0.829
=============================
Summary   -  21.494    21.549
V_y Shape is 
(75709, 1)
=========== SHAPE INPUT-OUTPUT MATRIXES =====================
shape x_training : (227182, 644)  y_training: :  (227182, 24)
shape x_test     :  (75708, 644)  y_test      :  (75708, 24)
shape x_val      :  (75708, 644)  y_val       :  (75708, 24)
=============================================================
(644,)
Model: "MLP"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_66 (InputLayer)       [(None, 644)]             0         
                                                                 
 dense_260 (Dense)           (None, 1024)              660480    
                                                                 
 elu_65 (ELU)                (None, 1024)              0         
                                                                 
 dropout_195 (Dropout)       (None, 1024)              0         
                                                                 
 dense_261 (Dense)           (None, 512)               524800    
                                                                 
 dropout_196 (Dropout)       (None, 512)               0         
                                                                 
 dense_262 (Dense)           (None, 512)               262656    
                                                                 
 dropout_197 (Dropout)       (None, 512)               0         
                                                                 
 dense_263 (Dense)           (None, 24)                12312     
                                                                 
=================================================================
Total params: 1,460,248
Trainable params: 1,460,248
Non-trainable params: 0
_________________________________________________________________
Epoch 1/200
222/222 [==============================] - 2s 8ms/step - loss: 0.4941 - val_loss: 0.1149
Epoch 2/200
222/222 [==============================] - 1s 6ms/step - loss: 0.2518 - val_loss: 0.1045
Epoch 3/200
222/222 [==============================] - 1s 6ms/step - loss: 0.2087 - val_loss: 0.1001
Epoch 4/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1873 - val_loss: 0.0990
Epoch 5/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1735 - val_loss: 0.0975
Epoch 6/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1641 - val_loss: 0.0965
Epoch 7/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1569 - val_loss: 0.0970
Epoch 8/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1507 - val_loss: 0.0925
Epoch 9/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1459 - val_loss: 0.0927
Epoch 10/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1414 - val_loss: 0.0922
Epoch 11/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1382 - val_loss: 0.0919
Epoch 12/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1350 - val_loss: 0.0905
Epoch 13/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1321 - val_loss: 0.0891
Epoch 14/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1297 - val_loss: 0.0909
Epoch 15/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1276 - val_loss: 0.0903
Epoch 16/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1257 - val_loss: 0.0906
Epoch 17/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1240 - val_loss: 0.0900
Epoch 18/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1226 - val_loss: 0.0898
Epoch 19/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1209 - val_loss: 0.0919
Epoch 20/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1200 - val_loss: 0.0890
Epoch 21/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1189 - val_loss: 0.0884
Epoch 22/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1179 - val_loss: 0.0904
Epoch 23/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1167 - val_loss: 0.0899
Epoch 24/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1160 - val_loss: 0.0867
Epoch 25/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1152 - val_loss: 0.0888
Epoch 26/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1144 - val_loss: 0.0879
Epoch 27/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1136 - val_loss: 0.0867
Epoch 28/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1128 - val_loss: 0.0890
Epoch 29/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1126 - val_loss: 0.0880
Epoch 30/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1118 - val_loss: 0.0875
Epoch 31/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1113 - val_loss: 0.0894
Epoch 32/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1104 - val_loss: 0.0874
Epoch 33/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1100 - val_loss: 0.0876
Epoch 34/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1095 - val_loss: 0.0868
Epoch 35/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1090 - val_loss: 0.0855
Epoch 36/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1085 - val_loss: 0.0864
Epoch 37/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1079 - val_loss: 0.0875
Epoch 38/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1074 - val_loss: 0.0878
Epoch 39/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1072 - val_loss: 0.0864
Epoch 40/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1064 - val_loss: 0.0860
Epoch 41/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1059 - val_loss: 0.0864
Epoch 42/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1052 - val_loss: 0.0841
Epoch 43/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1051 - val_loss: 0.0840
Epoch 44/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1044 - val_loss: 0.0880
Epoch 45/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1036 - val_loss: 0.0839
Epoch 46/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1034 - val_loss: 0.0840
Epoch 47/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1029 - val_loss: 0.0848
Epoch 48/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1022 - val_loss: 0.0835
Epoch 49/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1021 - val_loss: 0.0853
Epoch 50/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1012 - val_loss: 0.0850
Epoch 51/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1008 - val_loss: 0.0844
Epoch 52/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1003 - val_loss: 0.0840
Epoch 53/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0999 - val_loss: 0.0852
Epoch 54/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0993 - val_loss: 0.0847
Epoch 55/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0992 - val_loss: 0.0836
Epoch 56/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0985 - val_loss: 0.0868
Epoch 57/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0982 - val_loss: 0.0833
Epoch 58/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0978 - val_loss: 0.0835
Epoch 59/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0975 - val_loss: 0.0837
Epoch 60/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0973 - val_loss: 0.0860
Epoch 61/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0966 - val_loss: 0.0839
Epoch 62/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0961 - val_loss: 0.0825
Epoch 63/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0959 - val_loss: 0.0836
Epoch 64/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0953 - val_loss: 0.0838
Epoch 65/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0951 - val_loss: 0.0847
Epoch 66/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0949 - val_loss: 0.0860
Epoch 67/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0944 - val_loss: 0.0837
Epoch 68/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0940 - val_loss: 0.0834
Epoch 69/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0937 - val_loss: 0.0847
Epoch 70/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0934 - val_loss: 0.0834
Epoch 71/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0933 - val_loss: 0.0832
Epoch 72/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0928 - val_loss: 0.0839
Val_yp Shape is 
(75708, 24)
Results === Test == Validation ===== 92
=============================
step 1    -  0.990    0.989
step 2    -  0.983    0.982
step 3    -  0.975    0.973
step 4    -  0.966    0.964
step 5    -  0.957    0.955
step 6    -  0.948    0.946
step 7    -  0.938    0.937
step 8    -  0.930    0.929
step 9    -  0.922    0.922
step 10   -  0.914    0.914
step 11   -  0.906    0.907
step 12   -  0.899    0.900
step 13   -  0.892    0.893
step 14   -  0.885    0.887
step 15   -  0.879    0.881
step 16   -  0.874    0.876
step 17   -  0.868    0.871
step 18   -  0.862    0.866
step 19   -  0.856    0.861
step 20   -  0.852    0.857
step 21   -  0.846    0.851
step 22   -  0.840    0.846
step 23   -  0.836    0.842
step 24   -  0.831    0.838
=============================
Summary   -  21.652    21.687
V_y Shape is 
(75708, 1)
=========== SHAPE INPUT-OUTPUT MATRIXES =====================
shape x_training : (227181, 651)  y_training: :  (227181, 24)
shape x_test     :  (75707, 651)  y_test      :  (75707, 24)
shape x_val      :  (75708, 651)  y_val       :  (75708, 24)
=============================================================
(651,)
Model: "MLP"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_67 (InputLayer)       [(None, 651)]             0         
                                                                 
 dense_264 (Dense)           (None, 1024)              667648    
                                                                 
 elu_66 (ELU)                (None, 1024)              0         
                                                                 
 dropout_198 (Dropout)       (None, 1024)              0         
                                                                 
 dense_265 (Dense)           (None, 512)               524800    
                                                                 
 dropout_199 (Dropout)       (None, 512)               0         
                                                                 
 dense_266 (Dense)           (None, 512)               262656    
                                                                 
 dropout_200 (Dropout)       (None, 512)               0         
                                                                 
 dense_267 (Dense)           (None, 24)                12312     
                                                                 
=================================================================
Total params: 1,467,416
Trainable params: 1,467,416
Non-trainable params: 0
_________________________________________________________________
Epoch 1/200
222/222 [==============================] - 2s 8ms/step - loss: 0.4945 - val_loss: 0.1156
Epoch 2/200
222/222 [==============================] - 1s 6ms/step - loss: 0.2520 - val_loss: 0.1047
Epoch 3/200
222/222 [==============================] - 1s 6ms/step - loss: 0.2092 - val_loss: 0.1041
Epoch 4/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1882 - val_loss: 0.0958
Epoch 5/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1744 - val_loss: 0.0965
Epoch 6/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1645 - val_loss: 0.0971
Epoch 7/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1570 - val_loss: 0.0996
Epoch 8/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1511 - val_loss: 0.0916
Epoch 9/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1459 - val_loss: 0.0964
Epoch 10/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1416 - val_loss: 0.0950
Epoch 11/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1383 - val_loss: 0.0903
Epoch 12/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1348 - val_loss: 0.0908
Epoch 13/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1324 - val_loss: 0.0900
Epoch 14/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1299 - val_loss: 0.0891
Epoch 15/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1277 - val_loss: 0.0883
Epoch 16/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1257 - val_loss: 0.0906
Epoch 17/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1239 - val_loss: 0.0915
Epoch 18/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1224 - val_loss: 0.0898
Epoch 19/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1210 - val_loss: 0.0905
Epoch 20/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1198 - val_loss: 0.0885
Epoch 21/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1184 - val_loss: 0.0897
Epoch 22/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1174 - val_loss: 0.0902
Epoch 23/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1167 - val_loss: 0.0883
Epoch 24/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1158 - val_loss: 0.0880
Epoch 25/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1151 - val_loss: 0.0873
Epoch 26/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1144 - val_loss: 0.0891
Epoch 27/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1134 - val_loss: 0.0901
Epoch 28/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1127 - val_loss: 0.0885
Epoch 29/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1122 - val_loss: 0.0906
Epoch 30/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1118 - val_loss: 0.0885
Epoch 31/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1110 - val_loss: 0.0871
Epoch 32/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1106 - val_loss: 0.0879
Epoch 33/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1102 - val_loss: 0.0871
Epoch 34/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1094 - val_loss: 0.0863
Epoch 35/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1089 - val_loss: 0.0872
Epoch 36/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1085 - val_loss: 0.0857
Epoch 37/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1078 - val_loss: 0.0874
Epoch 38/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1074 - val_loss: 0.0865
Epoch 39/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1065 - val_loss: 0.0851
Epoch 40/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1060 - val_loss: 0.0876
Epoch 41/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1057 - val_loss: 0.0861
Epoch 42/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1052 - val_loss: 0.0836
Epoch 43/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1047 - val_loss: 0.0877
Epoch 44/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1040 - val_loss: 0.0866
Epoch 45/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1037 - val_loss: 0.0863
Epoch 46/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1031 - val_loss: 0.0841
Epoch 47/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1026 - val_loss: 0.0858
Epoch 48/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1022 - val_loss: 0.0851
Epoch 49/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1015 - val_loss: 0.0858
Epoch 50/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1009 - val_loss: 0.0838
Epoch 51/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1007 - val_loss: 0.0862
Epoch 52/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1002 - val_loss: 0.0834
Epoch 53/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0995 - val_loss: 0.0829
Epoch 54/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0992 - val_loss: 0.0830
Epoch 55/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0987 - val_loss: 0.0840
Epoch 56/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0982 - val_loss: 0.0829
Epoch 57/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0978 - val_loss: 0.0822
Epoch 58/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0975 - val_loss: 0.0823
Epoch 59/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0971 - val_loss: 0.0852
Epoch 60/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0966 - val_loss: 0.0831
Epoch 61/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0961 - val_loss: 0.0841
Epoch 62/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0959 - val_loss: 0.0825
Epoch 63/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0955 - val_loss: 0.0830
Epoch 64/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0951 - val_loss: 0.0845
Epoch 65/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0948 - val_loss: 0.0838
Epoch 66/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0944 - val_loss: 0.0843
Epoch 67/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0943 - val_loss: 0.0843
Val_yp Shape is 
(75708, 24)
Results === Test == Validation ===== 93
=============================
step 1    -  0.990    0.989
step 2    -  0.984    0.982
step 3    -  0.975    0.973
step 4    -  0.966    0.964
step 5    -  0.957    0.954
step 6    -  0.947    0.945
step 7    -  0.938    0.937
step 8    -  0.929    0.928
step 9    -  0.921    0.920
step 10   -  0.914    0.913
step 11   -  0.907    0.907
step 12   -  0.900    0.900
step 13   -  0.894    0.895
step 14   -  0.889    0.890
step 15   -  0.883    0.884
step 16   -  0.877    0.879
step 17   -  0.872    0.874
step 18   -  0.867    0.870
step 19   -  0.861    0.864
step 20   -  0.856    0.860
step 21   -  0.852    0.856
step 22   -  0.847    0.852
step 23   -  0.842    0.848
step 24   -  0.836    0.843
=============================
Summary   -  21.705    21.729
V_y Shape is 
(75708, 1)
=========== SHAPE INPUT-OUTPUT MATRIXES =====================
shape x_training : (227180, 658)  y_training: :  (227180, 24)
shape x_test     :  (75707, 658)  y_test      :  (75707, 24)
shape x_val      :  (75707, 658)  y_val       :  (75707, 24)
=============================================================
(658,)
Model: "MLP"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_68 (InputLayer)       [(None, 658)]             0         
                                                                 
 dense_268 (Dense)           (None, 1024)              674816    
                                                                 
 elu_67 (ELU)                (None, 1024)              0         
                                                                 
 dropout_201 (Dropout)       (None, 1024)              0         
                                                                 
 dense_269 (Dense)           (None, 512)               524800    
                                                                 
 dropout_202 (Dropout)       (None, 512)               0         
                                                                 
 dense_270 (Dense)           (None, 512)               262656    
                                                                 
 dropout_203 (Dropout)       (None, 512)               0         
                                                                 
 dense_271 (Dense)           (None, 24)                12312     
                                                                 
=================================================================
Total params: 1,474,584
Trainable params: 1,474,584
Non-trainable params: 0
_________________________________________________________________
Epoch 1/200
222/222 [==============================] - 2s 9ms/step - loss: 0.4987 - val_loss: 0.1132
Epoch 2/200
222/222 [==============================] - 1s 7ms/step - loss: 0.2528 - val_loss: 0.1034
Epoch 3/200
222/222 [==============================] - 1s 7ms/step - loss: 0.2095 - val_loss: 0.1004
Epoch 4/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1886 - val_loss: 0.0989
Epoch 5/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1748 - val_loss: 0.0975
Epoch 6/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1651 - val_loss: 0.0960
Epoch 7/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1573 - val_loss: 0.0926
Epoch 8/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1513 - val_loss: 0.0933
Epoch 9/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1462 - val_loss: 0.0928
Epoch 10/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1422 - val_loss: 0.0932
Epoch 11/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1383 - val_loss: 0.0923
Epoch 12/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1353 - val_loss: 0.0893
Epoch 13/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1324 - val_loss: 0.0905
Epoch 14/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1299 - val_loss: 0.0928
Epoch 15/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1280 - val_loss: 0.0904
Epoch 16/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1258 - val_loss: 0.0889
Epoch 17/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1242 - val_loss: 0.0893
Epoch 18/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1225 - val_loss: 0.0890
Epoch 19/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1211 - val_loss: 0.0885
Epoch 20/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1200 - val_loss: 0.0899
Epoch 21/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1187 - val_loss: 0.0899
Epoch 22/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1176 - val_loss: 0.0876
Epoch 23/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1167 - val_loss: 0.0898
Epoch 24/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1156 - val_loss: 0.0884
Epoch 25/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1150 - val_loss: 0.0863
Epoch 26/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1142 - val_loss: 0.0888
Epoch 27/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1136 - val_loss: 0.0873
Epoch 28/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1128 - val_loss: 0.0883
Epoch 29/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1122 - val_loss: 0.0876
Epoch 30/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1117 - val_loss: 0.0889
Epoch 31/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1111 - val_loss: 0.0885
Epoch 32/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1106 - val_loss: 0.0880
Epoch 33/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1099 - val_loss: 0.0857
Epoch 34/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1093 - val_loss: 0.0870
Epoch 35/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1089 - val_loss: 0.0869
Epoch 36/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1085 - val_loss: 0.0863
Epoch 37/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1079 - val_loss: 0.0855
Epoch 38/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1073 - val_loss: 0.0879
Epoch 39/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1067 - val_loss: 0.0861
Epoch 40/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1064 - val_loss: 0.0858
Epoch 41/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1055 - val_loss: 0.0860
Epoch 42/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1053 - val_loss: 0.0850
Epoch 43/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1046 - val_loss: 0.0853
Epoch 44/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1042 - val_loss: 0.0847
Epoch 45/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1038 - val_loss: 0.0843
Epoch 46/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1032 - val_loss: 0.0861
Epoch 47/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1026 - val_loss: 0.0839
Epoch 48/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1022 - val_loss: 0.0836
Epoch 49/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1018 - val_loss: 0.0863
Epoch 50/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1010 - val_loss: 0.0860
Epoch 51/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1006 - val_loss: 0.0850
Epoch 52/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1004 - val_loss: 0.0842
Epoch 53/200
222/222 [==============================] - 2s 7ms/step - loss: 0.0999 - val_loss: 0.0869
Epoch 54/200
222/222 [==============================] - 2s 7ms/step - loss: 0.0993 - val_loss: 0.0860
Epoch 55/200
222/222 [==============================] - 2s 7ms/step - loss: 0.0991 - val_loss: 0.0837
Epoch 56/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0986 - val_loss: 0.0842
Epoch 57/200
222/222 [==============================] - 2s 7ms/step - loss: 0.0980 - val_loss: 0.0840
Epoch 58/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0974 - val_loss: 0.0851
Val_yp Shape is 
(75707, 24)
Results === Test == Validation ===== 94
=============================
step 1    -  0.989    0.988
step 2    -  0.983    0.981
step 3    -  0.974    0.972
step 4    -  0.965    0.963
step 5    -  0.956    0.954
step 6    -  0.947    0.945
step 7    -  0.937    0.936
step 8    -  0.929    0.928
step 9    -  0.920    0.920
step 10   -  0.912    0.913
step 11   -  0.905    0.906
step 12   -  0.898    0.900
step 13   -  0.891    0.894
step 14   -  0.885    0.889
step 15   -  0.879    0.883
step 16   -  0.873    0.878
step 17   -  0.868    0.874
step 18   -  0.862    0.868
step 19   -  0.857    0.864
step 20   -  0.853    0.860
step 21   -  0.848    0.856
step 22   -  0.844    0.853
step 23   -  0.841    0.851
step 24   -  0.837    0.847
=============================
Summary   -  21.654    21.723
V_y Shape is 
(75707, 1)
=========== SHAPE INPUT-OUTPUT MATRIXES =====================
shape x_training : (227179, 665)  y_training: :  (227179, 24)
shape x_test     :  (75706, 665)  y_test      :  (75706, 24)
shape x_val      :  (75707, 665)  y_val       :  (75707, 24)
=============================================================
(665,)
Model: "MLP"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_69 (InputLayer)       [(None, 665)]             0         
                                                                 
 dense_272 (Dense)           (None, 1024)              681984    
                                                                 
 elu_68 (ELU)                (None, 1024)              0         
                                                                 
 dropout_204 (Dropout)       (None, 1024)              0         
                                                                 
 dense_273 (Dense)           (None, 512)               524800    
                                                                 
 dropout_205 (Dropout)       (None, 512)               0         
                                                                 
 dense_274 (Dense)           (None, 512)               262656    
                                                                 
 dropout_206 (Dropout)       (None, 512)               0         
                                                                 
 dense_275 (Dense)           (None, 24)                12312     
                                                                 
=================================================================
Total params: 1,481,752
Trainable params: 1,481,752
Non-trainable params: 0
_________________________________________________________________
Epoch 1/200
222/222 [==============================] - 2s 7ms/step - loss: 0.5047 - val_loss: 0.1146
Epoch 2/200
222/222 [==============================] - 1s 6ms/step - loss: 0.2549 - val_loss: 0.1065
Epoch 3/200
222/222 [==============================] - 1s 7ms/step - loss: 0.2105 - val_loss: 0.1035
Epoch 4/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1890 - val_loss: 0.0981
Epoch 5/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1749 - val_loss: 0.0979
Epoch 6/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1652 - val_loss: 0.0961
Epoch 7/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1576 - val_loss: 0.0955
Epoch 8/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1515 - val_loss: 0.0931
Epoch 9/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1460 - val_loss: 0.0925
Epoch 10/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1419 - val_loss: 0.0951
Epoch 11/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1382 - val_loss: 0.0941
Epoch 12/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1350 - val_loss: 0.0921
Epoch 13/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1321 - val_loss: 0.0908
Epoch 14/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1300 - val_loss: 0.0919
Epoch 15/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1278 - val_loss: 0.0895
Epoch 16/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1258 - val_loss: 0.0905
Epoch 17/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1241 - val_loss: 0.0901
Epoch 18/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1224 - val_loss: 0.0899
Epoch 19/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1210 - val_loss: 0.0886
Epoch 20/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1198 - val_loss: 0.0875
Epoch 21/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1184 - val_loss: 0.0877
Epoch 22/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1175 - val_loss: 0.0894
Epoch 23/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1167 - val_loss: 0.0897
Epoch 24/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1157 - val_loss: 0.0890
Epoch 25/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1149 - val_loss: 0.0876
Epoch 26/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1139 - val_loss: 0.0894
Epoch 27/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1132 - val_loss: 0.0886
Epoch 28/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1127 - val_loss: 0.0879
Epoch 29/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1121 - val_loss: 0.0879
Epoch 30/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1113 - val_loss: 0.0877
Val_yp Shape is 
(75707, 24)
Results === Test == Validation ===== 95
=============================
step 1    -  0.989    0.988
step 2    -  0.981    0.980
step 3    -  0.972    0.971
step 4    -  0.962    0.960
step 5    -  0.952    0.950
step 6    -  0.942    0.940
step 7    -  0.931    0.930
step 8    -  0.921    0.921
step 9    -  0.912    0.912
step 10   -  0.904    0.905
step 11   -  0.897    0.898
step 12   -  0.889    0.891
step 13   -  0.883    0.885
step 14   -  0.878    0.881
step 15   -  0.874    0.877
step 16   -  0.869    0.872
step 17   -  0.865    0.869
step 18   -  0.861    0.865
step 19   -  0.857    0.862
step 20   -  0.851    0.857
step 21   -  0.847    0.853
step 22   -  0.843    0.849
step 23   -  0.837    0.845
step 24   -  0.832    0.840
=============================
Summary   -  21.549    21.601
V_y Shape is 
(75707, 1)
=========== SHAPE INPUT-OUTPUT MATRIXES =====================
shape x_training : (227178, 672)  y_training: :  (227178, 24)
shape x_test     :  (75706, 672)  y_test      :  (75706, 24)
shape x_val      :  (75706, 672)  y_val       :  (75706, 24)
=============================================================
(672,)
Model: "MLP"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_70 (InputLayer)       [(None, 672)]             0         
                                                                 
 dense_276 (Dense)           (None, 1024)              689152    
                                                                 
 elu_69 (ELU)                (None, 1024)              0         
                                                                 
 dropout_207 (Dropout)       (None, 1024)              0         
                                                                 
 dense_277 (Dense)           (None, 512)               524800    
                                                                 
 dropout_208 (Dropout)       (None, 512)               0         
                                                                 
 dense_278 (Dense)           (None, 512)               262656    
                                                                 
 dropout_209 (Dropout)       (None, 512)               0         
                                                                 
 dense_279 (Dense)           (None, 24)                12312     
                                                                 
=================================================================
Total params: 1,488,920
Trainable params: 1,488,920
Non-trainable params: 0
_________________________________________________________________
Epoch 1/200
222/222 [==============================] - 2s 9ms/step - loss: 0.4969 - val_loss: 0.1167
Epoch 2/200
222/222 [==============================] - 1s 7ms/step - loss: 0.2534 - val_loss: 0.1052
Epoch 3/200
222/222 [==============================] - 1s 7ms/step - loss: 0.2094 - val_loss: 0.1012
Epoch 4/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1879 - val_loss: 0.0984
Epoch 5/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1746 - val_loss: 0.0996
Epoch 6/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1647 - val_loss: 0.0953
Epoch 7/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1568 - val_loss: 0.0965
Epoch 8/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1510 - val_loss: 0.0922
Epoch 9/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1457 - val_loss: 0.0913
Epoch 10/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1416 - val_loss: 0.0917
Epoch 11/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1378 - val_loss: 0.0924
Epoch 12/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1349 - val_loss: 0.0927
Epoch 13/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1322 - val_loss: 0.0949
Epoch 14/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1297 - val_loss: 0.0907
Epoch 15/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1275 - val_loss: 0.0917
Epoch 16/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1251 - val_loss: 0.0901
Epoch 17/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1236 - val_loss: 0.0910
Epoch 18/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1222 - val_loss: 0.0885
Epoch 19/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1208 - val_loss: 0.0893
Epoch 20/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1194 - val_loss: 0.0900
Epoch 21/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1182 - val_loss: 0.0870
Epoch 22/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1171 - val_loss: 0.0870
Epoch 23/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1164 - val_loss: 0.0889
Epoch 24/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1154 - val_loss: 0.0887
Epoch 25/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1146 - val_loss: 0.0908
Epoch 26/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1138 - val_loss: 0.0896
Epoch 27/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1133 - val_loss: 0.0897
Epoch 28/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1124 - val_loss: 0.0863
Epoch 29/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1118 - val_loss: 0.0881
Epoch 30/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1111 - val_loss: 0.0880
Epoch 31/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1107 - val_loss: 0.0869
Epoch 32/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1099 - val_loss: 0.0876
Epoch 33/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1095 - val_loss: 0.0869
Epoch 34/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1090 - val_loss: 0.0861
Epoch 35/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1085 - val_loss: 0.0873
Epoch 36/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1078 - val_loss: 0.0878
Epoch 37/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1074 - val_loss: 0.0876
Epoch 38/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1069 - val_loss: 0.0867
Epoch 39/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1063 - val_loss: 0.0859
Epoch 40/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1058 - val_loss: 0.0876
Epoch 41/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1053 - val_loss: 0.0861
Epoch 42/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1049 - val_loss: 0.0874
Epoch 43/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1041 - val_loss: 0.0866
Epoch 44/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1037 - val_loss: 0.0874
Epoch 45/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1032 - val_loss: 0.0842
Epoch 46/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1026 - val_loss: 0.0857
Epoch 47/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1021 - val_loss: 0.0869
Epoch 48/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1018 - val_loss: 0.0850
Epoch 49/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1011 - val_loss: 0.0855
Epoch 50/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1005 - val_loss: 0.0848
Epoch 51/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1000 - val_loss: 0.0853
Epoch 52/200
222/222 [==============================] - 2s 7ms/step - loss: 0.0995 - val_loss: 0.0828
Epoch 53/200
222/222 [==============================] - 2s 7ms/step - loss: 0.0992 - val_loss: 0.0846
Epoch 54/200
222/222 [==============================] - 2s 7ms/step - loss: 0.0989 - val_loss: 0.0837
Epoch 55/200
222/222 [==============================] - 2s 7ms/step - loss: 0.0982 - val_loss: 0.0845
Epoch 56/200
222/222 [==============================] - 2s 7ms/step - loss: 0.0979 - val_loss: 0.0836
Epoch 57/200
222/222 [==============================] - 2s 7ms/step - loss: 0.0973 - val_loss: 0.0844
Epoch 58/200
222/222 [==============================] - 2s 7ms/step - loss: 0.0971 - val_loss: 0.0845
Epoch 59/200
222/222 [==============================] - 2s 7ms/step - loss: 0.0965 - val_loss: 0.0838
Epoch 60/200
222/222 [==============================] - 2s 7ms/step - loss: 0.0962 - val_loss: 0.0846
Epoch 61/200
222/222 [==============================] - 2s 7ms/step - loss: 0.0958 - val_loss: 0.0849
Epoch 62/200
222/222 [==============================] - 2s 7ms/step - loss: 0.0953 - val_loss: 0.0837
Val_yp Shape is 
(75706, 24)
Results === Test == Validation ===== 96
=============================
step 1    -  0.990    0.989
step 2    -  0.983    0.982
step 3    -  0.974    0.973
step 4    -  0.965    0.964
step 5    -  0.956    0.954
step 6    -  0.946    0.945
step 7    -  0.937    0.936
step 8    -  0.928    0.928
step 9    -  0.919    0.920
step 10   -  0.912    0.913
step 11   -  0.904    0.906
step 12   -  0.897    0.899
step 13   -  0.890    0.892
step 14   -  0.883    0.886
step 15   -  0.876    0.880
step 16   -  0.870    0.874
step 17   -  0.862    0.866
step 18   -  0.855    0.861
step 19   -  0.850    0.856
step 20   -  0.844    0.851
step 21   -  0.841    0.848
step 22   -  0.836    0.844
step 23   -  0.832    0.841
step 24   -  0.827    0.837
=============================
Summary   -  21.578    21.646
V_y Shape is 
(75706, 1)
=========== SHAPE INPUT-OUTPUT MATRIXES =====================
shape x_training : (227177, 679)  y_training: :  (227177, 24)
shape x_test     :  (75705, 679)  y_test      :  (75705, 24)
shape x_val      :  (75706, 679)  y_val       :  (75706, 24)
=============================================================
(679,)
Model: "MLP"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_71 (InputLayer)       [(None, 679)]             0         
                                                                 
 dense_280 (Dense)           (None, 1024)              696320    
                                                                 
 elu_70 (ELU)                (None, 1024)              0         
                                                                 
 dropout_210 (Dropout)       (None, 1024)              0         
                                                                 
 dense_281 (Dense)           (None, 512)               524800    
                                                                 
 dropout_211 (Dropout)       (None, 512)               0         
                                                                 
 dense_282 (Dense)           (None, 512)               262656    
                                                                 
 dropout_212 (Dropout)       (None, 512)               0         
                                                                 
 dense_283 (Dense)           (None, 24)                12312     
                                                                 
=================================================================
Total params: 1,496,088
Trainable params: 1,496,088
Non-trainable params: 0

=========== SHAPE INPUT-OUTPUT MATRIXES =====================
shape x_training : (227178, 672)  y_training: :  (227178, 24)
shape x_test     :  (75706, 672)  y_test      :  (75706, 24)
shape x_val      :  (75706, 672)  y_val       :  (75706, 24)
=============================================================
(672,)
Model: "MLP"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_1 (InputLayer)        [(None, 672)]             0         
                                                                 
 dense (Dense)               (None, 1024)              689152    
                                                                 
 elu (ELU)                   (None, 1024)              0         
                                                                 
 dropout (Dropout)           (None, 1024)              0         
                                                                 
 dense_1 (Dense)             (None, 512)               524800    
                                                                 
 dropout_1 (Dropout)         (None, 512)               0         
                                                                 
 dense_2 (Dense)             (None, 512)               262656    
                                                                 
 dropout_2 (Dropout)         (None, 512)               0         
                                                                 
 dense_3 (Dense)             (None, 24)                12312     
                                                                 
=================================================================
Total params: 1,488,920
Trainable params: 1,488,920
Non-trainable params: 0
_________________________________________________________________
Epoch 1/200
222/222 [==============================] - 3s 7ms/step - loss: 0.4968 - val_loss: 0.1157
Epoch 2/200
222/222 [==============================] - 1s 6ms/step - loss: 0.2515 - val_loss: 0.1047
Epoch 3/200
222/222 [==============================] - 1s 6ms/step - loss: 0.2086 - val_loss: 0.1017
Epoch 4/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1876 - val_loss: 0.0979
Epoch 5/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1738 - val_loss: 0.0973
Epoch 6/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1642 - val_loss: 0.0936
Epoch 7/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1566 - val_loss: 0.0954
Epoch 8/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1506 - val_loss: 0.0996
Epoch 9/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1453 - val_loss: 0.0939
Epoch 10/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1414 - val_loss: 0.0932
Epoch 11/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1376 - val_loss: 0.0914
Epoch 12/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1346 - val_loss: 0.0926
Epoch 13/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1317 - val_loss: 0.0914
Epoch 14/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1295 - val_loss: 0.0919
Epoch 15/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1273 - val_loss: 0.0924
Epoch 16/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1249 - val_loss: 0.0900
Epoch 17/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1236 - val_loss: 0.0895
Epoch 18/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1220 - val_loss: 0.0903
Epoch 19/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1205 - val_loss: 0.0917
Epoch 20/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1194 - val_loss: 0.0909
Epoch 21/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1180 - val_loss: 0.0884
Epoch 22/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1170 - val_loss: 0.0885
Epoch 23/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1163 - val_loss: 0.0920
Epoch 24/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1152 - val_loss: 0.0865
Epoch 25/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1147 - val_loss: 0.0887
Epoch 26/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1137 - val_loss: 0.0874
Epoch 27/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1129 - val_loss: 0.0900
Epoch 28/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1123 - val_loss: 0.0857
Epoch 29/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1120 - val_loss: 0.0888
Epoch 30/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1112 - val_loss: 0.0878
Epoch 31/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1106 - val_loss: 0.0876
Epoch 32/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1102 - val_loss: 0.0860
Epoch 33/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1094 - val_loss: 0.0875
Epoch 34/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1091 - val_loss: 0.0872
Epoch 35/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1082 - val_loss: 0.0868
Epoch 36/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1079 - val_loss: 0.0866
Epoch 37/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1071 - val_loss: 0.0878
Epoch 38/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1068 - val_loss: 0.0863
Val_yp Shape is 
(75706, 24)
Results === Test == Validation ===== 96
=============================
step 1    -  0.988    0.987
step 2    -  0.980    0.979
step 3    -  0.971    0.969
step 4    -  0.960    0.958
step 5    -  0.950    0.948
step 6    -  0.940    0.938
step 7    -  0.930    0.929
step 8    -  0.920    0.920
step 9    -  0.912    0.912
step 10   -  0.905    0.905
step 11   -  0.897    0.898
step 12   -  0.890    0.891
step 13   -  0.883    0.885
step 14   -  0.877    0.880
step 15   -  0.871    0.874
step 16   -  0.866    0.869
step 17   -  0.860    0.864
step 18   -  0.854    0.858
step 19   -  0.847    0.852
step 20   -  0.841    0.847
step 21   -  0.837    0.843
step 22   -  0.831    0.837
step 23   -  0.826    0.833
step 24   -  0.821    0.829
=============================
Summary   -  21.456    21.507
V_y Shape is 
(75706, 1)
=========== SHAPE INPUT-OUTPUT MATRIXES =====================
shape x_training : (227177, 679)  y_training: :  (227177, 24)
shape x_test     :  (75705, 679)  y_test      :  (75705, 24)
shape x_val      :  (75706, 679)  y_val       :  (75706, 24)
=============================================================
(679,)
Model: "MLP"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_2 (InputLayer)        [(None, 679)]             0         
                                                                 
 dense_4 (Dense)             (None, 1024)              696320    
                                                                 
 elu_1 (ELU)                 (None, 1024)              0         
                                                                 
 dropout_3 (Dropout)         (None, 1024)              0         
                                                                 
 dense_5 (Dense)             (None, 512)               524800    
                                                                 
 dropout_4 (Dropout)         (None, 512)               0         
                                                                 
 dense_6 (Dense)             (None, 512)               262656    
                                                                 
 dropout_5 (Dropout)         (None, 512)               0         
                                                                 
 dense_7 (Dense)             (None, 24)                12312     
                                                                 
=================================================================
Total params: 1,496,088
Trainable params: 1,496,088
Non-trainable params: 0
_________________________________________________________________
Epoch 1/200
222/222 [==============================] - 2s 9ms/step - loss: 0.5024 - val_loss: 0.1144
Epoch 2/200
222/222 [==============================] - 1s 6ms/step - loss: 0.2537 - val_loss: 0.1057
Epoch 3/200
222/222 [==============================] - 1s 6ms/step - loss: 0.2099 - val_loss: 0.1014
Epoch 4/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1883 - val_loss: 0.0977
Epoch 5/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1745 - val_loss: 0.0970
Epoch 6/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1646 - val_loss: 0.0988
Epoch 7/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1571 - val_loss: 0.0954
Epoch 8/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1509 - val_loss: 0.0964
Epoch 9/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1463 - val_loss: 0.0956
Epoch 10/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1419 - val_loss: 0.0907
Epoch 11/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1379 - val_loss: 0.0920
Epoch 12/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1347 - val_loss: 0.0919
Epoch 13/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1317 - val_loss: 0.0928
Epoch 14/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1296 - val_loss: 0.0913
Epoch 15/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1274 - val_loss: 0.0936
Epoch 16/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1253 - val_loss: 0.0922
Epoch 17/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1236 - val_loss: 0.0915
Epoch 18/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1221 - val_loss: 0.0882
Epoch 19/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1205 - val_loss: 0.0892
Epoch 20/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1191 - val_loss: 0.0874
Epoch 21/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1180 - val_loss: 0.0876
Epoch 22/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1170 - val_loss: 0.0893
Epoch 23/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1162 - val_loss: 0.0872
Epoch 24/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1154 - val_loss: 0.0868
Epoch 25/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1144 - val_loss: 0.0882
Epoch 26/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1137 - val_loss: 0.0894
Epoch 27/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1131 - val_loss: 0.0898
Epoch 28/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1123 - val_loss: 0.0898
Epoch 29/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1118 - val_loss: 0.0880
Epoch 30/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1111 - val_loss: 0.0869
Epoch 31/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1105 - val_loss: 0.0868
Epoch 32/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1097 - val_loss: 0.0869
Epoch 33/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1093 - val_loss: 0.0873
Epoch 34/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1087 - val_loss: 0.0876
Epoch 35/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1082 - val_loss: 0.0889
Epoch 36/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1076 - val_loss: 0.0883
Epoch 37/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1068 - val_loss: 0.0854
Epoch 38/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1065 - val_loss: 0.0876
Epoch 39/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1059 - val_loss: 0.0855
Epoch 40/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1054 - val_loss: 0.0883
Epoch 41/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1050 - val_loss: 0.0863
Epoch 42/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1042 - val_loss: 0.0841
Epoch 43/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1037 - val_loss: 0.0865
Epoch 44/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1033 - val_loss: 0.0866
Epoch 45/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1027 - val_loss: 0.0875
Epoch 46/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1022 - val_loss: 0.0849
Epoch 47/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1016 - val_loss: 0.0855
Epoch 48/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1013 - val_loss: 0.0861
Epoch 49/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1007 - val_loss: 0.0848
Epoch 50/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1003 - val_loss: 0.0873
Epoch 51/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0995 - val_loss: 0.0848
Epoch 52/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0992 - val_loss: 0.0844
Val_yp Shape is 
(75706, 24)
Results === Test == Validation ===== 97
=============================
step 1    -  0.990    0.989
step 2    -  0.983    0.981
step 3    -  0.974    0.972
step 4    -  0.964    0.962
step 5    -  0.954    0.952
step 6    -  0.945    0.943
step 7    -  0.935    0.934
step 8    -  0.926    0.924
step 9    -  0.917    0.917
step 10   -  0.909    0.909
step 11   -  0.902    0.902
step 12   -  0.895    0.896
step 13   -  0.888    0.890
step 14   -  0.882    0.883
step 15   -  0.876    0.878
step 16   -  0.870    0.873
step 17   -  0.865    0.867
step 18   -  0.860    0.863
step 19   -  0.855    0.859
step 20   -  0.851    0.856
step 21   -  0.847    0.852
step 22   -  0.843    0.850
step 23   -  0.838    0.846
step 24   -  0.835    0.843
=============================
Summary   -  21.605    21.641
V_y Shape is 
(75706, 1)
=========== SHAPE INPUT-OUTPUT MATRIXES =====================
shape x_training : (227176, 686)  y_training: :  (227176, 24)
shape x_test     :  (75705, 686)  y_test      :  (75705, 24)
shape x_val      :  (75705, 686)  y_val       :  (75705, 24)
=============================================================
(686,)
Model: "MLP"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_3 (InputLayer)        [(None, 686)]             0         
                                                                 
 dense_8 (Dense)             (None, 1024)              703488    
                                                                 
 elu_2 (ELU)                 (None, 1024)              0         
                                                                 
 dropout_6 (Dropout)         (None, 1024)              0         
                                                                 
 dense_9 (Dense)             (None, 512)               524800    
                                                                 
 dropout_7 (Dropout)         (None, 512)               0         
                                                                 
 dense_10 (Dense)            (None, 512)               262656    
                                                                 
 dropout_8 (Dropout)         (None, 512)               0         
                                                                 
 dense_11 (Dense)            (None, 24)                12312     
                                                                 
=================================================================
Total params: 1,503,256
Trainable params: 1,503,256
Non-trainable params: 0
_________________________________________________________________
Epoch 1/200
222/222 [==============================] - 2s 8ms/step - loss: 0.5023 - val_loss: 0.1150
Epoch 2/200
222/222 [==============================] - 1s 6ms/step - loss: 0.2554 - val_loss: 0.1028
Epoch 3/200
222/222 [==============================] - 1s 7ms/step - loss: 0.2108 - val_loss: 0.1011
Epoch 4/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1889 - val_loss: 0.0996
Epoch 5/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1751 - val_loss: 0.0971
Epoch 6/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1644 - val_loss: 0.0952
Epoch 7/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1571 - val_loss: 0.0959
Epoch 8/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1508 - val_loss: 0.0937
Epoch 9/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1458 - val_loss: 0.0926
Epoch 10/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1415 - val_loss: 0.0929
Epoch 11/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1377 - val_loss: 0.0919
Epoch 12/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1346 - val_loss: 0.0930
Epoch 13/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1320 - val_loss: 0.0914
Epoch 14/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1292 - val_loss: 0.0903
Epoch 15/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1273 - val_loss: 0.0890
Epoch 16/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1251 - val_loss: 0.0894
Epoch 17/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1232 - val_loss: 0.0877
Epoch 18/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1218 - val_loss: 0.0894
Epoch 19/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1203 - val_loss: 0.0874
Epoch 20/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1190 - val_loss: 0.0911
Epoch 21/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1178 - val_loss: 0.0879
Epoch 22/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1169 - val_loss: 0.0896
Epoch 23/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1161 - val_loss: 0.0898
Epoch 24/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1150 - val_loss: 0.0887
Epoch 25/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1143 - val_loss: 0.0868
Epoch 26/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1135 - val_loss: 0.0895
Epoch 27/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1127 - val_loss: 0.0883
Epoch 28/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1120 - val_loss: 0.0877
Epoch 29/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1115 - val_loss: 0.0884
Epoch 30/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1107 - val_loss: 0.0878
Epoch 31/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1101 - val_loss: 0.0875
Epoch 32/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1094 - val_loss: 0.0875
Epoch 33/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1091 - val_loss: 0.0885
Epoch 34/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1084 - val_loss: 0.0882
Epoch 35/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1080 - val_loss: 0.0858
Epoch 36/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1072 - val_loss: 0.0857
Epoch 37/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1067 - val_loss: 0.0871
Epoch 38/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1062 - val_loss: 0.0878
Epoch 39/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1056 - val_loss: 0.0849
Epoch 40/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1050 - val_loss: 0.0878
Epoch 41/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1046 - val_loss: 0.0851
Epoch 42/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1043 - val_loss: 0.0853
Epoch 43/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1035 - val_loss: 0.0879
Epoch 44/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1032 - val_loss: 0.0865
Epoch 45/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1027 - val_loss: 0.0863
Epoch 46/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1019 - val_loss: 0.0875
Epoch 47/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1014 - val_loss: 0.0856
Epoch 48/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1009 - val_loss: 0.0853
Epoch 49/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1007 - val_loss: 0.0874
Val_yp Shape is 
(75705, 24)
Results === Test == Validation ===== 98
=============================
step 1    -  0.989    0.988
step 2    -  0.982    0.980
step 3    -  0.973    0.971
step 4    -  0.963    0.961
step 5    -  0.953    0.950
step 6    -  0.943    0.941
step 7    -  0.933    0.931
step 8    -  0.923    0.922
step 9    -  0.914    0.913
step 10   -  0.905    0.904
step 11   -  0.896    0.896
step 12   -  0.887    0.887
step 13   -  0.879    0.880
step 14   -  0.873    0.874
step 15   -  0.867    0.868
step 16   -  0.860    0.862
step 17   -  0.855    0.857
step 18   -  0.850    0.853
step 19   -  0.845    0.849
step 20   -  0.841    0.845
step 21   -  0.836    0.841
step 22   -  0.832    0.838
step 23   -  0.827    0.834
step 24   -  0.824    0.831
=============================
Summary   -  21.449    21.475
V_y Shape is 
(75705, 1)
=========== SHAPE INPUT-OUTPUT MATRIXES =====================
shape x_training : (227175, 693)  y_training: :  (227175, 24)
shape x_test     :  (75704, 693)  y_test      :  (75704, 24)
shape x_val      :  (75705, 693)  y_val       :  (75705, 24)
=============================================================
(693,)
Model: "MLP"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_4 (InputLayer)        [(None, 693)]             0         
                                                                 
 dense_12 (Dense)            (None, 1024)              710656    
                                                                 
 elu_3 (ELU)                 (None, 1024)              0         
                                                                 
 dropout_9 (Dropout)         (None, 1024)              0         
                                                                 
 dense_13 (Dense)            (None, 512)               524800    
                                                                 
 dropout_10 (Dropout)        (None, 512)               0         
                                                                 
 dense_14 (Dense)            (None, 512)               262656    
                                                                 
 dropout_11 (Dropout)        (None, 512)               0         
                                                                 
 dense_15 (Dense)            (None, 24)                12312     
                                                                 
=================================================================
Total params: 1,510,424
Trainable params: 1,510,424
Non-trainable params: 0
_________________________________________________________________
Epoch 1/200
222/222 [==============================] - 2s 7ms/step - loss: 0.5078 - val_loss: 0.1159
Epoch 2/200
222/222 [==============================] - 1s 6ms/step - loss: 0.2560 - val_loss: 0.1067
Epoch 3/200
222/222 [==============================] - 1s 6ms/step - loss: 0.2111 - val_loss: 0.1018
Epoch 4/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1894 - val_loss: 0.0999
Epoch 5/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1756 - val_loss: 0.0990
Epoch 6/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1655 - val_loss: 0.0955
Epoch 7/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1575 - val_loss: 0.0953
Epoch 8/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1513 - val_loss: 0.0930
Epoch 9/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1463 - val_loss: 0.0971
Epoch 10/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1419 - val_loss: 0.0925
Epoch 11/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1381 - val_loss: 0.0926
Epoch 12/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1347 - val_loss: 0.0908
Epoch 13/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1321 - val_loss: 0.0917
Epoch 14/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1295 - val_loss: 0.0910
Epoch 15/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1274 - val_loss: 0.0923
Epoch 16/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1252 - val_loss: 0.0903
Epoch 17/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1235 - val_loss: 0.0902
Epoch 18/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1219 - val_loss: 0.0918
Epoch 19/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1203 - val_loss: 0.0891
Epoch 20/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1191 - val_loss: 0.0903
Epoch 21/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1180 - val_loss: 0.0897
Epoch 22/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1168 - val_loss: 0.0901
Epoch 23/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1160 - val_loss: 0.0885
Epoch 24/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1149 - val_loss: 0.0873
Epoch 25/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1141 - val_loss: 0.0871
Epoch 26/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1135 - val_loss: 0.0879
Epoch 27/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1125 - val_loss: 0.0892
Epoch 28/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1120 - val_loss: 0.0874
Epoch 29/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1114 - val_loss: 0.0892
Epoch 30/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1107 - val_loss: 0.0872
Epoch 31/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1103 - val_loss: 0.0865
Epoch 32/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1094 - val_loss: 0.0884
Epoch 33/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1088 - val_loss: 0.0880
Epoch 34/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1081 - val_loss: 0.0879
Epoch 35/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1078 - val_loss: 0.0883
Epoch 36/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1073 - val_loss: 0.0856
Epoch 37/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1068 - val_loss: 0.0862
Epoch 38/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1061 - val_loss: 0.0879
Epoch 39/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1058 - val_loss: 0.0862
Epoch 40/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1049 - val_loss: 0.0877
Epoch 41/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1044 - val_loss: 0.0886
Epoch 42/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1040 - val_loss: 0.0859
Epoch 43/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1034 - val_loss: 0.0864
Epoch 44/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1028 - val_loss: 0.0854
Epoch 45/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1023 - val_loss: 0.0844
Epoch 46/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1018 - val_loss: 0.0871
Epoch 47/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1012 - val_loss: 0.0854
Epoch 48/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1006 - val_loss: 0.0859
Epoch 49/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1004 - val_loss: 0.0853
Epoch 50/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0998 - val_loss: 0.0846
Epoch 51/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0991 - val_loss: 0.0855
Epoch 52/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0989 - val_loss: 0.0853
Epoch 53/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0984 - val_loss: 0.0854
Epoch 54/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0979 - val_loss: 0.0843
Epoch 55/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0974 - val_loss: 0.0874
Epoch 56/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0970 - val_loss: 0.0858
Epoch 57/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0965 - val_loss: 0.0875
Epoch 58/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0962 - val_loss: 0.0844
Epoch 59/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0956 - val_loss: 0.0846
Epoch 60/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0953 - val_loss: 0.0850
Epoch 61/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0950 - val_loss: 0.0856
Epoch 62/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0944 - val_loss: 0.0849
Epoch 63/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0941 - val_loss: 0.0854
Epoch 64/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0938 - val_loss: 0.0840
Epoch 65/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0935 - val_loss: 0.0855
Epoch 66/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0928 - val_loss: 0.0851
Epoch 67/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0924 - val_loss: 0.0870
Epoch 68/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0923 - val_loss: 0.0845
Epoch 69/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0920 - val_loss: 0.0857
Epoch 70/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0913 - val_loss: 0.0864
Epoch 71/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0914 - val_loss: 0.0839
Epoch 72/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0909 - val_loss: 0.0831
Epoch 73/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0906 - val_loss: 0.0842
Epoch 74/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0901 - val_loss: 0.0832
Epoch 75/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0899 - val_loss: 0.0856
Epoch 76/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0897 - val_loss: 0.0838
Epoch 77/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0893 - val_loss: 0.0848
Epoch 78/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0888 - val_loss: 0.0841
Epoch 79/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0886 - val_loss: 0.0839
Epoch 80/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0884 - val_loss: 0.0841
Epoch 81/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0882 - val_loss: 0.0855
Epoch 82/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0878 - val_loss: 0.0851
Val_yp Shape is 
(75705, 24)
Results === Test == Validation ===== 99
=============================
step 1    -  0.990    0.989
step 2    -  0.983    0.982
step 3    -  0.975    0.973
step 4    -  0.966    0.964
step 5    -  0.956    0.954
step 6    -  0.947    0.945
step 7    -  0.937    0.937
step 8    -  0.928    0.928
step 9    -  0.920    0.920
step 10   -  0.911    0.912
step 11   -  0.903    0.905
step 12   -  0.896    0.898
step 13   -  0.889    0.891
step 14   -  0.882    0.885
step 15   -  0.876    0.879
step 16   -  0.870    0.874
step 17   -  0.864    0.869
step 18   -  0.859    0.864
step 19   -  0.855    0.861
step 20   -  0.850    0.857
step 21   -  0.846    0.853
step 22   -  0.841    0.849
step 23   -  0.835    0.845
step 24   -  0.830    0.841
=============================
Summary   -  21.608    21.676
V_y Shape is 
(75705, 1)
=========== SHAPE INPUT-OUTPUT MATRIXES =====================
shape x_training : (227174, 700)  y_training: :  (227174, 24)
shape x_test     :  (75704, 700)  y_test      :  (75704, 24)
shape x_val      :  (75704, 700)  y_val       :  (75704, 24)
=============================================================
(700,)
Model: "MLP"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_5 (InputLayer)        [(None, 700)]             0         
                                                                 
 dense_16 (Dense)            (None, 1024)              717824    
                                                                 
 elu_4 (ELU)                 (None, 1024)              0         
                                                                 
 dropout_12 (Dropout)        (None, 1024)              0         
                                                                 
 dense_17 (Dense)            (None, 512)               524800    
                                                                 
 dropout_13 (Dropout)        (None, 512)               0         
                                                                 
 dense_18 (Dense)            (None, 512)               262656    
                                                                 
 dropout_14 (Dropout)        (None, 512)               0         
                                                                 
 dense_19 (Dense)            (None, 24)                12312     
                                                                 
=================================================================
Total params: 1,517,592
Trainable params: 1,517,592
Non-trainable params: 0
_________________________________________________________________
Epoch 1/200
222/222 [==============================] - 2s 7ms/step - loss: 0.5142 - val_loss: 0.1134
Epoch 2/200
222/222 [==============================] - 1s 6ms/step - loss: 0.2552 - val_loss: 0.1047
Epoch 3/200
222/222 [==============================] - 1s 6ms/step - loss: 0.2113 - val_loss: 0.1016
Epoch 4/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1894 - val_loss: 0.0967
Epoch 5/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1754 - val_loss: 0.0968
Epoch 6/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1655 - val_loss: 0.0955
Epoch 7/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1576 - val_loss: 0.0988
Epoch 8/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1516 - val_loss: 0.0936
Epoch 9/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1462 - val_loss: 0.0928
Epoch 10/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1419 - val_loss: 0.0948
Epoch 11/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1381 - val_loss: 0.0920
Epoch 12/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1351 - val_loss: 0.0939
Epoch 13/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1322 - val_loss: 0.0926
Epoch 14/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1293 - val_loss: 0.0897
Epoch 15/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1273 - val_loss: 0.0929
Epoch 16/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1254 - val_loss: 0.0895
Epoch 17/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1238 - val_loss: 0.0885
Epoch 18/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1219 - val_loss: 0.0889
Epoch 19/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1206 - val_loss: 0.0889
Epoch 20/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1192 - val_loss: 0.0886
Epoch 21/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1181 - val_loss: 0.0902
Epoch 22/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1166 - val_loss: 0.0889
Epoch 23/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1157 - val_loss: 0.0889
Epoch 24/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1151 - val_loss: 0.0878
Epoch 25/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1138 - val_loss: 0.0880
Epoch 26/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1133 - val_loss: 0.0880
Epoch 27/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1125 - val_loss: 0.0867
Epoch 28/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1118 - val_loss: 0.0881
Epoch 29/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1113 - val_loss: 0.0879
Epoch 30/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1108 - val_loss: 0.0875
Epoch 31/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1100 - val_loss: 0.0866
Epoch 32/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1095 - val_loss: 0.0892
Epoch 33/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1089 - val_loss: 0.0867
Epoch 34/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1082 - val_loss: 0.0882
Epoch 35/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1075 - val_loss: 0.0883
Epoch 36/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1071 - val_loss: 0.0878
Epoch 37/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1065 - val_loss: 0.0884
Epoch 38/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1060 - val_loss: 0.0879
Epoch 39/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1055 - val_loss: 0.0863
Epoch 40/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1050 - val_loss: 0.0862
Epoch 41/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1045 - val_loss: 0.0873
Epoch 42/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1038 - val_loss: 0.0876
Epoch 43/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1033 - val_loss: 0.0875
Epoch 44/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1030 - val_loss: 0.0848
Epoch 45/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1026 - val_loss: 0.0881
Epoch 46/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1018 - val_loss: 0.0856
Epoch 47/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1014 - val_loss: 0.0875
Epoch 48/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1010 - val_loss: 0.0842
Epoch 49/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1005 - val_loss: 0.0853
Epoch 50/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0998 - val_loss: 0.0861
Epoch 51/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0996 - val_loss: 0.0850
Epoch 52/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0989 - val_loss: 0.0869
Epoch 53/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0983 - val_loss: 0.0845
Epoch 54/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0978 - val_loss: 0.0853
Epoch 55/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0975 - val_loss: 0.0861
Epoch 56/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0971 - val_loss: 0.0848
Epoch 57/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0966 - val_loss: 0.0863
Epoch 58/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0961 - val_loss: 0.0837
Epoch 59/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0956 - val_loss: 0.0838
Epoch 60/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0953 - val_loss: 0.0840
Epoch 61/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0949 - val_loss: 0.0847
Epoch 62/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0943 - val_loss: 0.0841
Epoch 63/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0939 - val_loss: 0.0849
Epoch 64/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0937 - val_loss: 0.0840
Epoch 65/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0933 - val_loss: 0.0854
Epoch 66/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0932 - val_loss: 0.0843
Epoch 67/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0927 - val_loss: 0.0841
Epoch 68/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0922 - val_loss: 0.0848
Val_yp Shape is 
(75704, 24)
Results === Test == Validation ===== 100
=============================
step 1    -  0.990    0.989
step 2    -  0.983    0.982
step 3    -  0.975    0.973
step 4    -  0.965    0.963
step 5    -  0.956    0.953
step 6    -  0.946    0.944
step 7    -  0.935    0.934
step 8    -  0.926    0.925
step 9    -  0.916    0.915
step 10   -  0.906    0.906
step 11   -  0.896    0.896
step 12   -  0.886    0.887
step 13   -  0.876    0.878
step 14   -  0.867    0.869
step 15   -  0.862    0.864
step 16   -  0.857    0.860
step 17   -  0.853    0.856
step 18   -  0.849    0.853
step 19   -  0.844    0.849
step 20   -  0.840    0.845
step 21   -  0.837    0.843
step 22   -  0.833    0.840
step 23   -  0.828    0.836
step 24   -  0.821    0.830
=============================
Summary   -  21.445    21.489
V_y Shape is 
(75704, 1)
=========== SHAPE INPUT-OUTPUT MATRIXES =====================
shape x_training : (227173, 707)  y_training: :  (227173, 24)
shape x_test     :  (75703, 707)  y_test      :  (75703, 24)
shape x_val      :  (75704, 707)  y_val       :  (75704, 24)
=============================================================
(707,)
Model: "MLP"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_6 (InputLayer)        [(None, 707)]             0         
                                                                 
 dense_20 (Dense)            (None, 1024)              724992    
                                                                 
 elu_5 (ELU)                 (None, 1024)              0         
                                                                 
 dropout_15 (Dropout)        (None, 1024)              0         
                                                                 
 dense_21 (Dense)            (None, 512)               524800    
                                                                 
 dropout_16 (Dropout)        (None, 512)               0         
                                                                 
 dense_22 (Dense)            (None, 512)               262656    
                                                                 
 dropout_17 (Dropout)        (None, 512)               0         
                                                                 
 dense_23 (Dense)            (None, 24)                12312     
                                                                 
=================================================================
Total params: 1,524,760
Trainable params: 1,524,760
Non-trainable params: 0
_________________________________________________________________
Epoch 1/200
222/222 [==============================] - 2s 8ms/step - loss: 0.5049 - val_loss: 0.1165
Epoch 2/200
222/222 [==============================] - 1s 6ms/step - loss: 0.2559 - val_loss: 0.1033
Epoch 3/200
222/222 [==============================] - 1s 6ms/step - loss: 0.2115 - val_loss: 0.1017
Epoch 4/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1897 - val_loss: 0.1014
Epoch 5/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1753 - val_loss: 0.0956
Epoch 6/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1654 - val_loss: 0.0954
Epoch 7/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1577 - val_loss: 0.0942
Epoch 8/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1510 - val_loss: 0.0965
Epoch 9/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1459 - val_loss: 0.0928
Epoch 10/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1414 - val_loss: 0.0912
Epoch 11/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1378 - val_loss: 0.0923
Epoch 12/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1345 - val_loss: 0.0967
Epoch 13/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1315 - val_loss: 0.0903
Epoch 14/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1292 - val_loss: 0.0929
Epoch 15/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1270 - val_loss: 0.0929
Epoch 16/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1249 - val_loss: 0.0914
Epoch 17/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1232 - val_loss: 0.0896
Epoch 18/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1215 - val_loss: 0.0879
Epoch 19/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1201 - val_loss: 0.0879
Epoch 20/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1187 - val_loss: 0.0904
Epoch 21/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1177 - val_loss: 0.0919
Epoch 22/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1165 - val_loss: 0.0896
Epoch 23/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1158 - val_loss: 0.0888
Epoch 24/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1147 - val_loss: 0.0899
Epoch 25/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1139 - val_loss: 0.0893
Epoch 26/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1132 - val_loss: 0.0874
Epoch 27/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1122 - val_loss: 0.0889
Epoch 28/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1116 - val_loss: 0.0890
Epoch 29/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1111 - val_loss: 0.0884
Epoch 30/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1107 - val_loss: 0.0871
Epoch 31/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1097 - val_loss: 0.0867
Epoch 32/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1091 - val_loss: 0.0877
Epoch 33/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1087 - val_loss: 0.0873
Epoch 34/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1079 - val_loss: 0.0873
Epoch 35/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1073 - val_loss: 0.0862
Epoch 36/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1069 - val_loss: 0.0873
Epoch 37/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1061 - val_loss: 0.0868
Epoch 38/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1058 - val_loss: 0.0869
Epoch 39/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1051 - val_loss: 0.0868
Epoch 40/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1048 - val_loss: 0.0860
Epoch 41/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1039 - val_loss: 0.0900
Epoch 42/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1036 - val_loss: 0.0855
Epoch 43/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1031 - val_loss: 0.0871
Epoch 44/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1026 - val_loss: 0.0864
Epoch 45/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1021 - val_loss: 0.0865
Epoch 46/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1017 - val_loss: 0.0855
Epoch 47/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1009 - val_loss: 0.0871
Epoch 48/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1004 - val_loss: 0.0869
Epoch 49/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1001 - val_loss: 0.0866
Epoch 50/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0996 - val_loss: 0.0839
Epoch 51/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0990 - val_loss: 0.0863
Epoch 52/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0986 - val_loss: 0.0843
Epoch 53/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0980 - val_loss: 0.0844
Epoch 54/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0975 - val_loss: 0.0855
Epoch 55/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0970 - val_loss: 0.0850
Epoch 56/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0966 - val_loss: 0.0856
Epoch 57/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0962 - val_loss: 0.0878
Epoch 58/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0958 - val_loss: 0.0870
Epoch 59/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0954 - val_loss: 0.0855
Epoch 60/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0950 - val_loss: 0.0866
Val_yp Shape is 
(75704, 24)
Results === Test == Validation ===== 101
=============================
step 1    -  0.989    0.988
step 2    -  0.983    0.981
step 3    -  0.974    0.973
step 4    -  0.966    0.964
step 5    -  0.957    0.955
step 6    -  0.947    0.945
step 7    -  0.938    0.936
step 8    -  0.929    0.928
step 9    -  0.920    0.919
step 10   -  0.912    0.912
step 11   -  0.904    0.904
step 12   -  0.896    0.897
step 13   -  0.890    0.891
step 14   -  0.883    0.885
step 15   -  0.877    0.879
step 16   -  0.872    0.874
step 17   -  0.867    0.869
step 18   -  0.861    0.864
step 19   -  0.856    0.860
step 20   -  0.851    0.855
step 21   -  0.847    0.851
step 22   -  0.844    0.849
step 23   -  0.840    0.846
step 24   -  0.836    0.843
=============================
Summary   -  21.639    21.668
V_y Shape is 
(75704, 1)
=========== SHAPE INPUT-OUTPUT MATRIXES =====================
shape x_training : (227172, 714)  y_training: :  (227172, 24)
shape x_test     :  (75703, 714)  y_test      :  (75703, 24)
shape x_val      :  (75703, 714)  y_val       :  (75703, 24)
=============================================================
(714,)
Model: "MLP"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_7 (InputLayer)        [(None, 714)]             0         
                                                                 
 dense_24 (Dense)            (None, 1024)              732160    
                                                                 
 elu_6 (ELU)                 (None, 1024)              0         
                                                                 
 dropout_18 (Dropout)        (None, 1024)              0         
                                                                 
 dense_25 (Dense)            (None, 512)               524800    
                                                                 
 dropout_19 (Dropout)        (None, 512)               0         
                                                                 
 dense_26 (Dense)            (None, 512)               262656    
                                                                 
 dropout_20 (Dropout)        (None, 512)               0         
                                                                 
 dense_27 (Dense)            (None, 24)                12312     
                                                                 
=================================================================
Total params: 1,531,928
Trainable params: 1,531,928
Non-trainable params: 0
_________________________________________________________________
Epoch 1/200
222/222 [==============================] - 2s 8ms/step - loss: 0.5059 - val_loss: 0.1157
Epoch 2/200
222/222 [==============================] - 1s 6ms/step - loss: 0.2570 - val_loss: 0.1039
Epoch 3/200
222/222 [==============================] - 1s 6ms/step - loss: 0.2117 - val_loss: 0.0996
Epoch 4/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1896 - val_loss: 0.0992
Epoch 5/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1753 - val_loss: 0.0986
Epoch 6/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1651 - val_loss: 0.0955
Epoch 7/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1575 - val_loss: 0.0983
Epoch 8/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1508 - val_loss: 0.0938
Epoch 9/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1458 - val_loss: 0.0931
Epoch 10/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1414 - val_loss: 0.0949
Epoch 11/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1377 - val_loss: 0.0954
Epoch 12/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1343 - val_loss: 0.0941
Epoch 13/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1317 - val_loss: 0.0926
Epoch 14/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1290 - val_loss: 0.0912
Epoch 15/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1268 - val_loss: 0.0895
Epoch 16/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1249 - val_loss: 0.0893
Epoch 17/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1233 - val_loss: 0.0923
Epoch 18/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1213 - val_loss: 0.0886
Epoch 19/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1201 - val_loss: 0.0911
Epoch 20/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1189 - val_loss: 0.0880
Epoch 21/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1175 - val_loss: 0.0888
Epoch 22/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1166 - val_loss: 0.0901
Epoch 23/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1156 - val_loss: 0.0876
Epoch 24/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1146 - val_loss: 0.0881
Epoch 25/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1135 - val_loss: 0.0879
Epoch 26/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1129 - val_loss: 0.0886
Epoch 27/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1121 - val_loss: 0.0888
Epoch 28/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1113 - val_loss: 0.0888
Epoch 29/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1110 - val_loss: 0.0875
Epoch 30/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1101 - val_loss: 0.0872
Epoch 31/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1095 - val_loss: 0.0869
Epoch 32/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1092 - val_loss: 0.0864
Epoch 33/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1083 - val_loss: 0.0882
Epoch 34/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1079 - val_loss: 0.0866
Epoch 35/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1072 - val_loss: 0.0883
Epoch 36/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1067 - val_loss: 0.0887
Epoch 37/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1061 - val_loss: 0.0872
Epoch 38/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1055 - val_loss: 0.0887
Epoch 39/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1048 - val_loss: 0.0871
Epoch 40/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1043 - val_loss: 0.0865
Epoch 41/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1038 - val_loss: 0.0869
Epoch 42/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1031 - val_loss: 0.0872
Val_yp Shape is 
(75703, 24)
Results === Test == Validation ===== 102
=============================
step 1    -  0.989    0.988
step 2    -  0.981    0.980
step 3    -  0.972    0.970
step 4    -  0.962    0.959
step 5    -  0.951    0.949
step 6    -  0.941    0.938
step 7    -  0.930    0.928
step 8    -  0.921    0.919
step 9    -  0.911    0.910
step 10   -  0.903    0.903
step 11   -  0.896    0.895
step 12   -  0.889    0.890
step 13   -  0.883    0.884
step 14   -  0.877    0.878
step 15   -  0.871    0.872
step 16   -  0.866    0.868
step 17   -  0.862    0.864
step 18   -  0.858    0.861
step 19   -  0.854    0.857
step 20   -  0.850    0.854
step 21   -  0.846    0.851
step 22   -  0.842    0.847
step 23   -  0.838    0.844
step 24   -  0.833    0.839
=============================
Summary   -  21.526    21.548
V_y Shape is 
(75703, 1)
=========== SHAPE INPUT-OUTPUT MATRIXES =====================
shape x_training : (227171, 721)  y_training: :  (227171, 24)
shape x_test     :  (75702, 721)  y_test      :  (75702, 24)
shape x_val      :  (75703, 721)  y_val       :  (75703, 24)
=============================================================
(721,)
Model: "MLP"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_8 (InputLayer)        [(None, 721)]             0         
                                                                 
 dense_28 (Dense)            (None, 1024)              739328    
                                                                 
 elu_7 (ELU)                 (None, 1024)              0         
                                                                 
 dropout_21 (Dropout)        (None, 1024)              0         
                                                                 
 dense_29 (Dense)            (None, 512)               524800    
                                                                 
 dropout_22 (Dropout)        (None, 512)               0         
                                                                 
 dense_30 (Dense)            (None, 512)               262656    
                                                                 
 dropout_23 (Dropout)        (None, 512)               0         
                                                                 
 dense_31 (Dense)            (None, 24)                12312     
                                                                 
=================================================================
Total params: 1,539,096
Trainable params: 1,539,096
Non-trainable params: 0
_________________________________________________________________
Epoch 1/200
222/222 [==============================] - 3s 7ms/step - loss: 0.5094 - val_loss: 0.1158
Epoch 2/200
222/222 [==============================] - 1s 6ms/step - loss: 0.2561 - val_loss: 0.1063
Epoch 3/200
222/222 [==============================] - 1s 6ms/step - loss: 0.2120 - val_loss: 0.0982
Epoch 4/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1898 - val_loss: 0.0971
Epoch 5/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1758 - val_loss: 0.0967
Epoch 6/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1656 - val_loss: 0.0942
Epoch 7/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1577 - val_loss: 0.0950
Epoch 8/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1512 - val_loss: 0.0925
Epoch 9/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1463 - val_loss: 0.0931
Epoch 10/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1418 - val_loss: 0.0930
Epoch 11/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1380 - val_loss: 0.0924
Epoch 12/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1345 - val_loss: 0.0913
Epoch 13/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1319 - val_loss: 0.0923
Epoch 14/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1296 - val_loss: 0.0915
Epoch 15/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1270 - val_loss: 0.0901
Epoch 16/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1248 - val_loss: 0.0885
Epoch 17/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1232 - val_loss: 0.0894
Epoch 18/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1214 - val_loss: 0.0889
Epoch 19/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1202 - val_loss: 0.0884
Epoch 20/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1187 - val_loss: 0.0894
Epoch 21/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1174 - val_loss: 0.0883
Epoch 22/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1165 - val_loss: 0.0910
Epoch 23/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1154 - val_loss: 0.0876
Epoch 24/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1147 - val_loss: 0.0900
Epoch 25/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1136 - val_loss: 0.0901
Epoch 26/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1128 - val_loss: 0.0886
Epoch 27/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1123 - val_loss: 0.0881
Epoch 28/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1114 - val_loss: 0.0881
Epoch 29/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1110 - val_loss: 0.0879
Epoch 30/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1102 - val_loss: 0.0871
Epoch 31/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1096 - val_loss: 0.0876
Epoch 32/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1090 - val_loss: 0.0870
Epoch 33/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1083 - val_loss: 0.0874
Epoch 34/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1078 - val_loss: 0.0866
Epoch 35/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1071 - val_loss: 0.0870
Epoch 36/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1063 - val_loss: 0.0879
Epoch 37/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1061 - val_loss: 0.0866
Epoch 38/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1053 - val_loss: 0.0876
Epoch 39/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1051 - val_loss: 0.0879
Epoch 40/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1043 - val_loss: 0.0875
Epoch 41/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1038 - val_loss: 0.0866
Epoch 42/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1034 - val_loss: 0.0859
Epoch 43/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1028 - val_loss: 0.0868
Epoch 44/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1022 - val_loss: 0.0866
Epoch 45/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1019 - val_loss: 0.0870
Epoch 46/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1009 - val_loss: 0.0854
Epoch 47/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1008 - val_loss: 0.0872
Epoch 48/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1000 - val_loss: 0.0884
Epoch 49/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0995 - val_loss: 0.0858
Epoch 50/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0988 - val_loss: 0.0865
Epoch 51/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0986 - val_loss: 0.0873
Epoch 52/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0979 - val_loss: 0.0880
Epoch 53/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0977 - val_loss: 0.0850
Epoch 54/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0970 - val_loss: 0.0873
Epoch 55/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0965 - val_loss: 0.0866
Epoch 56/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0961 - val_loss: 0.0868
Epoch 57/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0957 - val_loss: 0.0861
Epoch 58/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0951 - val_loss: 0.0880
Epoch 59/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0948 - val_loss: 0.0857
Epoch 60/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0944 - val_loss: 0.0850
Epoch 61/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0940 - val_loss: 0.0852
Epoch 62/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0937 - val_loss: 0.0869
Epoch 63/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0932 - val_loss: 0.0884
Epoch 64/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0927 - val_loss: 0.0859
Epoch 65/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0925 - val_loss: 0.0854
Epoch 66/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0921 - val_loss: 0.0850
Epoch 67/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0916 - val_loss: 0.0864
Epoch 68/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0913 - val_loss: 0.0875
Epoch 69/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0909 - val_loss: 0.0842
Epoch 70/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0907 - val_loss: 0.0851
Epoch 71/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0902 - val_loss: 0.0845
Epoch 72/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0899 - val_loss: 0.0852
Epoch 73/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0896 - val_loss: 0.0852
Epoch 74/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0892 - val_loss: 0.0853
Epoch 75/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0892 - val_loss: 0.0842
Epoch 76/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0888 - val_loss: 0.0853
Epoch 77/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0881 - val_loss: 0.0850
Epoch 78/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0879 - val_loss: 0.0849
Epoch 79/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0877 - val_loss: 0.0856
Val_yp Shape is 
(75703, 24)
Results === Test == Validation ===== 103
=============================
step 1    -  0.989    0.989
step 2    -  0.983    0.981
step 3    -  0.975    0.973
step 4    -  0.965    0.963
step 5    -  0.956    0.954
step 6    -  0.946    0.944
step 7    -  0.937    0.935
step 8    -  0.928    0.926
step 9    -  0.919    0.918
step 10   -  0.910    0.910
step 11   -  0.902    0.903
step 12   -  0.894    0.896
step 13   -  0.889    0.890
step 14   -  0.883    0.885
step 15   -  0.878    0.881
step 16   -  0.872    0.875
step 17   -  0.867    0.870
step 18   -  0.861    0.865
step 19   -  0.855    0.859
step 20   -  0.850    0.854
step 21   -  0.846    0.851
step 22   -  0.840    0.846
step 23   -  0.835    0.841
step 24   -  0.830    0.837
=============================
Summary   -  21.612    21.647
V_y Shape is 
(75703, 1)
=========== SHAPE INPUT-OUTPUT MATRIXES =====================
shape x_training : (227170, 728)  y_training: :  (227170, 24)
shape x_test     :  (75702, 728)  y_test      :  (75702, 24)
shape x_val      :  (75702, 728)  y_val       :  (75702, 24)
=============================================================
(728,)
Model: "MLP"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_9 (InputLayer)        [(None, 728)]             0         
                                                                 
 dense_32 (Dense)            (None, 1024)              746496    
                                                                 
 elu_8 (ELU)                 (None, 1024)              0         
                                                                 
 dropout_24 (Dropout)        (None, 1024)              0         
                                                                 
 dense_33 (Dense)            (None, 512)               524800    
                                                                 
 dropout_25 (Dropout)        (None, 512)               0         
                                                                 
 dense_34 (Dense)            (None, 512)               262656    
                                                                 
 dropout_26 (Dropout)        (None, 512)               0         
                                                                 
 dense_35 (Dense)            (None, 24)                12312     
                                                                 
=================================================================
Total params: 1,546,264
Trainable params: 1,546,264
Non-trainable params: 0
_________________________________________________________________
Epoch 1/200
222/222 [==============================] - 2s 7ms/step - loss: 0.5019 - val_loss: 0.1145
Epoch 2/200
222/222 [==============================] - 1s 6ms/step - loss: 0.2559 - val_loss: 0.1053
Epoch 3/200
222/222 [==============================] - 1s 6ms/step - loss: 0.2115 - val_loss: 0.1007
Epoch 4/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1897 - val_loss: 0.0999
Epoch 5/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1754 - val_loss: 0.0968
Epoch 6/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1652 - val_loss: 0.0968
Epoch 7/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1572 - val_loss: 0.0948
Epoch 8/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1509 - val_loss: 0.0924
Epoch 9/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1456 - val_loss: 0.0934
Epoch 10/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1413 - val_loss: 0.0940
Epoch 11/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1374 - val_loss: 0.0925
Epoch 12/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1341 - val_loss: 0.0901
Epoch 13/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1311 - val_loss: 0.0903
Epoch 14/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1287 - val_loss: 0.0902
Epoch 15/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1263 - val_loss: 0.0924
Epoch 16/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1244 - val_loss: 0.0896
Epoch 17/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1226 - val_loss: 0.0896
Epoch 18/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1209 - val_loss: 0.0912
Epoch 19/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1195 - val_loss: 0.0892
Epoch 20/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1183 - val_loss: 0.0898
Epoch 21/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1171 - val_loss: 0.0886
Epoch 22/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1164 - val_loss: 0.0920
Epoch 23/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1149 - val_loss: 0.0882
Epoch 24/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1140 - val_loss: 0.0878
Epoch 25/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1134 - val_loss: 0.0879
Epoch 26/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1125 - val_loss: 0.0885
Epoch 27/200
222/222 [==============================] - 2s 8ms/step - loss: 0.1118 - val_loss: 0.0873
Epoch 28/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1112 - val_loss: 0.0874
Epoch 29/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1105 - val_loss: 0.0862
Epoch 30/200
222/222 [==============================] - 2s 8ms/step - loss: 0.1099 - val_loss: 0.0879
Epoch 31/200
222/222 [==============================] - 2s 8ms/step - loss: 0.1090 - val_loss: 0.0869
Epoch 32/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1086 - val_loss: 0.0879
Epoch 33/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1080 - val_loss: 0.0883
Epoch 34/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1074 - val_loss: 0.0875
Epoch 35/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1066 - val_loss: 0.0871
Epoch 36/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1061 - val_loss: 0.0911
Epoch 37/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1058 - val_loss: 0.0875
Epoch 38/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1049 - val_loss: 0.0861
Epoch 39/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1044 - val_loss: 0.0878
Epoch 40/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1038 - val_loss: 0.0891
Epoch 41/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1031 - val_loss: 0.0860
Epoch 42/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1031 - val_loss: 0.0860
Epoch 43/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1020 - val_loss: 0.0858
Epoch 44/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1016 - val_loss: 0.0855
Epoch 45/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1010 - val_loss: 0.0862
Epoch 46/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1007 - val_loss: 0.0852
Epoch 47/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1000 - val_loss: 0.0851
Epoch 48/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0994 - val_loss: 0.0868
Epoch 49/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0991 - val_loss: 0.0855
Epoch 50/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0984 - val_loss: 0.0871
Epoch 51/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0978 - val_loss: 0.0853
Epoch 52/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0975 - val_loss: 0.0854
Epoch 53/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0969 - val_loss: 0.0870
Epoch 54/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0963 - val_loss: 0.0842
Epoch 55/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0958 - val_loss: 0.0842
Epoch 56/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0955 - val_loss: 0.0849
Epoch 57/200
222/222 [==============================] - 2s 7ms/step - loss: 0.0950 - val_loss: 0.0850
Epoch 58/200
222/222 [==============================] - 2s 7ms/step - loss: 0.0945 - val_loss: 0.0867
Epoch 59/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0942 - val_loss: 0.0846
Epoch 60/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0939 - val_loss: 0.0854
Epoch 61/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0935 - val_loss: 0.0855
Epoch 62/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0928 - val_loss: 0.0847
Epoch 63/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0924 - val_loss: 0.0860
Epoch 64/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0923 - val_loss: 0.0847
Val_yp Shape is 
(75702, 24)
Results === Test == Validation ===== 104
=============================
step 1    -  0.990    0.989
step 2    -  0.983    0.981
step 3    -  0.974    0.972
step 4    -  0.964    0.962
step 5    -  0.954    0.952
step 6    -  0.944    0.942
step 7    -  0.935    0.933
step 8    -  0.926    0.925
step 9    -  0.917    0.916
step 10   -  0.909    0.909
step 11   -  0.902    0.903
step 12   -  0.895    0.896
step 13   -  0.889    0.891
step 14   -  0.882    0.885
step 15   -  0.876    0.879
step 16   -  0.871    0.874
step 17   -  0.863    0.867
step 18   -  0.858    0.863
step 19   -  0.853    0.859
step 20   -  0.848    0.854
step 21   -  0.842    0.850
step 22   -  0.838    0.846
step 23   -  0.833    0.843
step 24   -  0.830    0.840
=============================
Summary   -  21.577    21.632
V_y Shape is 
(75702, 1)
=========== SHAPE INPUT-OUTPUT MATRIXES =====================
shape x_training : (227169, 735)  y_training: :  (227169, 24)
shape x_test     :  (75701, 735)  y_test      :  (75701, 24)
shape x_val      :  (75702, 735)  y_val       :  (75702, 24)
=============================================================
(735,)
Model: "MLP"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_10 (InputLayer)       [(None, 735)]             0         
                                                                 
 dense_36 (Dense)            (None, 1024)              753664    
                                                                 
 elu_9 (ELU)                 (None, 1024)              0         
                                                                 
 dropout_27 (Dropout)        (None, 1024)              0         
                                                                 
 dense_37 (Dense)            (None, 512)               524800    
                                                                 
 dropout_28 (Dropout)        (None, 512)               0         
                                                                 
 dense_38 (Dense)            (None, 512)               262656    
                                                                 
 dropout_29 (Dropout)        (None, 512)               0         
                                                                 
 dense_39 (Dense)            (None, 24)                12312     
                                                                 
=================================================================
Total params: 1,553,432
Trainable params: 1,553,432
Non-trainable params: 0
_________________________________________________________________
Epoch 1/200
222/222 [==============================] - 2s 7ms/step - loss: 0.5052 - val_loss: 0.1187
Epoch 2/200
222/222 [==============================] - 1s 6ms/step - loss: 0.2575 - val_loss: 0.1043
Epoch 3/200
222/222 [==============================] - 2s 7ms/step - loss: 0.2118 - val_loss: 0.1007
Epoch 4/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1902 - val_loss: 0.1014
Epoch 5/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1761 - val_loss: 0.0988
Epoch 6/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1651 - val_loss: 0.0954
Epoch 7/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1576 - val_loss: 0.0953
Epoch 8/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1512 - val_loss: 0.0931
Epoch 9/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1457 - val_loss: 0.0923
Epoch 10/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1413 - val_loss: 0.0919
Epoch 11/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1378 - val_loss: 0.0907
Epoch 12/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1342 - val_loss: 0.0923
Epoch 13/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1312 - val_loss: 0.0901
Epoch 14/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1290 - val_loss: 0.0922
Epoch 15/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1266 - val_loss: 0.0898
Epoch 16/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1245 - val_loss: 0.0901
Epoch 17/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1228 - val_loss: 0.0894
Epoch 18/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1211 - val_loss: 0.0887
Epoch 19/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1197 - val_loss: 0.0907
Epoch 20/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1181 - val_loss: 0.0882
Epoch 21/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1170 - val_loss: 0.0881
Epoch 22/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1159 - val_loss: 0.0882
Epoch 23/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1148 - val_loss: 0.0900
Epoch 24/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1140 - val_loss: 0.0878
Epoch 25/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1134 - val_loss: 0.0870
Epoch 26/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1124 - val_loss: 0.0897
Epoch 27/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1117 - val_loss: 0.0894
Epoch 28/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1110 - val_loss: 0.0873
Epoch 29/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1104 - val_loss: 0.0882
Epoch 30/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1097 - val_loss: 0.0878
Epoch 31/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1090 - val_loss: 0.0893
Epoch 32/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1085 - val_loss: 0.0894
Epoch 33/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1081 - val_loss: 0.0871
Epoch 34/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1073 - val_loss: 0.0865
Epoch 35/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1068 - val_loss: 0.0875
Epoch 36/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1059 - val_loss: 0.0876
Epoch 37/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1055 - val_loss: 0.0870
Epoch 38/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1050 - val_loss: 0.0875
Epoch 39/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1040 - val_loss: 0.0880
Epoch 40/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1038 - val_loss: 0.0868
Epoch 41/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1031 - val_loss: 0.0875
Epoch 42/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1028 - val_loss: 0.0864
Epoch 43/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1021 - val_loss: 0.0870
Epoch 44/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1016 - val_loss: 0.0858
Epoch 45/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1012 - val_loss: 0.0870
Epoch 46/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1006 - val_loss: 0.0862
Epoch 47/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1000 - val_loss: 0.0864
Epoch 48/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0991 - val_loss: 0.0847
Epoch 49/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0991 - val_loss: 0.0872
Epoch 50/200
222/222 [==============================] - 2s 7ms/step - loss: 0.0985 - val_loss: 0.0872
Epoch 51/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0978 - val_loss: 0.0849
Epoch 52/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0974 - val_loss: 0.0868
Epoch 53/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0971 - val_loss: 0.0851
Epoch 54/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0964 - val_loss: 0.0865
Epoch 55/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0959 - val_loss: 0.0856
Epoch 56/200
222/222 [==============================] - 2s 7ms/step - loss: 0.0954 - val_loss: 0.0866
Epoch 57/200
222/222 [==============================] - 2s 7ms/step - loss: 0.0948 - val_loss: 0.0859
Epoch 58/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0946 - val_loss: 0.0855
Val_yp Shape is 
(75702, 24)
Results === Test == Validation ===== 105
=============================
step 1    -  0.989    0.989
step 2    -  0.982    0.981
step 3    -  0.974    0.972
step 4    -  0.965    0.963
step 5    -  0.955    0.953
step 6    -  0.945    0.943
step 7    -  0.936    0.934
step 8    -  0.926    0.925
step 9    -  0.917    0.916
step 10   -  0.909    0.909
step 11   -  0.901    0.901
step 12   -  0.894    0.894
step 13   -  0.886    0.887
step 14   -  0.879    0.880
step 15   -  0.871    0.872
step 16   -  0.862    0.864
step 17   -  0.854    0.857
step 18   -  0.847    0.850
step 19   -  0.841    0.846
step 20   -  0.837    0.842
step 21   -  0.832    0.838
step 22   -  0.827    0.835
step 23   -  0.824    0.833
step 24   -  0.819    0.829
=============================
Summary   -  21.471    21.512
V_y Shape is 
(75702, 1)
=========== SHAPE INPUT-OUTPUT MATRIXES =====================
shape x_training : (227168, 742)  y_training: :  (227168, 24)
shape x_test     :  (75701, 742)  y_test      :  (75701, 24)
shape x_val      :  (75701, 742)  y_val       :  (75701, 24)
=============================================================
(742,)
Model: "MLP"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_11 (InputLayer)       [(None, 742)]             0         
                                                                 
 dense_40 (Dense)            (None, 1024)              760832    
                                                                 
 elu_10 (ELU)                (None, 1024)              0         
                                                                 
 dropout_30 (Dropout)        (None, 1024)              0         
                                                                 
 dense_41 (Dense)            (None, 512)               524800    
                                                                 
 dropout_31 (Dropout)        (None, 512)               0         
                                                                 
 dense_42 (Dense)            (None, 512)               262656    
                                                                 
 dropout_32 (Dropout)        (None, 512)               0         
                                                                 
 dense_43 (Dense)            (None, 24)                12312     
                                                                 
=================================================================
Total params: 1,560,600
Trainable params: 1,560,600
Non-trainable params: 0
_________________________________________________________________
Epoch 1/200
222/222 [==============================] - 2s 7ms/step - loss: 0.5240 - val_loss: 0.1171
Epoch 2/200
222/222 [==============================] - 1s 6ms/step - loss: 0.2588 - val_loss: 0.1042
Epoch 3/200
222/222 [==============================] - 1s 6ms/step - loss: 0.2132 - val_loss: 0.1002
Epoch 4/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1908 - val_loss: 0.0969
Epoch 5/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1767 - val_loss: 0.0967
Epoch 6/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1660 - val_loss: 0.0982
Epoch 7/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1580 - val_loss: 0.0942
Epoch 8/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1514 - val_loss: 0.0956
Epoch 9/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1466 - val_loss: 0.0938
Epoch 10/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1419 - val_loss: 0.0925
Epoch 11/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1381 - val_loss: 0.0923
Epoch 12/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1348 - val_loss: 0.0890
Epoch 13/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1318 - val_loss: 0.0939
Epoch 14/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1291 - val_loss: 0.0902
Epoch 15/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1270 - val_loss: 0.0892
Epoch 16/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1250 - val_loss: 0.0924
Epoch 17/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1228 - val_loss: 0.0909
Epoch 18/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1211 - val_loss: 0.0908
Epoch 19/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1196 - val_loss: 0.0902
Epoch 20/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1183 - val_loss: 0.0885
Epoch 21/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1171 - val_loss: 0.0892
Epoch 22/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1162 - val_loss: 0.0902
Epoch 23/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1150 - val_loss: 0.0907
Epoch 24/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1142 - val_loss: 0.0887
Epoch 25/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1132 - val_loss: 0.0897
Epoch 26/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1124 - val_loss: 0.0900
Epoch 27/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1117 - val_loss: 0.0879
Epoch 28/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1109 - val_loss: 0.0883
Epoch 29/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1102 - val_loss: 0.0886
Epoch 30/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1095 - val_loss: 0.0886
Epoch 31/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1089 - val_loss: 0.0869
Epoch 32/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1082 - val_loss: 0.0870
Epoch 33/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1077 - val_loss: 0.0871
Epoch 34/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1070 - val_loss: 0.0885
Epoch 35/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1062 - val_loss: 0.0883
Epoch 36/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1057 - val_loss: 0.0873
Epoch 37/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1052 - val_loss: 0.0871
Epoch 38/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1047 - val_loss: 0.0881
Epoch 39/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1039 - val_loss: 0.0886
Epoch 40/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1035 - val_loss: 0.0871
Epoch 41/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1031 - val_loss: 0.0880
Val_yp Shape is 
(75701, 24)
Results === Test == Validation ===== 106
=============================
step 1    -  0.989    0.988
step 2    -  0.981    0.980
step 3    -  0.973    0.971
step 4    -  0.963    0.961
step 5    -  0.953    0.951
step 6    -  0.943    0.941
step 7    -  0.933    0.931
step 8    -  0.923    0.921
step 9    -  0.914    0.913
step 10   -  0.906    0.905
step 11   -  0.898    0.897
step 12   -  0.891    0.891
step 13   -  0.885    0.885
step 14   -  0.879    0.879
step 15   -  0.874    0.875
step 16   -  0.870    0.870
step 17   -  0.864    0.866
step 18   -  0.859    0.861
step 19   -  0.854    0.857
step 20   -  0.849    0.852
step 21   -  0.844    0.848
step 22   -  0.839    0.844
step 23   -  0.834    0.839
step 24   -  0.829    0.834
=============================
Summary   -  21.548    21.559
V_y Shape is 
(75701, 1)
=========== SHAPE INPUT-OUTPUT MATRIXES =====================
shape x_training : (227167, 749)  y_training: :  (227167, 24)
shape x_test     :  (75700, 749)  y_test      :  (75700, 24)
shape x_val      :  (75701, 749)  y_val       :  (75701, 24)
=============================================================
(749,)
Model: "MLP"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_12 (InputLayer)       [(None, 749)]             0         
                                                                 
 dense_44 (Dense)            (None, 1024)              768000    
                                                                 
 elu_11 (ELU)                (None, 1024)              0         
                                                                 
 dropout_33 (Dropout)        (None, 1024)              0         
                                                                 
 dense_45 (Dense)            (None, 512)               524800    
                                                                 
 dropout_34 (Dropout)        (None, 512)               0         
                                                                 
 dense_46 (Dense)            (None, 512)               262656    
                                                                 
 dropout_35 (Dropout)        (None, 512)               0         
                                                                 
 dense_47 (Dense)            (None, 24)                12312     
                                                                 
=================================================================
Total params: 1,567,768
Trainable params: 1,567,768
Non-trainable params: 0
_________________________________________________________________
Epoch 1/200
222/222 [==============================] - 2s 9ms/step - loss: 0.5073 - val_loss: 0.1169
Epoch 2/200
222/222 [==============================] - 1s 7ms/step - loss: 0.2570 - val_loss: 0.1064
Epoch 3/200
222/222 [==============================] - 1s 6ms/step - loss: 0.2119 - val_loss: 0.1017
Epoch 4/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1900 - val_loss: 0.0991
Epoch 5/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1756 - val_loss: 0.0986
Epoch 6/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1655 - val_loss: 0.0966
Epoch 7/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1578 - val_loss: 0.0949
Epoch 8/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1510 - val_loss: 0.0931
Epoch 9/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1457 - val_loss: 0.0960
Epoch 10/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1412 - val_loss: 0.0941
Epoch 11/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1373 - val_loss: 0.0931
Epoch 12/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1340 - val_loss: 0.0913
Epoch 13/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1309 - val_loss: 0.0899
Epoch 14/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1287 - val_loss: 0.0907
Epoch 15/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1262 - val_loss: 0.0902
Epoch 16/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1242 - val_loss: 0.0910
Epoch 17/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1225 - val_loss: 0.0896
Epoch 18/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1206 - val_loss: 0.0905
Epoch 19/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1193 - val_loss: 0.0894
Epoch 20/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1179 - val_loss: 0.0910
Epoch 21/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1168 - val_loss: 0.0885
Epoch 22/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1156 - val_loss: 0.0931
Epoch 23/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1147 - val_loss: 0.0894
Epoch 24/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1138 - val_loss: 0.0883
Epoch 25/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1128 - val_loss: 0.0890
Epoch 26/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1121 - val_loss: 0.0887
Epoch 27/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1112 - val_loss: 0.0880
Epoch 28/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1104 - val_loss: 0.0899
Epoch 29/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1099 - val_loss: 0.0885
Epoch 30/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1095 - val_loss: 0.0886
Epoch 31/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1085 - val_loss: 0.0871
Epoch 32/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1081 - val_loss: 0.0875
Epoch 33/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1073 - val_loss: 0.0861
Epoch 34/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1067 - val_loss: 0.0868
Epoch 35/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1062 - val_loss: 0.0868
Epoch 36/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1056 - val_loss: 0.0862
Epoch 37/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1053 - val_loss: 0.0860
Epoch 38/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1045 - val_loss: 0.0879
Epoch 39/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1037 - val_loss: 0.0879
Epoch 40/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1034 - val_loss: 0.0863
Epoch 41/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1026 - val_loss: 0.0878
Epoch 42/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1022 - val_loss: 0.0863
Epoch 43/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1016 - val_loss: 0.0860
Epoch 44/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1009 - val_loss: 0.0866
Epoch 45/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1008 - val_loss: 0.0863
Epoch 46/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1000 - val_loss: 0.0860
Epoch 47/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0993 - val_loss: 0.0869
Epoch 48/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0988 - val_loss: 0.0868
Epoch 49/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0985 - val_loss: 0.0869
Epoch 50/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0979 - val_loss: 0.0866
Epoch 51/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0973 - val_loss: 0.0877
Epoch 52/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0968 - val_loss: 0.0882
Epoch 53/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0965 - val_loss: 0.0851
Epoch 54/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0960 - val_loss: 0.0865
Epoch 55/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0954 - val_loss: 0.0862
Epoch 56/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0947 - val_loss: 0.0855
Epoch 57/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0945 - val_loss: 0.0867
Epoch 58/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0940 - val_loss: 0.0862
Epoch 59/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0935 - val_loss: 0.0861
Epoch 60/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0931 - val_loss: 0.0848
Epoch 61/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0929 - val_loss: 0.0869
Epoch 62/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0922 - val_loss: 0.0846
Epoch 63/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0918 - val_loss: 0.0858
Epoch 64/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0916 - val_loss: 0.0873
Epoch 65/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0914 - val_loss: 0.0842
Epoch 66/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0907 - val_loss: 0.0853
Epoch 67/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0902 - val_loss: 0.0849
Epoch 68/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0901 - val_loss: 0.0852
Epoch 69/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0897 - val_loss: 0.0848
Epoch 70/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0896 - val_loss: 0.0860
Epoch 71/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0889 - val_loss: 0.0856
Epoch 72/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0886 - val_loss: 0.0843
Epoch 73/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0884 - val_loss: 0.0857
Epoch 74/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0878 - val_loss: 0.0841
Epoch 75/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0875 - val_loss: 0.0853
Epoch 76/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0871 - val_loss: 0.0850
Epoch 77/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0868 - val_loss: 0.0844
Epoch 78/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0867 - val_loss: 0.0858
Epoch 79/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0863 - val_loss: 0.0862
Epoch 80/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0861 - val_loss: 0.0869
Epoch 81/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0857 - val_loss: 0.0859
Epoch 82/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0855 - val_loss: 0.0846
Epoch 83/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0851 - val_loss: 0.0856
Epoch 84/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0850 - val_loss: 0.0860
Val_yp Shape is 
(75701, 24)
Results === Test == Validation ===== 107
=============================
step 1    -  0.990    0.989
step 2    -  0.983    0.982
step 3    -  0.975    0.973
step 4    -  0.966    0.964
step 5    -  0.957    0.955
step 6    -  0.947    0.946
step 7    -  0.938    0.937
step 8    -  0.929    0.929
step 9    -  0.921    0.921
step 10   -  0.912    0.914
step 11   -  0.906    0.907
step 12   -  0.899    0.901
step 13   -  0.892    0.895
step 14   -  0.887    0.890
step 15   -  0.881    0.885
step 16   -  0.876    0.880
step 17   -  0.871    0.875
step 18   -  0.865    0.870
step 19   -  0.860    0.866
step 20   -  0.855    0.862
step 21   -  0.849    0.857
step 22   -  0.844    0.852
step 23   -  0.838    0.847
step 24   -  0.833    0.843
=============================
Summary   -  21.674    21.741
V_y Shape is 
(75701, 1)
=========== SHAPE INPUT-OUTPUT MATRIXES =====================
shape x_training : (227166, 756)  y_training: :  (227166, 24)
shape x_test     :  (75700, 756)  y_test      :  (75700, 24)
shape x_val      :  (75700, 756)  y_val       :  (75700, 24)
=============================================================
(756,)
Model: "MLP"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_13 (InputLayer)       [(None, 756)]             0         
                                                                 
 dense_48 (Dense)            (None, 1024)              775168    
                                                                 
 elu_12 (ELU)                (None, 1024)              0         
                                                                 
 dropout_36 (Dropout)        (None, 1024)              0         
                                                                 
 dense_49 (Dense)            (None, 512)               524800    
                                                                 
 dropout_37 (Dropout)        (None, 512)               0         
                                                                 
 dense_50 (Dense)            (None, 512)               262656    
                                                                 
 dropout_38 (Dropout)        (None, 512)               0         
                                                                 
 dense_51 (Dense)            (None, 24)                12312     
                                                                 
=================================================================
Total params: 1,574,936
Trainable params: 1,574,936
Non-trainable params: 0
_________________________________________________________________
Epoch 1/200
222/222 [==============================] - 2s 7ms/step - loss: 0.5098 - val_loss: 0.1164
Epoch 2/200
222/222 [==============================] - 1s 6ms/step - loss: 0.2582 - val_loss: 0.1058
Epoch 3/200
222/222 [==============================] - 1s 6ms/step - loss: 0.2124 - val_loss: 0.1001
Epoch 4/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1901 - val_loss: 0.0972
Epoch 5/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1755 - val_loss: 0.0968
Epoch 6/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1650 - val_loss: 0.0944
Epoch 7/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1574 - val_loss: 0.0933
Epoch 8/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1507 - val_loss: 0.0944
Epoch 9/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1454 - val_loss: 0.0916
Epoch 10/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1412 - val_loss: 0.0913
Epoch 11/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1369 - val_loss: 0.0904
Epoch 12/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1338 - val_loss: 0.0956
Epoch 13/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1309 - val_loss: 0.0907
Epoch 14/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1284 - val_loss: 0.0905
Epoch 15/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1262 - val_loss: 0.0906
Epoch 16/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1240 - val_loss: 0.0923
Epoch 17/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1219 - val_loss: 0.0905
Epoch 18/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1205 - val_loss: 0.0892
Epoch 19/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1189 - val_loss: 0.0921
Epoch 20/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1177 - val_loss: 0.0878
Epoch 21/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1164 - val_loss: 0.0875
Epoch 22/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1154 - val_loss: 0.0883
Epoch 23/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1144 - val_loss: 0.0894
Epoch 24/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1136 - val_loss: 0.0880
Epoch 25/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1127 - val_loss: 0.0901
Epoch 26/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1119 - val_loss: 0.0872
Epoch 27/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1112 - val_loss: 0.0868
Epoch 28/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1106 - val_loss: 0.0861
Epoch 29/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1097 - val_loss: 0.0887
Epoch 30/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1092 - val_loss: 0.0868
Epoch 31/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1085 - val_loss: 0.0871
Epoch 32/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1077 - val_loss: 0.0892
Epoch 33/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1072 - val_loss: 0.0879
Epoch 34/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1066 - val_loss: 0.0862
Epoch 35/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1060 - val_loss: 0.0881
Epoch 36/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1053 - val_loss: 0.0891
Epoch 37/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1045 - val_loss: 0.0893
Epoch 38/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1042 - val_loss: 0.0872
Val_yp Shape is 
(75700, 24)
Results === Test == Validation ===== 108
=============================
step 1    -  0.989    0.988
step 2    -  0.981    0.980
step 3    -  0.973    0.971
step 4    -  0.963    0.961
step 5    -  0.953    0.951
step 6    -  0.943    0.940
step 7    -  0.933    0.931
step 8    -  0.924    0.922
step 9    -  0.914    0.912
step 10   -  0.905    0.903
step 11   -  0.897    0.896
step 12   -  0.890    0.889
step 13   -  0.883    0.882
step 14   -  0.876    0.876
step 15   -  0.869    0.869
step 16   -  0.861    0.863
step 17   -  0.857    0.859
step 18   -  0.850    0.853
step 19   -  0.847    0.851
step 20   -  0.843    0.847
step 21   -  0.838    0.843
step 22   -  0.834    0.840
step 23   -  0.830    0.837
step 24   -  0.826    0.834
=============================
Summary   -  21.478    21.501
V_y Shape is 
(75700, 1)



=========== SHAPE INPUT-OUTPUT MATRIXES =====================
shape x_training : (227146, 896)  y_training: :  (227146, 24)
shape x_test     :  (75690, 896)  y_test      :  (75690, 24)
shape x_val      :  (75690, 896)  y_val       :  (75690, 24)
=============================================================
(896,)
Model: "MLP"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_1 (InputLayer)        [(None, 896)]             0         
                                                                 
 dense (Dense)               (None, 1024)              918528    
                                                                 
 elu (ELU)                   (None, 1024)              0         
                                                                 
 dropout (Dropout)           (None, 1024)              0         
                                                                 
 dense_1 (Dense)             (None, 512)               524800    
                                                                 
 dropout_1 (Dropout)         (None, 512)               0         
                                                                 
 dense_2 (Dense)             (None, 512)               262656    
                                                                 
 dropout_2 (Dropout)         (None, 512)               0         
                                                                 
 dense_3 (Dense)             (None, 24)                12312     
                                                                 
=================================================================
Total params: 1,718,296
Trainable params: 1,718,296
Non-trainable params: 0
_________________________________________________________________
Epoch 1/200
222/222 [==============================] - 3s 7ms/step - loss: 0.5396 - val_loss: 0.1197
Epoch 2/200
222/222 [==============================] - 1s 6ms/step - loss: 0.2662 - val_loss: 0.1071
Epoch 3/200
222/222 [==============================] - 1s 6ms/step - loss: 0.2177 - val_loss: 0.1006
Epoch 4/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1940 - val_loss: 0.0999
Epoch 5/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1785 - val_loss: 0.0987
Epoch 6/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1677 - val_loss: 0.0949
Epoch 7/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1592 - val_loss: 0.0967
Epoch 8/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1520 - val_loss: 0.0938
Epoch 9/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1463 - val_loss: 0.0948
Epoch 10/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1416 - val_loss: 0.0907
Epoch 11/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1371 - val_loss: 0.0937
Epoch 12/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1335 - val_loss: 0.0924
Epoch 13/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1305 - val_loss: 0.0941
Epoch 14/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1277 - val_loss: 0.0904
Epoch 15/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1253 - val_loss: 0.0910
Epoch 16/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1227 - val_loss: 0.0907
Epoch 17/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1209 - val_loss: 0.0894
Epoch 18/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1189 - val_loss: 0.0920
Epoch 19/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1174 - val_loss: 0.0880
Epoch 20/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1160 - val_loss: 0.0895
Epoch 21/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1146 - val_loss: 0.0880
Epoch 22/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1133 - val_loss: 0.0878
Epoch 23/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1123 - val_loss: 0.0898
Epoch 24/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1116 - val_loss: 0.0890
Epoch 25/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1101 - val_loss: 0.0878
Epoch 26/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1091 - val_loss: 0.0885
Epoch 27/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1084 - val_loss: 0.0883
Epoch 28/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1075 - val_loss: 0.0892
Epoch 29/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1068 - val_loss: 0.0876
Epoch 30/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1063 - val_loss: 0.0881
Epoch 31/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1054 - val_loss: 0.0900
Epoch 32/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1043 - val_loss: 0.0898
Epoch 33/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1037 - val_loss: 0.0908
Epoch 34/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1034 - val_loss: 0.0898
Epoch 35/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1025 - val_loss: 0.0908
Epoch 36/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1018 - val_loss: 0.0876
Epoch 37/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1012 - val_loss: 0.0891
Epoch 38/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1007 - val_loss: 0.0856
Epoch 39/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0998 - val_loss: 0.0870
Epoch 40/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0994 - val_loss: 0.0874
Epoch 41/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0984 - val_loss: 0.0881
Epoch 42/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0980 - val_loss: 0.0886
Epoch 43/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0973 - val_loss: 0.0884
Epoch 44/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0967 - val_loss: 0.0900
Epoch 45/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0961 - val_loss: 0.0874
Epoch 46/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0956 - val_loss: 0.0881
Epoch 47/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0951 - val_loss: 0.0905
Epoch 48/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0943 - val_loss: 0.0877
Val_yp Shape is 
(75690, 24)
Results === Test == Validation ===== 128
=============================
step 1    -  0.989    0.988
step 2    -  0.982    0.980
step 3    -  0.972    0.970
step 4    -  0.963    0.960
step 5    -  0.952    0.950
step 6    -  0.942    0.940
step 7    -  0.933    0.931
step 8    -  0.923    0.922
step 9    -  0.913    0.912
step 10   -  0.905    0.904
step 11   -  0.898    0.898
step 12   -  0.890    0.891
step 13   -  0.884    0.885
step 14   -  0.878    0.879
step 15   -  0.871    0.873
step 16   -  0.865    0.866
step 17   -  0.861    0.863
step 18   -  0.856    0.858
step 19   -  0.849    0.852
step 20   -  0.843    0.847
step 21   -  0.837    0.841
step 22   -  0.832    0.837
step 23   -  0.826    0.832
step 24   -  0.820    0.827
=============================
Summary   -  21.482    21.504
V_y Shape is 
(75690, 1)
=========== SHAPE INPUT-OUTPUT MATRIXES =====================
shape x_training : (227145, 903)  y_training: :  (227145, 24)
shape x_test     :  (75689, 903)  y_test      :  (75689, 24)
shape x_val      :  (75690, 903)  y_val       :  (75690, 24)
=============================================================
(903,)
Model: "MLP"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_2 (InputLayer)        [(None, 903)]             0         
                                                                 
 dense_4 (Dense)             (None, 1024)              925696    
                                                                 
 elu_1 (ELU)                 (None, 1024)              0         
                                                                 
 dropout_3 (Dropout)         (None, 1024)              0         
                                                                 
 dense_5 (Dense)             (None, 512)               524800    
                                                                 
 dropout_4 (Dropout)         (None, 512)               0         
                                                                 
 dense_6 (Dense)             (None, 512)               262656    
                                                                 
 dropout_5 (Dropout)         (None, 512)               0         
                                                                 
 dense_7 (Dense)             (None, 24)                12312     
                                                                 
=================================================================
Total params: 1,725,464
Trainable params: 1,725,464
Non-trainable params: 0
_________________________________________________________________
Epoch 1/200
222/222 [==============================] - 2s 7ms/step - loss: 0.5339 - val_loss: 0.1175
Epoch 2/200
222/222 [==============================] - 2s 7ms/step - loss: 0.2643 - val_loss: 0.1065
Epoch 3/200
222/222 [==============================] - 2s 7ms/step - loss: 0.2164 - val_loss: 0.1012
Epoch 4/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1933 - val_loss: 0.1001
Epoch 5/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1779 - val_loss: 0.0985
Epoch 6/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1672 - val_loss: 0.0968
Epoch 7/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1587 - val_loss: 0.0974
Epoch 8/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1519 - val_loss: 0.0954
Epoch 9/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1461 - val_loss: 0.0918
Epoch 10/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1411 - val_loss: 0.0919
Epoch 11/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1369 - val_loss: 0.0902
Epoch 12/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1332 - val_loss: 0.0961
Epoch 13/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1304 - val_loss: 0.0913
Epoch 14/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1273 - val_loss: 0.0906
Epoch 15/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1247 - val_loss: 0.0903
Epoch 16/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1226 - val_loss: 0.0906
Epoch 17/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1204 - val_loss: 0.0928
Epoch 18/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1188 - val_loss: 0.0904
Epoch 19/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1173 - val_loss: 0.0901
Epoch 20/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1157 - val_loss: 0.0900
Epoch 21/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1142 - val_loss: 0.0889
Epoch 22/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1130 - val_loss: 0.0916
Epoch 23/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1119 - val_loss: 0.0891
Epoch 24/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1108 - val_loss: 0.0902
Epoch 25/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1101 - val_loss: 0.0885
Epoch 26/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1091 - val_loss: 0.0898
Epoch 27/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1080 - val_loss: 0.0886
Epoch 28/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1075 - val_loss: 0.0876
Epoch 29/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1066 - val_loss: 0.0895
Epoch 30/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1059 - val_loss: 0.0897
Epoch 31/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1047 - val_loss: 0.0885
Epoch 32/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1043 - val_loss: 0.0881
Epoch 33/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1036 - val_loss: 0.0889
Epoch 34/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1030 - val_loss: 0.0877
Epoch 35/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1022 - val_loss: 0.0890
Epoch 36/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1013 - val_loss: 0.0896
Epoch 37/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1008 - val_loss: 0.0884
Epoch 38/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0999 - val_loss: 0.0882
Val_yp Shape is 
(75690, 24)
Results === Test == Validation ===== 129
=============================
step 1    -  0.988    0.987
step 2    -  0.981    0.979
step 3    -  0.972    0.970
step 4    -  0.962    0.960
step 5    -  0.951    0.949
step 6    -  0.941    0.938
step 7    -  0.930    0.929
step 8    -  0.920    0.919
step 9    -  0.911    0.911
step 10   -  0.903    0.903
step 11   -  0.895    0.895
step 12   -  0.888    0.889
step 13   -  0.882    0.884
step 14   -  0.877    0.879
step 15   -  0.872    0.875
step 16   -  0.867    0.870
step 17   -  0.862    0.865
step 18   -  0.858    0.862
step 19   -  0.855    0.860
step 20   -  0.849    0.855
step 21   -  0.845    0.852
step 22   -  0.840    0.848
step 23   -  0.835    0.844
step 24   -  0.828    0.839
=============================
Summary   -  21.513    21.563
V_y Shape is 
(75690, 1)
=========== SHAPE INPUT-OUTPUT MATRIXES =====================
shape x_training : (227144, 910)  y_training: :  (227144, 24)
shape x_test     :  (75689, 910)  y_test      :  (75689, 24)
shape x_val      :  (75689, 910)  y_val       :  (75689, 24)
=============================================================
(910,)
Model: "MLP"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_3 (InputLayer)        [(None, 910)]             0         
                                                                 
 dense_8 (Dense)             (None, 1024)              932864    
                                                                 
 elu_2 (ELU)                 (None, 1024)              0         
                                                                 
 dropout_6 (Dropout)         (None, 1024)              0         
                                                                 
 dense_9 (Dense)             (None, 512)               524800    
                                                                 
 dropout_7 (Dropout)         (None, 512)               0         
                                                                 
 dense_10 (Dense)            (None, 512)               262656    
                                                                 
 dropout_8 (Dropout)         (None, 512)               0         
                                                                 
 dense_11 (Dense)            (None, 24)                12312     
                                                                 
=================================================================
Total params: 1,732,632
Trainable params: 1,732,632
Non-trainable params: 0
_________________________________________________________________
Epoch 1/200
222/222 [==============================] - 2s 9ms/step - loss: 0.5302 - val_loss: 0.1171
Epoch 2/200
222/222 [==============================] - 2s 7ms/step - loss: 0.2646 - val_loss: 0.1084
Epoch 3/200
222/222 [==============================] - 2s 7ms/step - loss: 0.2170 - val_loss: 0.1068
Epoch 4/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1932 - val_loss: 0.0988
Epoch 5/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1784 - val_loss: 0.0965
Epoch 6/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1671 - val_loss: 0.0952
Epoch 7/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1585 - val_loss: 0.0945
Epoch 8/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1518 - val_loss: 0.0913
Epoch 9/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1459 - val_loss: 0.0938
Epoch 10/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1409 - val_loss: 0.0912
Epoch 11/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1368 - val_loss: 0.0910
Epoch 12/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1332 - val_loss: 0.0907
Epoch 13/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1297 - val_loss: 0.0928
Epoch 14/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1271 - val_loss: 0.0934
Epoch 15/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1245 - val_loss: 0.0930
Epoch 16/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1225 - val_loss: 0.0904
Epoch 17/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1204 - val_loss: 0.0897
Epoch 18/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1186 - val_loss: 0.0892
Epoch 19/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1170 - val_loss: 0.0899
Epoch 20/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1152 - val_loss: 0.0895
Epoch 21/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1140 - val_loss: 0.0898
Epoch 22/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1127 - val_loss: 0.0896
Epoch 23/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1116 - val_loss: 0.0906
Epoch 24/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1107 - val_loss: 0.0886
Epoch 25/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1097 - val_loss: 0.0892
Epoch 26/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1091 - val_loss: 0.0883
Epoch 27/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1080 - val_loss: 0.0892
Epoch 28/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1071 - val_loss: 0.0877
Epoch 29/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1065 - val_loss: 0.0888
Epoch 30/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1056 - val_loss: 0.0875
Epoch 31/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1049 - val_loss: 0.0892
Epoch 32/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1040 - val_loss: 0.0915
Epoch 33/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1032 - val_loss: 0.0875
Epoch 34/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1026 - val_loss: 0.0880
Epoch 35/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1019 - val_loss: 0.0888
Epoch 36/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1012 - val_loss: 0.0880
Epoch 37/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1003 - val_loss: 0.0881
Epoch 38/200
222/222 [==============================] - 2s 7ms/step - loss: 0.0999 - val_loss: 0.0879
Epoch 39/200
222/222 [==============================] - 2s 7ms/step - loss: 0.0988 - val_loss: 0.0879
Epoch 40/200
222/222 [==============================] - 2s 7ms/step - loss: 0.0982 - val_loss: 0.0876
Val_yp Shape is 
(75689, 24)
Results === Test == Validation ===== 130
=============================
step 1    -  0.988    0.987
step 2    -  0.981    0.979
step 3    -  0.972    0.970
step 4    -  0.961    0.959
step 5    -  0.951    0.949
step 6    -  0.941    0.939
step 7    -  0.931    0.930
step 8    -  0.921    0.921
step 9    -  0.913    0.914
step 10   -  0.905    0.906
step 11   -  0.898    0.900
step 12   -  0.891    0.893
step 13   -  0.883    0.886
step 14   -  0.877    0.881
step 15   -  0.872    0.876
step 16   -  0.867    0.871
step 17   -  0.860    0.866
step 18   -  0.855    0.861
step 19   -  0.850    0.857
step 20   -  0.844    0.851
step 21   -  0.840    0.848
step 22   -  0.835    0.844
step 23   -  0.830    0.840
step 24   -  0.824    0.835
=============================
Summary   -  21.487    21.565
V_y Shape is 
(75689, 1)
=========== SHAPE INPUT-OUTPUT MATRIXES =====================
shape x_training : (227143, 917)  y_training: :  (227143, 24)
shape x_test     :  (75688, 917)  y_test      :  (75688, 24)
shape x_val      :  (75689, 917)  y_val       :  (75689, 24)
=============================================================
(917,)
Model: "MLP"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_4 (InputLayer)        [(None, 917)]             0         
                                                                 
 dense_12 (Dense)            (None, 1024)              940032    
                                                                 
 elu_3 (ELU)                 (None, 1024)              0         
                                                                 
 dropout_9 (Dropout)         (None, 1024)              0         
                                                                 
 dense_13 (Dense)            (None, 512)               524800    
                                                                 
 dropout_10 (Dropout)        (None, 512)               0         
                                                                 
 dense_14 (Dense)            (None, 512)               262656    
                                                                 
 dropout_11 (Dropout)        (None, 512)               0         
                                                                 
 dense_15 (Dense)            (None, 24)                12312     
                                                                 
=================================================================
Total params: 1,739,800
Trainable params: 1,739,800
Non-trainable params: 0
_________________________________________________________________
Epoch 1/200
222/222 [==============================] - 2s 7ms/step - loss: 0.5311 - val_loss: 0.1178
Epoch 2/200
222/222 [==============================] - 1s 7ms/step - loss: 0.2652 - val_loss: 0.1054
Epoch 3/200
222/222 [==============================] - 1s 7ms/step - loss: 0.2172 - val_loss: 0.1028
Epoch 4/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1934 - val_loss: 0.1033
Epoch 5/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1783 - val_loss: 0.0982
Epoch 6/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1670 - val_loss: 0.0950
Epoch 7/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1587 - val_loss: 0.0957
Epoch 8/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1517 - val_loss: 0.0935
Epoch 9/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1458 - val_loss: 0.0928
Epoch 10/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1411 - val_loss: 0.0940
Epoch 11/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1368 - val_loss: 0.0919
Epoch 12/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1333 - val_loss: 0.0914
Epoch 13/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1296 - val_loss: 0.0895
Epoch 14/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1269 - val_loss: 0.0913
Epoch 15/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1242 - val_loss: 0.0894
Epoch 16/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1224 - val_loss: 0.0910
Epoch 17/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1202 - val_loss: 0.0908
Epoch 18/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1182 - val_loss: 0.0894
Epoch 19/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1164 - val_loss: 0.0891
Epoch 20/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1152 - val_loss: 0.0899
Epoch 21/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1139 - val_loss: 0.0887
Epoch 22/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1126 - val_loss: 0.0890
Epoch 23/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1116 - val_loss: 0.0877
Epoch 24/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1107 - val_loss: 0.0924
Epoch 25/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1097 - val_loss: 0.0901
Epoch 26/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1086 - val_loss: 0.0870
Epoch 27/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1077 - val_loss: 0.0884
Epoch 28/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1072 - val_loss: 0.0882
Epoch 29/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1061 - val_loss: 0.0883
Epoch 30/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1052 - val_loss: 0.0897
Epoch 31/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1045 - val_loss: 0.0891
Epoch 32/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1040 - val_loss: 0.0889
Epoch 33/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1031 - val_loss: 0.0898
Epoch 34/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1023 - val_loss: 0.0884
Epoch 35/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1017 - val_loss: 0.0876
Epoch 36/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1012 - val_loss: 0.0898
Val_yp Shape is 
(75689, 24)
Results === Test == Validation ===== 131
=============================
step 1    -  0.988    0.987
step 2    -  0.981    0.979
step 3    -  0.972    0.970
step 4    -  0.962    0.960
step 5    -  0.951    0.949
step 6    -  0.941    0.939
step 7    -  0.930    0.929
step 8    -  0.920    0.920
step 9    -  0.911    0.911
step 10   -  0.903    0.904
step 11   -  0.895    0.897
step 12   -  0.888    0.890
step 13   -  0.882    0.885
step 14   -  0.876    0.879
step 15   -  0.869    0.873
step 16   -  0.864    0.868
step 17   -  0.858    0.863
step 18   -  0.851    0.857
step 19   -  0.846    0.853
step 20   -  0.842    0.849
step 21   -  0.838    0.846
step 22   -  0.833    0.842
step 23   -  0.829    0.838
step 24   -  0.824    0.834
=============================
Summary   -  21.451    21.524
V_y Shape is 
(75689, 1)
=========== SHAPE INPUT-OUTPUT MATRIXES =====================
shape x_training : (227142, 924)  y_training: :  (227142, 24)
shape x_test     :  (75688, 924)  y_test      :  (75688, 24)
shape x_val      :  (75688, 924)  y_val       :  (75688, 24)
=============================================================
(924,)
Model: "MLP"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_5 (InputLayer)        [(None, 924)]             0         
                                                                 
 dense_16 (Dense)            (None, 1024)              947200    
                                                                 
 elu_4 (ELU)                 (None, 1024)              0         
                                                                 
 dropout_12 (Dropout)        (None, 1024)              0         
                                                                 
 dense_17 (Dense)            (None, 512)               524800    
                                                                 
 dropout_13 (Dropout)        (None, 512)               0         
                                                                 
 dense_18 (Dense)            (None, 512)               262656    
                                                                 
 dropout_14 (Dropout)        (None, 512)               0         
                                                                 
 dense_19 (Dense)            (None, 24)                12312     
                                                                 
=================================================================
Total params: 1,746,968
Trainable params: 1,746,968
Non-trainable params: 0
_________________________________________________________________
Epoch 1/200
222/222 [==============================] - 2s 7ms/step - loss: 0.5407 - val_loss: 0.1193
Epoch 2/200
222/222 [==============================] - 1s 7ms/step - loss: 0.2683 - val_loss: 0.1060
Epoch 3/200
222/222 [==============================] - 1s 7ms/step - loss: 0.2192 - val_loss: 0.1008
Epoch 4/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1950 - val_loss: 0.0993
Epoch 5/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1793 - val_loss: 0.0968
Epoch 6/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1682 - val_loss: 0.0964
Epoch 7/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1593 - val_loss: 0.0952
Epoch 8/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1521 - val_loss: 0.1005
Epoch 9/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1466 - val_loss: 0.0920
Epoch 10/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1416 - val_loss: 0.0927
Epoch 11/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1372 - val_loss: 0.0912
Epoch 12/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1334 - val_loss: 0.0925
Epoch 13/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1299 - val_loss: 0.0923
Epoch 14/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1273 - val_loss: 0.0903
Epoch 15/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1250 - val_loss: 0.0884
Epoch 16/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1226 - val_loss: 0.0898
Epoch 17/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1206 - val_loss: 0.0898
Epoch 18/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1185 - val_loss: 0.0912
Epoch 19/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1170 - val_loss: 0.0888
Epoch 20/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1151 - val_loss: 0.0883
Epoch 21/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1139 - val_loss: 0.0877
Epoch 22/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1127 - val_loss: 0.0893
Epoch 23/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1117 - val_loss: 0.0890
Epoch 24/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1104 - val_loss: 0.0884
Epoch 25/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1097 - val_loss: 0.0885
Epoch 26/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1089 - val_loss: 0.0885
Epoch 27/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1077 - val_loss: 0.0880
Epoch 28/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1070 - val_loss: 0.0880
Epoch 29/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1063 - val_loss: 0.0890
Epoch 30/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1054 - val_loss: 0.0885
Epoch 31/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1046 - val_loss: 0.0882
Val_yp Shape is 
(75688, 24)
Results === Test == Validation ===== 132
=============================
step 1    -  0.988    0.987
step 2    -  0.981    0.979
step 3    -  0.972    0.970
step 4    -  0.962    0.960
step 5    -  0.952    0.950
step 6    -  0.942    0.940
step 7    -  0.932    0.930
step 8    -  0.922    0.921
step 9    -  0.913    0.913
step 10   -  0.904    0.905
step 11   -  0.895    0.896
step 12   -  0.888    0.889
step 13   -  0.881    0.883
step 14   -  0.875    0.877
step 15   -  0.868    0.871
step 16   -  0.862    0.865
step 17   -  0.856    0.860
step 18   -  0.851    0.855
step 19   -  0.845    0.850
step 20   -  0.839    0.845
step 21   -  0.834    0.841
step 22   -  0.831    0.838
step 23   -  0.830    0.837
step 24   -  0.827    0.835
=============================
Summary   -  21.449    21.499
V_y Shape is 
(75688, 1)
=========== SHAPE INPUT-OUTPUT MATRIXES =====================
shape x_training : (227141, 931)  y_training: :  (227141, 24)
shape x_test     :  (75687, 931)  y_test      :  (75687, 24)
shape x_val      :  (75688, 931)  y_val       :  (75688, 24)
=============================================================
(931,)
Model: "MLP"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_6 (InputLayer)        [(None, 931)]             0         
                                                                 
 dense_20 (Dense)            (None, 1024)              954368    
                                                                 
 elu_5 (ELU)                 (None, 1024)              0         
                                                                 
 dropout_15 (Dropout)        (None, 1024)              0         
                                                                 
 dense_21 (Dense)            (None, 512)               524800    
                                                                 
 dropout_16 (Dropout)        (None, 512)               0         
                                                                 
 dense_22 (Dense)            (None, 512)               262656    
                                                                 
 dropout_17 (Dropout)        (None, 512)               0         
                                                                 
 dense_23 (Dense)            (None, 24)                12312     
                                                                 
=================================================================
Total params: 1,754,136
Trainable params: 1,754,136
Non-trainable params: 0
_________________________________________________________________
Epoch 1/200
222/222 [==============================] - 2s 7ms/step - loss: 0.5323 - val_loss: 0.1165
Epoch 2/200
222/222 [==============================] - 1s 7ms/step - loss: 0.2660 - val_loss: 0.1058
Epoch 3/200
222/222 [==============================] - 1s 7ms/step - loss: 0.2176 - val_loss: 0.1024
Epoch 4/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1938 - val_loss: 0.1002
Epoch 5/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1787 - val_loss: 0.0966
Epoch 6/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1674 - val_loss: 0.0946
Epoch 7/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1587 - val_loss: 0.0953
Epoch 8/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1515 - val_loss: 0.0952
Epoch 9/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1456 - val_loss: 0.0923
Epoch 10/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1408 - val_loss: 0.0917
Epoch 11/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1366 - val_loss: 0.0926
Epoch 12/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1330 - val_loss: 0.0902
Epoch 13/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1297 - val_loss: 0.0923
Epoch 14/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1269 - val_loss: 0.0910
Epoch 15/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1244 - val_loss: 0.0918
Epoch 16/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1222 - val_loss: 0.0934
Epoch 17/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1198 - val_loss: 0.0925
Epoch 18/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1183 - val_loss: 0.0937
Epoch 19/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1165 - val_loss: 0.0891
Epoch 20/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1149 - val_loss: 0.0897
Epoch 21/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1135 - val_loss: 0.0906
Epoch 22/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1126 - val_loss: 0.0897
Epoch 23/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1112 - val_loss: 0.0881
Epoch 24/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1102 - val_loss: 0.0898
Epoch 25/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1094 - val_loss: 0.0881
Epoch 26/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1082 - val_loss: 0.0886
Epoch 27/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1076 - val_loss: 0.0894
Epoch 28/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1068 - val_loss: 0.0896
Epoch 29/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1058 - val_loss: 0.0893
Epoch 30/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1051 - val_loss: 0.0895
Epoch 31/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1044 - val_loss: 0.0903
Epoch 32/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1035 - val_loss: 0.0884
Epoch 33/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1029 - val_loss: 0.0903
Epoch 34/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1020 - val_loss: 0.0884
Epoch 35/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1012 - val_loss: 0.0899
Val_yp Shape is 
(75688, 24)
Results === Test == Validation ===== 133
=============================
step 1    -  0.988    0.987
step 2    -  0.980    0.979
step 3    -  0.970    0.968
step 4    -  0.960    0.958
step 5    -  0.949    0.947
step 6    -  0.938    0.936
step 7    -  0.927    0.926
step 8    -  0.917    0.916
step 9    -  0.907    0.907
step 10   -  0.897    0.897
step 11   -  0.888    0.890
step 12   -  0.880    0.882
step 13   -  0.872    0.875
step 14   -  0.866    0.869
step 15   -  0.861    0.864
step 16   -  0.856    0.860
step 17   -  0.850    0.855
step 18   -  0.844    0.849
step 19   -  0.839    0.846
step 20   -  0.835    0.842
step 21   -  0.830    0.838
step 22   -  0.826    0.836
step 23   -  0.821    0.831
step 24   -  0.816    0.827
=============================
Summary   -  21.320    21.387
V_y Shape is 
(75688, 1)
=========== SHAPE INPUT-OUTPUT MATRIXES =====================
shape x_training : (227140, 938)  y_training: :  (227140, 24)
shape x_test     :  (75687, 938)  y_test      :  (75687, 24)
shape x_val      :  (75687, 938)  y_val       :  (75687, 24)
=============================================================
(938,)
Model: "MLP"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_7 (InputLayer)        [(None, 938)]             0         
                                                                 
 dense_24 (Dense)            (None, 1024)              961536    
                                                                 
 elu_6 (ELU)                 (None, 1024)              0         
                                                                 
 dropout_18 (Dropout)        (None, 1024)              0         
                                                                 
 dense_25 (Dense)            (None, 512)               524800    
                                                                 
 dropout_19 (Dropout)        (None, 512)               0         
                                                                 
 dense_26 (Dense)            (None, 512)               262656    
                                                                 
 dropout_20 (Dropout)        (None, 512)               0         
                                                                 
 dense_27 (Dense)            (None, 24)                12312     
                                                                 
=================================================================
Total params: 1,761,304
Trainable params: 1,761,304
Non-trainable params: 0
_________________________________________________________________
Epoch 1/200
222/222 [==============================] - 2s 7ms/step - loss: 0.5372 - val_loss: 0.1173
Epoch 2/200
222/222 [==============================] - 1s 6ms/step - loss: 0.2658 - val_loss: 0.1065
Epoch 3/200
222/222 [==============================] - 1s 6ms/step - loss: 0.2175 - val_loss: 0.1029
Epoch 4/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1936 - val_loss: 0.0982
Epoch 5/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1783 - val_loss: 0.0987
Epoch 6/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1673 - val_loss: 0.0947
Epoch 7/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1583 - val_loss: 0.0961
Epoch 8/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1515 - val_loss: 0.0967
Epoch 9/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1457 - val_loss: 0.0942
Epoch 10/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1409 - val_loss: 0.0919
Epoch 11/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1367 - val_loss: 0.0922
Epoch 12/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1329 - val_loss: 0.0918
Epoch 13/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1297 - val_loss: 0.0899
Epoch 14/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1269 - val_loss: 0.0912
Epoch 15/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1244 - val_loss: 0.0902
Epoch 16/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1217 - val_loss: 0.0896
Epoch 17/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1199 - val_loss: 0.0904
Epoch 18/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1180 - val_loss: 0.0913
Epoch 19/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1166 - val_loss: 0.0890
Epoch 20/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1147 - val_loss: 0.0898
Epoch 21/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1135 - val_loss: 0.0885
Epoch 22/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1122 - val_loss: 0.0894
Epoch 23/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1112 - val_loss: 0.0927
Epoch 24/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1102 - val_loss: 0.0950
Epoch 25/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1091 - val_loss: 0.0885
Epoch 26/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1081 - val_loss: 0.0878
Epoch 27/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1071 - val_loss: 0.0897
Epoch 28/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1065 - val_loss: 0.0885
Epoch 29/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1056 - val_loss: 0.0881
Epoch 30/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1046 - val_loss: 0.0884
Epoch 31/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1038 - val_loss: 0.0878
Epoch 32/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1034 - val_loss: 0.0905
Epoch 33/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1023 - val_loss: 0.0909
Epoch 34/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1016 - val_loss: 0.0886
Epoch 35/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1010 - val_loss: 0.0890
Epoch 36/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1006 - val_loss: 0.0889
Epoch 37/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0995 - val_loss: 0.0888
Epoch 38/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0990 - val_loss: 0.0889
Epoch 39/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0981 - val_loss: 0.0872
Epoch 40/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0976 - val_loss: 0.0879
Epoch 41/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0970 - val_loss: 0.0872
Epoch 42/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0964 - val_loss: 0.0896
Epoch 43/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0957 - val_loss: 0.0874
Epoch 44/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0952 - val_loss: 0.0878
Epoch 45/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0943 - val_loss: 0.0869
Epoch 46/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0940 - val_loss: 0.0880
Epoch 47/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0934 - val_loss: 0.0885
Epoch 48/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0925 - val_loss: 0.0901
Epoch 49/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0922 - val_loss: 0.0868
Epoch 50/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0917 - val_loss: 0.0904
Epoch 51/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0910 - val_loss: 0.0902
Epoch 52/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0908 - val_loss: 0.0870
Epoch 53/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0898 - val_loss: 0.0875
Epoch 54/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0894 - val_loss: 0.0875
Epoch 55/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0887 - val_loss: 0.0869
Epoch 56/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0885 - val_loss: 0.0877
Epoch 57/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0880 - val_loss: 0.0889
Epoch 58/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0873 - val_loss: 0.0883
Epoch 59/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0870 - val_loss: 0.0903
Val_yp Shape is 
(75687, 24)
Results === Test == Validation ===== 134
=============================
step 1    -  0.988    0.987
step 2    -  0.981    0.979
step 3    -  0.972    0.970
step 4    -  0.962    0.959
step 5    -  0.952    0.949
step 6    -  0.940    0.938
step 7    -  0.930    0.928
step 8    -  0.920    0.918
step 9    -  0.910    0.908
step 10   -  0.898    0.897
step 11   -  0.889    0.889
step 12   -  0.880    0.881
step 13   -  0.874    0.875
step 14   -  0.866    0.868
step 15   -  0.859    0.861
step 16   -  0.854    0.855
step 17   -  0.846    0.848
step 18   -  0.838    0.840
step 19   -  0.832    0.836
step 20   -  0.828    0.832
step 21   -  0.826    0.831
step 22   -  0.823    0.829
step 23   -  0.816    0.823
step 24   -  0.808    0.816
=============================
Summary   -  21.293    21.316
V_y Shape is 
(75687, 1)
=========== SHAPE INPUT-OUTPUT MATRIXES =====================
shape x_training : (227139, 945)  y_training: :  (227139, 24)
shape x_test     :  (75686, 945)  y_test      :  (75686, 24)
shape x_val      :  (75687, 945)  y_val       :  (75687, 24)
=============================================================
(945,)
Model: "MLP"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_8 (InputLayer)        [(None, 945)]             0         
                                                                 
 dense_28 (Dense)            (None, 1024)              968704    
                                                                 
 elu_7 (ELU)                 (None, 1024)              0         
                                                                 
 dropout_21 (Dropout)        (None, 1024)              0         
                                                                 
 dense_29 (Dense)            (None, 512)               524800    
                                                                 
 dropout_22 (Dropout)        (None, 512)               0         
                                                                 
 dense_30 (Dense)            (None, 512)               262656    
                                                                 
 dropout_23 (Dropout)        (None, 512)               0         
                                                                 
 dense_31 (Dense)            (None, 24)                12312     
                                                                 
=================================================================
Total params: 1,768,472
Trainable params: 1,768,472
Non-trainable params: 0
_________________________________________________________________
Epoch 1/200
222/222 [==============================] - 4s 8ms/step - loss: 0.5295 - val_loss: 0.1176
Epoch 2/200
222/222 [==============================] - 1s 6ms/step - loss: 0.2651 - val_loss: 0.1071
Epoch 3/200
222/222 [==============================] - 1s 7ms/step - loss: 0.2171 - val_loss: 0.1013
Epoch 4/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1933 - val_loss: 0.0984
Epoch 5/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1778 - val_loss: 0.0995
Epoch 6/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1667 - val_loss: 0.0948
Epoch 7/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1580 - val_loss: 0.0964
Epoch 8/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1511 - val_loss: 0.0933
Epoch 9/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1453 - val_loss: 0.0919
Epoch 10/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1402 - val_loss: 0.0940
Epoch 11/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1362 - val_loss: 0.0916
Epoch 12/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1328 - val_loss: 0.0899
Epoch 13/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1291 - val_loss: 0.0900
Epoch 14/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1264 - val_loss: 0.0937
Epoch 15/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1237 - val_loss: 0.0913
Epoch 16/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1216 - val_loss: 0.0896
Epoch 17/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1194 - val_loss: 0.0912
Epoch 18/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1180 - val_loss: 0.0884
Epoch 19/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1160 - val_loss: 0.0892
Epoch 20/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1145 - val_loss: 0.0907
Epoch 21/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1133 - val_loss: 0.0908
Epoch 22/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1120 - val_loss: 0.0910
Epoch 23/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1109 - val_loss: 0.0894
Epoch 24/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1095 - val_loss: 0.0907
Epoch 25/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1089 - val_loss: 0.0900
Epoch 26/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1079 - val_loss: 0.0890
Epoch 27/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1070 - val_loss: 0.0892
Epoch 28/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1065 - val_loss: 0.0892
Val_yp Shape is 
(75687, 24)
Results === Test == Validation ===== 135
=============================
step 1    -  0.988    0.987
step 2    -  0.981    0.979
step 3    -  0.971    0.969
step 4    -  0.961    0.959
step 5    -  0.951    0.949
step 6    -  0.941    0.938
step 7    -  0.931    0.929
step 8    -  0.921    0.920
step 9    -  0.912    0.911
step 10   -  0.905    0.904
step 11   -  0.897    0.897
step 12   -  0.890    0.890
step 13   -  0.883    0.883
step 14   -  0.877    0.878
step 15   -  0.870    0.872
step 16   -  0.864    0.866
step 17   -  0.859    0.861
step 18   -  0.853    0.857
step 19   -  0.848    0.852
step 20   -  0.844    0.848
step 21   -  0.839    0.844
step 22   -  0.835    0.841
step 23   -  0.830    0.837
step 24   -  0.825    0.832
=============================
Summary   -  21.477    21.504
V_y Shape is 
(75687, 1)
=========== SHAPE INPUT-OUTPUT MATRIXES =====================
shape x_training : (227138, 952)  y_training: :  (227138, 24)
shape x_test     :  (75686, 952)  y_test      :  (75686, 24)
shape x_val      :  (75686, 952)  y_val       :  (75686, 24)
=============================================================
(952,)
Model: "MLP"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_9 (InputLayer)        [(None, 952)]             0         
                                                                 
 dense_32 (Dense)            (None, 1024)              975872    
                                                                 
 elu_8 (ELU)                 (None, 1024)              0         
                                                                 
 dropout_24 (Dropout)        (None, 1024)              0         
                                                                 
 dense_33 (Dense)            (None, 512)               524800    
                                                                 
 dropout_25 (Dropout)        (None, 512)               0         
                                                                 
 dense_34 (Dense)            (None, 512)               262656    
                                                                 
 dropout_26 (Dropout)        (None, 512)               0         
                                                                 
 dense_35 (Dense)            (None, 24)                12312     
                                                                 
=================================================================
Total params: 1,775,640
Trainable params: 1,775,640
Non-trainable params: 0
_________________________________________________________________
Epoch 1/200
222/222 [==============================] - 2s 7ms/step - loss: 0.5429 - val_loss: 0.1182
Epoch 2/200
222/222 [==============================] - 1s 7ms/step - loss: 0.2690 - val_loss: 0.1074
Epoch 3/200
222/222 [==============================] - 2s 7ms/step - loss: 0.2194 - val_loss: 0.1013
Epoch 4/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1951 - val_loss: 0.0998
Epoch 5/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1791 - val_loss: 0.0959
Epoch 6/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1680 - val_loss: 0.0949
Epoch 7/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1594 - val_loss: 0.0980
Epoch 8/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1524 - val_loss: 0.0951
Epoch 9/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1464 - val_loss: 0.0929
Epoch 10/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1412 - val_loss: 0.0936
Epoch 11/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1371 - val_loss: 0.0905
Epoch 12/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1332 - val_loss: 0.0909
Epoch 13/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1300 - val_loss: 0.0897
Epoch 14/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1271 - val_loss: 0.0900
Epoch 15/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1245 - val_loss: 0.0910
Epoch 16/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1222 - val_loss: 0.0894
Epoch 17/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1201 - val_loss: 0.0900
Epoch 18/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1181 - val_loss: 0.0933
Epoch 19/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1165 - val_loss: 0.0873
Epoch 20/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1149 - val_loss: 0.0900
Epoch 21/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1138 - val_loss: 0.0898
Epoch 22/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1123 - val_loss: 0.0896
Epoch 23/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1115 - val_loss: 0.0891
Epoch 24/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1103 - val_loss: 0.0882
Epoch 25/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1091 - val_loss: 0.0904
Epoch 26/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1084 - val_loss: 0.0912
Epoch 27/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1073 - val_loss: 0.0905
Epoch 28/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1064 - val_loss: 0.0889
Epoch 29/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1058 - val_loss: 0.0886
Val_yp Shape is 
(75686, 24)
Results === Test == Validation ===== 136
=============================
step 1    -  0.988    0.987
step 2    -  0.981    0.979
step 3    -  0.972    0.970
step 4    -  0.962    0.960
step 5    -  0.951    0.949
step 6    -  0.941    0.939
step 7    -  0.930    0.929
step 8    -  0.921    0.920
step 9    -  0.912    0.912
step 10   -  0.904    0.904
step 11   -  0.896    0.897
step 12   -  0.890    0.891
step 13   -  0.884    0.885
step 14   -  0.878    0.880
step 15   -  0.873    0.875
step 16   -  0.868    0.870
step 17   -  0.862    0.865
step 18   -  0.858    0.862
step 19   -  0.853    0.857
step 20   -  0.848    0.853
step 21   -  0.843    0.849
step 22   -  0.840    0.846
step 23   -  0.836    0.843
step 24   -  0.832    0.839
=============================
Summary   -  21.522    21.561
V_y Shape is 
(75686, 1)
=========== SHAPE INPUT-OUTPUT MATRIXES =====================
shape x_training : (227137, 959)  y_training: :  (227137, 24)
shape x_test     :  (75685, 959)  y_test      :  (75685, 24)
shape x_val      :  (75686, 959)  y_val       :  (75686, 24)
=============================================================
(959,)
Model: "MLP"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_10 (InputLayer)       [(None, 959)]             0         
                                                                 
 dense_36 (Dense)            (None, 1024)              983040    
                                                                 
 elu_9 (ELU)                 (None, 1024)              0         
                                                                 
 dropout_27 (Dropout)        (None, 1024)              0         
                                                                 
 dense_37 (Dense)            (None, 512)               524800    
                                                                 
 dropout_28 (Dropout)        (None, 512)               0         
                                                                 
 dense_38 (Dense)            (None, 512)               262656    
                                                                 
 dropout_29 (Dropout)        (None, 512)               0         
                                                                 
 dense_39 (Dense)            (None, 24)                12312     
                                                                 
=================================================================
Total params: 1,782,808
Trainable params: 1,782,808
Non-trainable params: 0
_________________________________________________________________
Epoch 1/200
222/222 [==============================] - 2s 7ms/step - loss: 0.5296 - val_loss: 0.1176
Epoch 2/200
222/222 [==============================] - 1s 6ms/step - loss: 0.2656 - val_loss: 0.1051
Epoch 3/200
222/222 [==============================] - 1s 6ms/step - loss: 0.2179 - val_loss: 0.1024
Epoch 4/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1939 - val_loss: 0.0976
Epoch 5/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1786 - val_loss: 0.0957
Epoch 6/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1670 - val_loss: 0.0975
Epoch 7/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1585 - val_loss: 0.0938
Epoch 8/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1514 - val_loss: 0.0926
Epoch 9/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1454 - val_loss: 0.0927
Epoch 10/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1403 - val_loss: 0.0920
Epoch 11/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1362 - val_loss: 0.0920
Epoch 12/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1324 - val_loss: 0.0919
Epoch 13/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1290 - val_loss: 0.0922
Epoch 14/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1264 - val_loss: 0.0929
Epoch 15/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1236 - val_loss: 0.0892
Epoch 16/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1210 - val_loss: 0.0904
Epoch 17/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1194 - val_loss: 0.0894
Epoch 18/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1174 - val_loss: 0.0897
Epoch 19/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1158 - val_loss: 0.0907
Epoch 20/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1143 - val_loss: 0.0916
Epoch 21/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1131 - val_loss: 0.0916
Epoch 22/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1119 - val_loss: 0.0929
Epoch 23/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1105 - val_loss: 0.0907
Epoch 24/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1094 - val_loss: 0.0906
Epoch 25/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1085 - val_loss: 0.0885
Epoch 26/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1078 - val_loss: 0.0871
Epoch 27/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1068 - val_loss: 0.0895
Epoch 28/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1061 - val_loss: 0.0887
Epoch 29/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1051 - val_loss: 0.0892
Epoch 30/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1042 - val_loss: 0.0900
Epoch 31/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1036 - val_loss: 0.0885
Epoch 32/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1028 - val_loss: 0.0922
Epoch 33/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1018 - val_loss: 0.0883
Epoch 34/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1011 - val_loss: 0.0885
Epoch 35/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1003 - val_loss: 0.0907
Epoch 36/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0999 - val_loss: 0.0881
Val_yp Shape is 
(75686, 24)
Results === Test == Validation ===== 137
=============================
step 1    -  0.988    0.988
step 2    -  0.981    0.980
step 3    -  0.972    0.970
step 4    -  0.962    0.960
step 5    -  0.952    0.950
step 6    -  0.941    0.940
step 7    -  0.931    0.929
step 8    -  0.920    0.919
step 9    -  0.910    0.909
step 10   -  0.901    0.901
step 11   -  0.893    0.894
step 12   -  0.886    0.887
step 13   -  0.879    0.881
step 14   -  0.872    0.874
step 15   -  0.866    0.869
step 16   -  0.860    0.864
step 17   -  0.854    0.858
step 18   -  0.850    0.855
step 19   -  0.844    0.850
step 20   -  0.839    0.846
step 21   -  0.834    0.842
step 22   -  0.829    0.838
step 23   -  0.822    0.832
step 24   -  0.816    0.826
=============================
Summary   -  21.403    21.460
V_y Shape is 
(75686, 1)
=========== SHAPE INPUT-OUTPUT MATRIXES =====================
shape x_training : (227136, 966)  y_training: :  (227136, 24)
shape x_test     :  (75685, 966)  y_test      :  (75685, 24)
shape x_val      :  (75685, 966)  y_val       :  (75685, 24)
=============================================================
(966,)
Model: "MLP"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_11 (InputLayer)       [(None, 966)]             0         
                                                                 
 dense_40 (Dense)            (None, 1024)              990208    
                                                                 
 elu_10 (ELU)                (None, 1024)              0         
                                                                 
 dropout_30 (Dropout)        (None, 1024)              0         
                                                                 
 dense_41 (Dense)            (None, 512)               524800    
                                                                 
 dropout_31 (Dropout)        (None, 512)               0         
                                                                 
 dense_42 (Dense)            (None, 512)               262656    
                                                                 
 dropout_32 (Dropout)        (None, 512)               0         
                                                                 
 dense_43 (Dense)            (None, 24)                12312     
                                                                 
=================================================================
Total params: 1,789,976
Trainable params: 1,789,976
Non-trainable params: 0
_________________________________________________________________
Epoch 1/200
222/222 [==============================] - 2s 8ms/step - loss: 0.5354 - val_loss: 0.1160
Epoch 2/200
222/222 [==============================] - 1s 7ms/step - loss: 0.2651 - val_loss: 0.1045
Epoch 3/200
222/222 [==============================] - 1s 7ms/step - loss: 0.2169 - val_loss: 0.1028
Epoch 4/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1935 - val_loss: 0.0984
Epoch 5/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1781 - val_loss: 0.0972
Epoch 6/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1667 - val_loss: 0.0957
Epoch 7/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1581 - val_loss: 0.0943
Epoch 8/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1509 - val_loss: 0.0929
Epoch 9/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1449 - val_loss: 0.0919
Epoch 10/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1402 - val_loss: 0.0930
Epoch 11/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1363 - val_loss: 0.0946
Epoch 12/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1323 - val_loss: 0.0917
Epoch 13/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1292 - val_loss: 0.0920
Epoch 14/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1262 - val_loss: 0.0912
Epoch 15/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1236 - val_loss: 0.0905
Epoch 16/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1214 - val_loss: 0.0934
Epoch 17/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1191 - val_loss: 0.0909
Epoch 18/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1173 - val_loss: 0.0895
Epoch 19/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1156 - val_loss: 0.0902
Epoch 20/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1143 - val_loss: 0.0888
Epoch 21/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1128 - val_loss: 0.0904
Epoch 22/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1116 - val_loss: 0.0925
Epoch 23/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1105 - val_loss: 0.0893
Epoch 24/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1097 - val_loss: 0.0879
Epoch 25/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1084 - val_loss: 0.0898
Epoch 26/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1076 - val_loss: 0.0892
Epoch 27/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1067 - val_loss: 0.0909
Epoch 28/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1058 - val_loss: 0.0899
Epoch 29/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1051 - val_loss: 0.0891
Epoch 30/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1043 - val_loss: 0.0898
Epoch 31/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1034 - val_loss: 0.0888
Epoch 32/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1024 - val_loss: 0.0895
Epoch 33/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1019 - val_loss: 0.0920
Epoch 34/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1012 - val_loss: 0.0895
Val_yp Shape is 
(75685, 24)
Results === Test == Validation ===== 138
=============================
step 1    -  0.988    0.987
step 2    -  0.980    0.979
step 3    -  0.971    0.970
step 4    -  0.962    0.960
step 5    -  0.952    0.950
step 6    -  0.942    0.941
step 7    -  0.932    0.931
step 8    -  0.923    0.922
step 9    -  0.914    0.914
step 10   -  0.905    0.906
step 11   -  0.897    0.898
step 12   -  0.889    0.890
step 13   -  0.882    0.883
step 14   -  0.875    0.877
step 15   -  0.870    0.872
step 16   -  0.865    0.868
step 17   -  0.862    0.865
step 18   -  0.856    0.861
step 19   -  0.852    0.857
step 20   -  0.849    0.855
step 21   -  0.845    0.851
step 22   -  0.840    0.847
step 23   -  0.835    0.843
step 24   -  0.830    0.839
=============================
Summary   -  21.517    21.567
V_y Shape is 
(75685, 1)
=========== SHAPE INPUT-OUTPUT MATRIXES =====================
shape x_training : (227135, 973)  y_training: :  (227135, 24)
shape x_test     :  (75684, 973)  y_test      :  (75684, 24)
shape x_val      :  (75685, 973)  y_val       :  (75685, 24)
=============================================================
(973,)
Model: "MLP"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_12 (InputLayer)       [(None, 973)]             0         
                                                                 
 dense_44 (Dense)            (None, 1024)              997376    
                                                                 
 elu_11 (ELU)                (None, 1024)              0         
                                                                 
 dropout_33 (Dropout)        (None, 1024)              0         
                                                                 
 dense_45 (Dense)            (None, 512)               524800    
                                                                 
 dropout_34 (Dropout)        (None, 512)               0         
                                                                 
 dense_46 (Dense)            (None, 512)               262656    
                                                                 
 dropout_35 (Dropout)        (None, 512)               0         
                                                                 
 dense_47 (Dense)            (None, 24)                12312     
                                                                 
=================================================================
Total params: 1,797,144
Trainable params: 1,797,144
Non-trainable params: 0
_________________________________________________________________
Epoch 1/200
222/222 [==============================] - 2s 8ms/step - loss: 0.5336 - val_loss: 0.1187
Epoch 2/200
222/222 [==============================] - 2s 7ms/step - loss: 0.2665 - val_loss: 0.1060
Epoch 3/200
222/222 [==============================] - 2s 7ms/step - loss: 0.2184 - val_loss: 0.1016
Epoch 4/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1942 - val_loss: 0.0991
Epoch 5/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1787 - val_loss: 0.0981
Epoch 6/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1672 - val_loss: 0.0945
Epoch 7/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1586 - val_loss: 0.0956
Epoch 8/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1514 - val_loss: 0.0968
Epoch 9/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1457 - val_loss: 0.0946
Epoch 10/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1403 - val_loss: 0.0919
Epoch 11/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1362 - val_loss: 0.0919
Epoch 12/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1326 - val_loss: 0.0933
Epoch 13/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1291 - val_loss: 0.0940
Epoch 14/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1260 - val_loss: 0.0895
Epoch 15/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1238 - val_loss: 0.0921
Epoch 16/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1212 - val_loss: 0.0923
Epoch 17/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1191 - val_loss: 0.0932
Epoch 18/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1172 - val_loss: 0.0901
Epoch 19/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1158 - val_loss: 0.0895
Epoch 20/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1142 - val_loss: 0.0916
Epoch 21/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1126 - val_loss: 0.0904
Epoch 22/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1116 - val_loss: 0.0894
Epoch 23/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1105 - val_loss: 0.0888
Epoch 24/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1094 - val_loss: 0.0874
Epoch 25/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1084 - val_loss: 0.0891
Epoch 26/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1074 - val_loss: 0.0884
Epoch 27/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1066 - val_loss: 0.0889
Epoch 28/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1058 - val_loss: 0.0890
Epoch 29/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1045 - val_loss: 0.0883
Epoch 30/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1040 - val_loss: 0.0897
Epoch 31/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1029 - val_loss: 0.0891
Epoch 32/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1023 - val_loss: 0.0899
Epoch 33/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1018 - val_loss: 0.0888
Epoch 34/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1011 - val_loss: 0.0893


=========== SHAPE INPUT-OUTPUT MATRIXES =====================
shape x_training : (227134, 980)  y_training: :  (227134, 24)
shape x_test     :  (75684, 980)  y_test      :  (75684, 24)
shape x_val      :  (75684, 980)  y_val       :  (75684, 24)
=============================================================
(980,)
Model: "MLP"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_1 (InputLayer)        [(None, 980)]             0         
                                                                 
 dense (Dense)               (None, 1024)              1004544   
                                                                 
 elu (ELU)                   (None, 1024)              0         
                                                                 
 dropout (Dropout)           (None, 1024)              0         
                                                                 
 dense_1 (Dense)             (None, 512)               524800    
                                                                 
 dropout_1 (Dropout)         (None, 512)               0         
                                                                 
 dense_2 (Dense)             (None, 512)               262656    
                                                                 
 dropout_2 (Dropout)         (None, 512)               0         
                                                                 
 dense_3 (Dense)             (None, 24)                12312     
                                                                 
=================================================================
Total params: 1,804,312
Trainable params: 1,804,312
Non-trainable params: 0
_________________________________________________________________
Epoch 1/200
222/222 [==============================] - 3s 7ms/step - loss: 0.5451 - val_loss: 0.1181
Epoch 2/200
222/222 [==============================] - 1s 6ms/step - loss: 0.2694 - val_loss: 0.1104
Epoch 3/200
222/222 [==============================] - 1s 6ms/step - loss: 0.2194 - val_loss: 0.1031
Epoch 4/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1947 - val_loss: 0.1000
Epoch 5/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1793 - val_loss: 0.0973
Epoch 6/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1679 - val_loss: 0.0980
Epoch 7/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1589 - val_loss: 0.0942
Epoch 8/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1519 - val_loss: 0.0954
Epoch 9/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1458 - val_loss: 0.0928
Epoch 10/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1408 - val_loss: 0.0933
Epoch 11/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1365 - val_loss: 0.0918
Epoch 12/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1328 - val_loss: 0.0911
Epoch 13/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1292 - val_loss: 0.0909
Epoch 14/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1262 - val_loss: 0.0923
Epoch 15/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1239 - val_loss: 0.0938
Epoch 16/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1217 - val_loss: 0.0914
Epoch 17/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1193 - val_loss: 0.0900
Epoch 18/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1176 - val_loss: 0.0944
Epoch 19/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1158 - val_loss: 0.0903
Epoch 20/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1143 - val_loss: 0.0914
Epoch 21/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1127 - val_loss: 0.0902
Epoch 22/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1116 - val_loss: 0.0897
Epoch 23/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1103 - val_loss: 0.0881
Epoch 24/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1092 - val_loss: 0.0883
Epoch 25/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1082 - val_loss: 0.0909
Epoch 26/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1072 - val_loss: 0.0926
Epoch 27/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1062 - val_loss: 0.0891
Epoch 28/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1055 - val_loss: 0.0903
Epoch 29/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1047 - val_loss: 0.0920
Epoch 30/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1038 - val_loss: 0.0907
Epoch 31/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1030 - val_loss: 0.0893
Epoch 32/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1022 - val_loss: 0.0893
Epoch 33/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1016 - val_loss: 0.0894
Val_yp Shape is 
(75684, 24)
Results === Test == Validation ===== 140
=============================
step 1    -  0.988    0.987
step 2    -  0.981    0.979
step 3    -  0.971    0.969
step 4    -  0.961    0.959
step 5    -  0.950    0.948
step 6    -  0.939    0.937
step 7    -  0.928    0.926
step 8    -  0.918    0.917
step 9    -  0.908    0.908
step 10   -  0.900    0.900
step 11   -  0.893    0.893
step 12   -  0.885    0.886
step 13   -  0.879    0.880
step 14   -  0.873    0.875
step 15   -  0.866    0.868
step 16   -  0.861    0.864
step 17   -  0.856    0.859
step 18   -  0.852    0.855
step 19   -  0.846    0.850
step 20   -  0.839    0.843
step 21   -  0.834    0.839
step 22   -  0.828    0.834
step 23   -  0.823    0.830
step 24   -  0.816    0.824
=============================
Summary   -  21.395    21.429
V_y Shape is 
(75684, 1)
=========== SHAPE INPUT-OUTPUT MATRIXES =====================
shape x_training : (227133, 987)  y_training: :  (227133, 24)
shape x_test     :  (75683, 987)  y_test      :  (75683, 24)
shape x_val      :  (75684, 987)  y_val       :  (75684, 24)
=============================================================
(987,)
Model: "MLP"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_2 (InputLayer)        [(None, 987)]             0         
                                                                 
 dense_4 (Dense)             (None, 1024)              1011712   
                                                                 
 elu_1 (ELU)                 (None, 1024)              0         
                                                                 
 dropout_3 (Dropout)         (None, 1024)              0         
                                                                 
 dense_5 (Dense)             (None, 512)               524800    
                                                                 
 dropout_4 (Dropout)         (None, 512)               0         
                                                                 
 dense_6 (Dense)             (None, 512)               262656    
                                                                 
 dropout_5 (Dropout)         (None, 512)               0         
                                                                 
 dense_7 (Dense)             (None, 24)                12312     
                                                                 
=================================================================
Total params: 1,811,480
Trainable params: 1,811,480
Non-trainable params: 0
_________________________________________________________________
Epoch 1/200
222/222 [==============================] - 2s 7ms/step - loss: 0.5416 - val_loss: 0.1177
Epoch 2/200
222/222 [==============================] - 2s 7ms/step - loss: 0.2681 - val_loss: 0.1082
Epoch 3/200
222/222 [==============================] - 2s 7ms/step - loss: 0.2193 - val_loss: 0.1031
Epoch 4/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1952 - val_loss: 0.1009
Epoch 5/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1792 - val_loss: 0.0964
Epoch 6/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1679 - val_loss: 0.0975
Epoch 7/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1589 - val_loss: 0.0945
Epoch 8/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1518 - val_loss: 0.0978
Epoch 9/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1457 - val_loss: 0.0921
Epoch 10/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1402 - val_loss: 0.0923
Epoch 11/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1362 - val_loss: 0.0916
Epoch 12/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1327 - val_loss: 0.0906
Epoch 13/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1292 - val_loss: 0.0938
Epoch 14/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1263 - val_loss: 0.0931
Epoch 15/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1234 - val_loss: 0.0943
Epoch 16/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1213 - val_loss: 0.0893
Epoch 17/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1191 - val_loss: 0.0909
Epoch 18/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1172 - val_loss: 0.0905
Epoch 19/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1156 - val_loss: 0.0898
Epoch 20/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1138 - val_loss: 0.0905
Epoch 21/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1128 - val_loss: 0.0896
Epoch 22/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1115 - val_loss: 0.0897
Epoch 23/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1101 - val_loss: 0.0900
Epoch 24/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1093 - val_loss: 0.0913
Epoch 25/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1081 - val_loss: 0.0891
Epoch 26/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1072 - val_loss: 0.0919
Epoch 27/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1061 - val_loss: 0.0919
Epoch 28/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1055 - val_loss: 0.0909
Epoch 29/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1046 - val_loss: 0.0891
Epoch 30/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1039 - val_loss: 0.0915
Epoch 31/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1030 - val_loss: 0.0896
Epoch 32/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1018 - val_loss: 0.0893
Epoch 33/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1013 - val_loss: 0.0888
Epoch 34/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1006 - val_loss: 0.0902
Epoch 35/200
222/222 [==============================] - 2s 7ms/step - loss: 0.0999 - val_loss: 0.0891
Epoch 36/200
222/222 [==============================] - 2s 7ms/step - loss: 0.0990 - val_loss: 0.0906
Epoch 37/200
222/222 [==============================] - 2s 7ms/step - loss: 0.0980 - val_loss: 0.0905
Epoch 38/200
222/222 [==============================] - 2s 7ms/step - loss: 0.0974 - val_loss: 0.0897
Epoch 39/200
222/222 [==============================] - 2s 7ms/step - loss: 0.0968 - val_loss: 0.0893
Epoch 40/200
222/222 [==============================] - 2s 7ms/step - loss: 0.0963 - val_loss: 0.0909
Epoch 41/200
222/222 [==============================] - 2s 7ms/step - loss: 0.0956 - val_loss: 0.0895
Epoch 42/200
222/222 [==============================] - 2s 7ms/step - loss: 0.0947 - val_loss: 0.0884
Epoch 43/200
222/222 [==============================] - 2s 7ms/step - loss: 0.0943 - val_loss: 0.0890
Epoch 44/200
222/222 [==============================] - 2s 7ms/step - loss: 0.0934 - val_loss: 0.0902
Epoch 45/200
222/222 [==============================] - 2s 7ms/step - loss: 0.0931 - val_loss: 0.0898
Epoch 46/200
222/222 [==============================] - 2s 7ms/step - loss: 0.0923 - val_loss: 0.0884
Epoch 47/200
222/222 [==============================] - 2s 7ms/step - loss: 0.0917 - val_loss: 0.0885
Epoch 48/200
222/222 [==============================] - 2s 7ms/step - loss: 0.0909 - val_loss: 0.0894
Epoch 49/200
222/222 [==============================] - 2s 7ms/step - loss: 0.0906 - val_loss: 0.0888
Epoch 50/200
222/222 [==============================] - 2s 7ms/step - loss: 0.0900 - val_loss: 0.0882
Epoch 51/200
222/222 [==============================] - 2s 7ms/step - loss: 0.0893 - val_loss: 0.0894
Epoch 52/200
222/222 [==============================] - 2s 7ms/step - loss: 0.0889 - val_loss: 0.0898
Epoch 53/200
222/222 [==============================] - 2s 7ms/step - loss: 0.0885 - val_loss: 0.0876
Epoch 54/200
222/222 [==============================] - 2s 7ms/step - loss: 0.0881 - val_loss: 0.0883
Epoch 55/200
222/222 [==============================] - 2s 7ms/step - loss: 0.0875 - val_loss: 0.0878
Epoch 56/200
222/222 [==============================] - 2s 7ms/step - loss: 0.0867 - val_loss: 0.0879
Epoch 57/200
222/222 [==============================] - 2s 7ms/step - loss: 0.0863 - val_loss: 0.0886
Epoch 58/200
222/222 [==============================] - 2s 7ms/step - loss: 0.0860 - val_loss: 0.0880
Epoch 59/200
222/222 [==============================] - 2s 7ms/step - loss: 0.0854 - val_loss: 0.0884
Epoch 60/200
222/222 [==============================] - 2s 7ms/step - loss: 0.0849 - val_loss: 0.0889
Epoch 61/200
222/222 [==============================] - 2s 7ms/step - loss: 0.0845 - val_loss: 0.0877
Epoch 62/200
222/222 [==============================] - 2s 7ms/step - loss: 0.0839 - val_loss: 0.0892
Epoch 63/200
222/222 [==============================] - 2s 7ms/step - loss: 0.0836 - val_loss: 0.0901
Val_yp Shape is 
(75684, 24)
Results === Test == Validation ===== 141
=============================
step 1    -  0.989    0.988
step 2    -  0.982    0.980
step 3    -  0.973    0.971
step 4    -  0.964    0.962
step 5    -  0.954    0.953
step 6    -  0.944    0.943
step 7    -  0.935    0.934
step 8    -  0.925    0.925
step 9    -  0.916    0.916
step 10   -  0.907    0.908
step 11   -  0.898    0.900
step 12   -  0.891    0.892
step 13   -  0.884    0.886
step 14   -  0.878    0.880
step 15   -  0.872    0.874
step 16   -  0.866    0.869
step 17   -  0.861    0.865
step 18   -  0.857    0.861
step 19   -  0.851    0.856
step 20   -  0.846    0.852
step 21   -  0.842    0.848
step 22   -  0.838    0.844
step 23   -  0.834    0.841
step 24   -  0.830    0.838
=============================
Summary   -  21.536    21.587
V_y Shape is 
(75684, 1)
=========== SHAPE INPUT-OUTPUT MATRIXES =====================
shape x_training : (227132, 994)  y_training: :  (227132, 24)
shape x_test     :  (75683, 994)  y_test      :  (75683, 24)
shape x_val      :  (75683, 994)  y_val       :  (75683, 24)
=============================================================
(994,)
Model: "MLP"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_3 (InputLayer)        [(None, 994)]             0         
                                                                 
 dense_8 (Dense)             (None, 1024)              1018880   
                                                                 
 elu_2 (ELU)                 (None, 1024)              0         
                                                                 
 dropout_6 (Dropout)         (None, 1024)              0         
                                                                 
 dense_9 (Dense)             (None, 512)               524800    
                                                                 
 dropout_7 (Dropout)         (None, 512)               0         
                                                                 
 dense_10 (Dense)            (None, 512)               262656    
                                                                 
 dropout_8 (Dropout)         (None, 512)               0         
                                                                 
 dense_11 (Dense)            (None, 24)                12312     
                                                                 
=================================================================
Total params: 1,818,648
Trainable params: 1,818,648
Non-trainable params: 0
_________________________________________________________________
Epoch 1/200
222/222 [==============================] - 2s 7ms/step - loss: 0.5383 - val_loss: 0.1180
Epoch 2/200
222/222 [==============================] - 1s 6ms/step - loss: 0.2676 - val_loss: 0.1081
Epoch 3/200
222/222 [==============================] - 1s 6ms/step - loss: 0.2184 - val_loss: 0.1054
Epoch 4/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1943 - val_loss: 0.0999
Epoch 5/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1785 - val_loss: 0.1009
Epoch 6/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1671 - val_loss: 0.0984
Epoch 7/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1583 - val_loss: 0.0976
Epoch 8/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1516 - val_loss: 0.0938
Epoch 9/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1457 - val_loss: 0.0935
Epoch 10/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1402 - val_loss: 0.0960
Epoch 11/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1359 - val_loss: 0.0922
Epoch 12/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1321 - val_loss: 0.0927
Epoch 13/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1287 - val_loss: 0.0940
Epoch 14/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1260 - val_loss: 0.0917
Epoch 15/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1232 - val_loss: 0.0912
Epoch 16/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1209 - val_loss: 0.0923
Epoch 17/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1189 - val_loss: 0.0916
Epoch 18/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1168 - val_loss: 0.0909
Epoch 19/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1153 - val_loss: 0.0903
Epoch 20/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1140 - val_loss: 0.0917
Epoch 21/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1123 - val_loss: 0.0915
Epoch 22/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1110 - val_loss: 0.0903
Epoch 23/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1099 - val_loss: 0.0901
Epoch 24/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1089 - val_loss: 0.0901
Epoch 25/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1081 - val_loss: 0.0926
Epoch 26/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1069 - val_loss: 0.0886
Epoch 27/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1061 - val_loss: 0.0891
Epoch 28/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1052 - val_loss: 0.0895
Epoch 29/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1044 - val_loss: 0.0880
Epoch 30/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1036 - val_loss: 0.0895
Epoch 31/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1027 - val_loss: 0.0900
Epoch 32/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1020 - val_loss: 0.0898
Epoch 33/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1011 - val_loss: 0.0895
Epoch 34/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1002 - val_loss: 0.0888
Epoch 35/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0995 - val_loss: 0.0894
Epoch 36/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0989 - val_loss: 0.0899
Epoch 37/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0980 - val_loss: 0.0912
Epoch 38/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0974 - val_loss: 0.0889
Epoch 39/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0967 - val_loss: 0.0888
Val_yp Shape is 
(75683, 24)
Results === Test == Validation ===== 142
=============================
step 1    -  0.988    0.987
step 2    -  0.981    0.979
step 3    -  0.972    0.970
step 4    -  0.962    0.959
step 5    -  0.951    0.949
step 6    -  0.941    0.939
step 7    -  0.931    0.929
step 8    -  0.921    0.920
step 9    -  0.911    0.910
step 10   -  0.903    0.903
step 11   -  0.895    0.895
step 12   -  0.889    0.889
step 13   -  0.881    0.881
step 14   -  0.874    0.875
step 15   -  0.867    0.869
step 16   -  0.860    0.862
step 17   -  0.855    0.857
step 18   -  0.850    0.854
step 19   -  0.845    0.849
step 20   -  0.840    0.845
step 21   -  0.837    0.842
step 22   -  0.833    0.840
step 23   -  0.831    0.839
step 24   -  0.827    0.836
=============================
Summary   -  21.446    21.478
V_y Shape is 
(75683, 1)
=========== SHAPE INPUT-OUTPUT MATRIXES =====================
shape x_training : (227131, 1001)  y_training: :  (227131, 24)
shape x_test     :  (75682, 1001)  y_test      :  (75682, 24)
shape x_val      :  (75683, 1001)  y_val       :  (75683, 24)
=============================================================
(1001,)
Model: "MLP"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_4 (InputLayer)        [(None, 1001)]            0         
                                                                 
 dense_12 (Dense)            (None, 1024)              1026048   
                                                                 
 elu_3 (ELU)                 (None, 1024)              0         
                                                                 
 dropout_9 (Dropout)         (None, 1024)              0         
                                                                 
 dense_13 (Dense)            (None, 512)               524800    
                                                                 
 dropout_10 (Dropout)        (None, 512)               0         
                                                                 
 dense_14 (Dense)            (None, 512)               262656    
                                                                 
 dropout_11 (Dropout)        (None, 512)               0         
                                                                 
 dense_15 (Dense)            (None, 24)                12312     
                                                                 
=================================================================
Total params: 1,825,816
Trainable params: 1,825,816
Non-trainable params: 0
_________________________________________________________________
Epoch 1/200
222/222 [==============================] - 2s 7ms/step - loss: 0.5430 - val_loss: 0.1190
Epoch 2/200
222/222 [==============================] - 2s 7ms/step - loss: 0.2693 - val_loss: 0.1100
Epoch 3/200
222/222 [==============================] - 2s 7ms/step - loss: 0.2196 - val_loss: 0.1039
Epoch 4/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1951 - val_loss: 0.0999
Epoch 5/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1794 - val_loss: 0.0976
Epoch 6/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1677 - val_loss: 0.0960
Epoch 7/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1591 - val_loss: 0.0965
Epoch 8/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1522 - val_loss: 0.0945
Epoch 9/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1457 - val_loss: 0.0927
Epoch 10/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1408 - val_loss: 0.0938
Epoch 11/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1361 - val_loss: 0.0950
Epoch 12/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1325 - val_loss: 0.0925
Epoch 13/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1293 - val_loss: 0.0946
Epoch 14/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1262 - val_loss: 0.0930
Epoch 15/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1234 - val_loss: 0.0895
Epoch 16/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1213 - val_loss: 0.0913
Epoch 17/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1189 - val_loss: 0.0909
Epoch 18/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1171 - val_loss: 0.0902
Epoch 19/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1154 - val_loss: 0.0886
Epoch 20/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1138 - val_loss: 0.0922
Epoch 21/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1123 - val_loss: 0.0903
Epoch 22/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1113 - val_loss: 0.0902
Epoch 23/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1100 - val_loss: 0.0903
Epoch 24/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1087 - val_loss: 0.0901
Epoch 25/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1080 - val_loss: 0.0904
Epoch 26/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1067 - val_loss: 0.0891
Epoch 27/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1059 - val_loss: 0.0900
Epoch 28/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1054 - val_loss: 0.0905
Epoch 29/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1043 - val_loss: 0.0899
Val_yp Shape is 
(75683, 24)
Results === Test == Validation ===== 143
=============================
step 1    -  0.987    0.986
step 2    -  0.980    0.978
step 3    -  0.971    0.969
step 4    -  0.961    0.959
step 5    -  0.950    0.948
step 6    -  0.940    0.939
step 7    -  0.930    0.929
step 8    -  0.920    0.919
step 9    -  0.910    0.910
step 10   -  0.901    0.902
step 11   -  0.893    0.894
step 12   -  0.885    0.887
step 13   -  0.878    0.881
step 14   -  0.871    0.875
step 15   -  0.865    0.869
step 16   -  0.859    0.864
step 17   -  0.853    0.858
step 18   -  0.848    0.854
step 19   -  0.843    0.850
step 20   -  0.839    0.846
step 21   -  0.834    0.842
step 22   -  0.829    0.838
step 23   -  0.824    0.834
step 24   -  0.819    0.830
=============================
Summary   -  21.388    21.464
V_y Shape is 
(75683, 1)
=========== SHAPE INPUT-OUTPUT MATRIXES =====================
shape x_training : (227130, 1008)  y_training: :  (227130, 24)
shape x_test     :  (75682, 1008)  y_test      :  (75682, 24)
shape x_val      :  (75682, 1008)  y_val       :  (75682, 24)
=============================================================
(1008,)
Model: "MLP"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_5 (InputLayer)        [(None, 1008)]            0         
                                                                 
 dense_16 (Dense)            (None, 1024)              1033216   
                                                                 
 elu_4 (ELU)                 (None, 1024)              0         
                                                                 
 dropout_12 (Dropout)        (None, 1024)              0         
                                                                 
 dense_17 (Dense)            (None, 512)               524800    
                                                                 
 dropout_13 (Dropout)        (None, 512)               0         
                                                                 
 dense_18 (Dense)            (None, 512)               262656    
                                                                 
 dropout_14 (Dropout)        (None, 512)               0         
                                                                 
 dense_19 (Dense)            (None, 24)                12312     
                                                                 
=================================================================
Total params: 1,832,984
Trainable params: 1,832,984
Non-trainable params: 0
_________________________________________________________________
Epoch 1/200
222/222 [==============================] - 2s 8ms/step - loss: 0.5367 - val_loss: 0.1193
Epoch 2/200
222/222 [==============================] - 2s 7ms/step - loss: 0.2672 - val_loss: 0.1071
Epoch 3/200
222/222 [==============================] - 2s 7ms/step - loss: 0.2185 - val_loss: 0.1049
Epoch 4/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1943 - val_loss: 0.0994
Epoch 5/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1787 - val_loss: 0.0963
Epoch 6/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1672 - val_loss: 0.0975
Epoch 7/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1584 - val_loss: 0.1021
Epoch 8/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1510 - val_loss: 0.0934
Epoch 9/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1451 - val_loss: 0.0925
Epoch 10/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1402 - val_loss: 0.0946
Epoch 11/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1359 - val_loss: 0.0927
Epoch 12/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1320 - val_loss: 0.0924
Epoch 13/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1286 - val_loss: 0.0921
Epoch 14/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1257 - val_loss: 0.0911
Epoch 15/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1228 - val_loss: 0.0906
Epoch 16/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1208 - val_loss: 0.0906
Epoch 17/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1185 - val_loss: 0.0905
Epoch 18/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1165 - val_loss: 0.0892
Epoch 19/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1150 - val_loss: 0.0895
Epoch 20/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1134 - val_loss: 0.0921
Epoch 21/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1120 - val_loss: 0.0905
Epoch 22/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1107 - val_loss: 0.0900
Epoch 23/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1096 - val_loss: 0.0899
Epoch 24/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1083 - val_loss: 0.0903
Epoch 25/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1072 - val_loss: 0.0897
Epoch 26/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1065 - val_loss: 0.0921
Epoch 27/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1055 - val_loss: 0.0908
Epoch 28/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1045 - val_loss: 0.0897
Val_yp Shape is 
(75682, 24)
Results === Test == Validation ===== 144
=============================
step 1    -  0.987    0.986
step 2    -  0.980    0.978
step 3    -  0.970    0.969
step 4    -  0.960    0.958
step 5    -  0.949    0.948
step 6    -  0.939    0.938
step 7    -  0.928    0.927
step 8    -  0.918    0.918
step 9    -  0.907    0.907
step 10   -  0.896    0.897
step 11   -  0.887    0.889
step 12   -  0.877    0.879
step 13   -  0.868    0.870
step 14   -  0.859    0.862
step 15   -  0.852    0.855
step 16   -  0.845    0.849
step 17   -  0.839    0.844
step 18   -  0.837    0.841
step 19   -  0.833    0.839
step 20   -  0.828    0.834
step 21   -  0.823    0.830
step 22   -  0.818    0.826
step 23   -  0.815    0.824
step 24   -  0.812    0.821
=============================
Summary   -  21.230    21.288
V_y Shape is 
(75682, 1)
=========== SHAPE INPUT-OUTPUT MATRIXES =====================
shape x_training : (227129, 1015)  y_training: :  (227129, 24)
shape x_test     :  (75681, 1015)  y_test      :  (75681, 24)
shape x_val      :  (75682, 1015)  y_val       :  (75682, 24)
=============================================================
(1015,)
Model: "MLP"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_6 (InputLayer)        [(None, 1015)]            0         
                                                                 
 dense_20 (Dense)            (None, 1024)              1040384   
                                                                 
 elu_5 (ELU)                 (None, 1024)              0         
                                                                 
 dropout_15 (Dropout)        (None, 1024)              0         
                                                                 
 dense_21 (Dense)            (None, 512)               524800    
                                                                 
 dropout_16 (Dropout)        (None, 512)               0         
                                                                 
 dense_22 (Dense)            (None, 512)               262656    
                                                                 
 dropout_17 (Dropout)        (None, 512)               0         
                                                                 
 dense_23 (Dense)            (None, 24)                12312     
                                                                 
=================================================================
Total params: 1,840,152
Trainable params: 1,840,152
Non-trainable params: 0
_________________________________________________________________
Epoch 1/200
222/222 [==============================] - 2s 7ms/step - loss: 0.5446 - val_loss: 0.1174
Epoch 2/200
222/222 [==============================] - 2s 7ms/step - loss: 0.2693 - val_loss: 0.1073
Epoch 3/200
222/222 [==============================] - 2s 7ms/step - loss: 0.2196 - val_loss: 0.1028
Epoch 4/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1955 - val_loss: 0.0991
Epoch 5/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1792 - val_loss: 0.1026
Epoch 6/200
222/222 [==============================] - 2s 11ms/step - loss: 0.1678 - val_loss: 0.0936
Epoch 7/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1589 - val_loss: 0.0970
Epoch 8/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1513 - val_loss: 0.0926
Epoch 9/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1453 - val_loss: 0.0926
Epoch 10/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1404 - val_loss: 0.0932
Epoch 11/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1359 - val_loss: 0.0946
Epoch 12/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1321 - val_loss: 0.0931
Epoch 13/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1286 - val_loss: 0.0916
Epoch 14/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1254 - val_loss: 0.0947
Epoch 15/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1231 - val_loss: 0.0909
Epoch 16/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1207 - val_loss: 0.0919
Epoch 17/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1185 - val_loss: 0.0905
Epoch 18/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1165 - val_loss: 0.0935
Epoch 19/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1148 - val_loss: 0.0920
Epoch 20/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1134 - val_loss: 0.0908
Epoch 21/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1121 - val_loss: 0.0913
Epoch 22/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1107 - val_loss: 0.0919
Epoch 23/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1095 - val_loss: 0.0907
Epoch 24/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1084 - val_loss: 0.0921
Epoch 25/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1072 - val_loss: 0.0893
Epoch 26/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1063 - val_loss: 0.0919
Epoch 27/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1054 - val_loss: 0.0907
Epoch 28/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1045 - val_loss: 0.0882
Epoch 29/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1038 - val_loss: 0.0899
Epoch 30/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1030 - val_loss: 0.0894
Epoch 31/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1021 - val_loss: 0.0886
Epoch 32/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1013 - val_loss: 0.0894
Epoch 33/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1005 - val_loss: 0.0885
Epoch 34/200
222/222 [==============================] - 2s 7ms/step - loss: 0.0997 - val_loss: 0.0882
Epoch 35/200
222/222 [==============================] - 2s 7ms/step - loss: 0.0990 - val_loss: 0.0908
Epoch 36/200
222/222 [==============================] - 2s 7ms/step - loss: 0.0980 - val_loss: 0.0883
Epoch 37/200
222/222 [==============================] - 2s 7ms/step - loss: 0.0978 - val_loss: 0.0897
Epoch 38/200
222/222 [==============================] - 2s 7ms/step - loss: 0.0966 - val_loss: 0.0887
Epoch 39/200
222/222 [==============================] - 2s 7ms/step - loss: 0.0959 - val_loss: 0.0903
Epoch 40/200
222/222 [==============================] - 2s 7ms/step - loss: 0.0951 - val_loss: 0.0921
Epoch 41/200
222/222 [==============================] - 2s 7ms/step - loss: 0.0949 - val_loss: 0.0907
Epoch 42/200
222/222 [==============================] - 2s 7ms/step - loss: 0.0942 - val_loss: 0.0908
Epoch 43/200
222/222 [==============================] - 2s 7ms/step - loss: 0.0935 - val_loss: 0.0877
Epoch 44/200
222/222 [==============================] - 2s 7ms/step - loss: 0.0930 - val_loss: 0.0886
Epoch 45/200
222/222 [==============================] - 2s 7ms/step - loss: 0.0922 - val_loss: 0.0912
Epoch 46/200
222/222 [==============================] - 2s 7ms/step - loss: 0.0916 - val_loss: 0.0902
Epoch 47/200
222/222 [==============================] - 2s 7ms/step - loss: 0.0909 - val_loss: 0.0900
Epoch 48/200
222/222 [==============================] - 2s 7ms/step - loss: 0.0902 - val_loss: 0.0894
Epoch 49/200
222/222 [==============================] - 2s 7ms/step - loss: 0.0899 - val_loss: 0.0891
Epoch 50/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0894 - val_loss: 0.0896
Epoch 51/200
222/222 [==============================] - 2s 7ms/step - loss: 0.0888 - val_loss: 0.0888
Epoch 52/200
222/222 [==============================] - 2s 7ms/step - loss: 0.0880 - val_loss: 0.0901
Epoch 53/200
222/222 [==============================] - 2s 7ms/step - loss: 0.0879 - val_loss: 0.0883
Val_yp Shape is 
(75682, 24)
Results === Test == Validation ===== 145
=============================
step 1    -  0.989    0.988
step 2    -  0.981    0.980
step 3    -  0.973    0.971
step 4    -  0.963    0.961
step 5    -  0.952    0.951
step 6    -  0.943    0.942
step 7    -  0.933    0.932
step 8    -  0.923    0.923
step 9    -  0.913    0.913
step 10   -  0.903    0.904
step 11   -  0.893    0.895
step 12   -  0.883    0.886
step 13   -  0.874    0.877
step 14   -  0.865    0.868
step 15   -  0.858    0.861
step 16   -  0.850    0.853
step 17   -  0.844    0.849
step 18   -  0.840    0.844
step 19   -  0.838    0.843
step 20   -  0.834    0.840
step 21   -  0.832    0.839
step 22   -  0.832    0.839
step 23   -  0.829    0.837
step 24   -  0.826    0.835
=============================
Summary   -  21.371    21.430
V_y Shape is 
(75682, 1)
=========== SHAPE INPUT-OUTPUT MATRIXES =====================
shape x_training : (227128, 1022)  y_training: :  (227128, 24)
shape x_test     :  (75681, 1022)  y_test      :  (75681, 24)
shape x_val      :  (75681, 1022)  y_val       :  (75681, 24)
=============================================================
(1022,)
Model: "MLP"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_7 (InputLayer)        [(None, 1022)]            0         
                                                                 
 dense_24 (Dense)            (None, 1024)              1047552   
                                                                 
 elu_6 (ELU)                 (None, 1024)              0         
                                                                 
 dropout_18 (Dropout)        (None, 1024)              0         
                                                                 
 dense_25 (Dense)            (None, 512)               524800    
                                                                 
 dropout_19 (Dropout)        (None, 512)               0         
                                                                 
 dense_26 (Dense)            (None, 512)               262656    
                                                                 
 dropout_20 (Dropout)        (None, 512)               0         
                                                                 
 dense_27 (Dense)            (None, 24)                12312     
                                                                 
=================================================================
Total params: 1,847,320
Trainable params: 1,847,320
Non-trainable params: 0
_________________________________________________________________
Epoch 1/200
222/222 [==============================] - 2s 8ms/step - loss: 0.5435 - val_loss: 0.1187
Epoch 2/200
222/222 [==============================] - 2s 7ms/step - loss: 0.2684 - val_loss: 0.1076
Epoch 3/200
222/222 [==============================] - 1s 6ms/step - loss: 0.2193 - val_loss: 0.1033
Epoch 4/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1947 - val_loss: 0.0987
Epoch 5/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1787 - val_loss: 0.0964
Epoch 6/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1674 - val_loss: 0.0954
Epoch 7/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1584 - val_loss: 0.0934
Epoch 8/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1512 - val_loss: 0.0978
Epoch 9/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1452 - val_loss: 0.0928
Epoch 10/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1401 - val_loss: 0.0946
Epoch 11/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1356 - val_loss: 0.0920
Epoch 12/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1319 - val_loss: 0.0913
Epoch 13/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1286 - val_loss: 0.0924
Epoch 14/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1258 - val_loss: 0.0912
Epoch 15/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1230 - val_loss: 0.0907
Epoch 16/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1205 - val_loss: 0.0902
Epoch 17/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1185 - val_loss: 0.0890
Epoch 18/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1165 - val_loss: 0.0893
Epoch 19/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1148 - val_loss: 0.0917
Epoch 20/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1134 - val_loss: 0.0900
Epoch 21/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1118 - val_loss: 0.0899
Epoch 22/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1105 - val_loss: 0.0909
Epoch 23/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1092 - val_loss: 0.0919
Epoch 24/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1082 - val_loss: 0.0896
Epoch 25/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1073 - val_loss: 0.0908
Epoch 26/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1063 - val_loss: 0.0900
Epoch 27/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1054 - val_loss: 0.0930

=========== SHAPE INPUT-OUTPUT MATRIXES =====================
shape x_training : (227128, 1022)  y_training: :  (227128, 24)
shape x_test     :  (75681, 1022)  y_test      :  (75681, 24)
shape x_val      :  (75681, 1022)  y_val       :  (75681, 24)
=============================================================
(1022,)
Model: "MLP"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_1 (InputLayer)        [(None, 1022)]            0         
                                                                 
 dense (Dense)               (None, 1024)              1047552   
                                                                 
 elu (ELU)                   (None, 1024)              0         
                                                                 
 dropout (Dropout)           (None, 1024)              0         
                                                                 
 dense_1 (Dense)             (None, 512)               524800    
                                                                 
 dropout_1 (Dropout)         (None, 512)               0         
                                                                 
 dense_2 (Dense)             (None, 512)               262656    
                                                                 
 dropout_2 (Dropout)         (None, 512)               0         
                                                                 
 dense_3 (Dense)             (None, 24)                12312     
                                                                 
=================================================================
Total params: 1,847,320
Trainable params: 1,847,320
Non-trainable params: 0
_________________________________________________________________
Epoch 1/200
222/222 [==============================] - 3s 7ms/step - loss: 0.5413 - val_loss: 0.1174
Epoch 2/200
222/222 [==============================] - 1s 6ms/step - loss: 0.2697 - val_loss: 0.1066
Epoch 3/200
222/222 [==============================] - 1s 6ms/step - loss: 0.2200 - val_loss: 0.1029
Epoch 4/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1956 - val_loss: 0.1017
Epoch 5/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1794 - val_loss: 0.1005
Epoch 6/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1679 - val_loss: 0.0960
Epoch 7/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1591 - val_loss: 0.0959
Epoch 8/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1514 - val_loss: 0.0952
Epoch 9/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1453 - val_loss: 0.0993
Epoch 10/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1404 - val_loss: 0.0919
Epoch 11/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1360 - val_loss: 0.0943
Epoch 12/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1320 - val_loss: 0.0919
Epoch 13/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1289 - val_loss: 0.0928
Epoch 14/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1256 - val_loss: 0.0925
Epoch 15/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1231 - val_loss: 0.0916
Epoch 16/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1206 - val_loss: 0.0903
Epoch 17/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1185 - val_loss: 0.0916
Epoch 18/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1168 - val_loss: 0.0901
Epoch 19/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1149 - val_loss: 0.0908
Epoch 20/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1135 - val_loss: 0.0897
Epoch 21/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1121 - val_loss: 0.0920
Epoch 22/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1107 - val_loss: 0.0886
Epoch 23/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1095 - val_loss: 0.0892
Epoch 24/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1083 - val_loss: 0.0921
Epoch 25/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1073 - val_loss: 0.0891
Epoch 26/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1061 - val_loss: 0.0911
Epoch 27/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1054 - val_loss: 0.0897
Epoch 28/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1043 - val_loss: 0.0878
Epoch 29/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1034 - val_loss: 0.0885
Epoch 30/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1029 - val_loss: 0.0917
Epoch 31/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1019 - val_loss: 0.0910
Epoch 32/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1013 - val_loss: 0.0896
Epoch 33/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1005 - val_loss: 0.0903
Epoch 34/200
222/222 [==============================] - 2s 7ms/step - loss: 0.0997 - val_loss: 0.0886
Epoch 35/200
222/222 [==============================] - 2s 7ms/step - loss: 0.0987 - val_loss: 0.0889
Epoch 36/200
222/222 [==============================] - 2s 7ms/step - loss: 0.0981 - val_loss: 0.0888
Epoch 37/200
222/222 [==============================] - 2s 7ms/step - loss: 0.0975 - val_loss: 0.0924
Epoch 38/200
222/222 [==============================] - 2s 7ms/step - loss: 0.0965 - val_loss: 0.0895
Val_yp Shape is 
(75681, 24)
Results === Test == Validation ===== 146
=============================
step 1    -  0.988    0.987
step 2    -  0.980    0.979
step 3    -  0.971    0.969
step 4    -  0.962    0.960
step 5    -  0.951    0.950
step 6    -  0.941    0.940
step 7    -  0.931    0.930
step 8    -  0.921    0.921
step 9    -  0.911    0.912
step 10   -  0.902    0.903
step 11   -  0.894    0.896
step 12   -  0.886    0.888
step 13   -  0.879    0.881
step 14   -  0.872    0.875
step 15   -  0.867    0.870
step 16   -  0.862    0.865
step 17   -  0.858    0.861
step 18   -  0.853    0.857
step 19   -  0.848    0.853
step 20   -  0.844    0.848
step 21   -  0.839    0.845
step 22   -  0.836    0.842
step 23   -  0.830    0.837
step 24   -  0.826    0.834
=============================
Summary   -  21.454    21.501
V_y Shape is 
(75681, 1)
=========== SHAPE INPUT-OUTPUT MATRIXES =====================
shape x_training : (227126, 1036)  y_training: :  (227126, 24)
shape x_test     :  (75680, 1036)  y_test      :  (75680, 24)
shape x_val      :  (75680, 1036)  y_val       :  (75680, 24)
=============================================================
(1036,)
Model: "MLP"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_2 (InputLayer)        [(None, 1036)]            0         
                                                                 
 dense_4 (Dense)             (None, 1024)              1061888   
                                                                 
 elu_1 (ELU)                 (None, 1024)              0         
                                                                 
 dropout_3 (Dropout)         (None, 1024)              0         
                                                                 
 dense_5 (Dense)             (None, 512)               524800    
                                                                 
 dropout_4 (Dropout)         (None, 512)               0         
                                                                 
 dense_6 (Dense)             (None, 512)               262656    
                                                                 
 dropout_5 (Dropout)         (None, 512)               0         
                                                                 
 dense_7 (Dense)             (None, 24)                12312     
                                                                 
=================================================================
Total params: 1,861,656
Trainable params: 1,861,656
Non-trainable params: 0
_________________________________________________________________
Epoch 1/200
222/222 [==============================] - 2s 7ms/step - loss: 0.5521 - val_loss: 0.1169
Epoch 2/200
222/222 [==============================] - 2s 7ms/step - loss: 0.2713 - val_loss: 0.1059
Epoch 3/200
222/222 [==============================] - 2s 7ms/step - loss: 0.2211 - val_loss: 0.1035
Epoch 4/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1967 - val_loss: 0.1034
Epoch 5/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1803 - val_loss: 0.1016
Epoch 6/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1686 - val_loss: 0.0963
Epoch 7/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1593 - val_loss: 0.0956
Epoch 8/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1526 - val_loss: 0.0931
Epoch 9/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1462 - val_loss: 0.0927
Epoch 10/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1409 - val_loss: 0.0919
Epoch 11/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1363 - val_loss: 0.0936
Epoch 12/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1324 - val_loss: 0.0939
Epoch 13/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1288 - val_loss: 0.0944
Epoch 14/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1262 - val_loss: 0.0902
Epoch 15/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1231 - val_loss: 0.0911
Epoch 16/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1209 - val_loss: 0.0908
Epoch 17/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1187 - val_loss: 0.0920
Epoch 18/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1169 - val_loss: 0.0886
Epoch 19/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1148 - val_loss: 0.0907
Epoch 20/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1136 - val_loss: 0.0901
Epoch 21/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1117 - val_loss: 0.0917
Epoch 22/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1105 - val_loss: 0.0895
Epoch 23/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1094 - val_loss: 0.0926
Epoch 24/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1082 - val_loss: 0.0915
Epoch 25/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1071 - val_loss: 0.0902
Epoch 26/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1063 - val_loss: 0.0889
Epoch 27/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1053 - val_loss: 0.0898
Epoch 28/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1041 - val_loss: 0.0887
Val_yp Shape is 
(75680, 24)
Results === Test == Validation ===== 148
=============================
step 1    -  0.988    0.987
step 2    -  0.980    0.979
step 3    -  0.971    0.969
step 4    -  0.960    0.958
step 5    -  0.950    0.947
step 6    -  0.939    0.937
step 7    -  0.928    0.926
step 8    -  0.917    0.916
step 9    -  0.908    0.908
step 10   -  0.899    0.899
step 11   -  0.890    0.890
step 12   -  0.882    0.883
step 13   -  0.875    0.876
step 14   -  0.869    0.871
step 15   -  0.864    0.865
step 16   -  0.860    0.862
step 17   -  0.855    0.858
step 18   -  0.851    0.855
step 19   -  0.847    0.851
step 20   -  0.842    0.847
step 21   -  0.838    0.843
step 22   -  0.832    0.839
step 23   -  0.825    0.833
step 24   -  0.819    0.827
=============================
Summary   -  21.389    21.426
V_y Shape is 
(75680, 1)
=========== SHAPE INPUT-OUTPUT MATRIXES =====================
shape x_training : (227124, 1050)  y_training: :  (227124, 24)
shape x_test     :  (75679, 1050)  y_test      :  (75679, 24)
shape x_val      :  (75679, 1050)  y_val       :  (75679, 24)
=============================================================
(1050,)
Model: "MLP"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_3 (InputLayer)        [(None, 1050)]            0         
                                                                 
 dense_8 (Dense)             (None, 1024)              1076224   
                                                                 
 elu_2 (ELU)                 (None, 1024)              0         
                                                                 
 dropout_6 (Dropout)         (None, 1024)              0         
                                                                 
 dense_9 (Dense)             (None, 512)               524800    
                                                                 
 dropout_7 (Dropout)         (None, 512)               0         
                                                                 
 dense_10 (Dense)            (None, 512)               262656    
                                                                 
 dropout_8 (Dropout)         (None, 512)               0         
                                                                 
 dense_11 (Dense)            (None, 24)                12312     
                                                                 
=================================================================
Total params: 1,875,992
Trainable params: 1,875,992
Non-trainable params: 0
_________________________________________________________________
Epoch 1/200
222/222 [==============================] - 2s 8ms/step - loss: 0.5378 - val_loss: 0.1224
Epoch 2/200
222/222 [==============================] - 2s 7ms/step - loss: 0.2693 - val_loss: 0.1101
Epoch 3/200
222/222 [==============================] - 2s 7ms/step - loss: 0.2196 - val_loss: 0.1052
Epoch 4/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1950 - val_loss: 0.1046
Epoch 5/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1790 - val_loss: 0.0973
Epoch 6/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1674 - val_loss: 0.0964
Epoch 7/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1584 - val_loss: 0.0974
Epoch 8/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1512 - val_loss: 0.0934
Epoch 9/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1449 - val_loss: 0.0973
Epoch 10/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1401 - val_loss: 0.0940
Epoch 11/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1355 - val_loss: 0.0927
Epoch 12/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1316 - val_loss: 0.0954
Epoch 13/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1282 - val_loss: 0.0913
Epoch 14/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1249 - val_loss: 0.0948
Epoch 15/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1225 - val_loss: 0.0910
Epoch 16/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1200 - val_loss: 0.0923
Epoch 17/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1181 - val_loss: 0.0918
Epoch 18/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1160 - val_loss: 0.0883
Epoch 19/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1143 - val_loss: 0.0957
Epoch 20/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1126 - val_loss: 0.0901
Epoch 21/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1112 - val_loss: 0.0909
Epoch 22/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1100 - val_loss: 0.0897
Epoch 23/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1085 - val_loss: 0.0916
Epoch 24/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1077 - val_loss: 0.0898
Epoch 25/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1064 - val_loss: 0.0907
Epoch 26/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1057 - val_loss: 0.0919
Epoch 27/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1047 - val_loss: 0.0892
Epoch 28/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1036 - val_loss: 0.0904
Val_yp Shape is 
(75679, 24)
Results === Test == Validation ===== 150
=============================
step 1    -  0.987    0.986
step 2    -  0.980    0.978
step 3    -  0.971    0.969
step 4    -  0.961    0.959
step 5    -  0.950    0.948
step 6    -  0.939    0.938
step 7    -  0.929    0.927
step 8    -  0.918    0.918
step 9    -  0.909    0.909
step 10   -  0.900    0.900
step 11   -  0.892    0.893
step 12   -  0.885    0.887
step 13   -  0.879    0.881
step 14   -  0.873    0.875
step 15   -  0.869    0.871
step 16   -  0.863    0.866
step 17   -  0.858    0.862
step 18   -  0.854    0.858
step 19   -  0.849    0.854
step 20   -  0.845    0.850
step 21   -  0.840    0.846
step 22   -  0.836    0.843
step 23   -  0.831    0.839
step 24   -  0.826    0.834
=============================
Summary   -  21.442    21.490
V_y Shape is 
(75679, 1)
=========== SHAPE INPUT-OUTPUT MATRIXES =====================
shape x_training : (227122, 1064)  y_training: :  (227122, 24)
shape x_test     :  (75678, 1064)  y_test      :  (75678, 24)
shape x_val      :  (75678, 1064)  y_val       :  (75678, 24)
=============================================================
(1064,)
Model: "MLP"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_4 (InputLayer)        [(None, 1064)]            0         
                                                                 
 dense_12 (Dense)            (None, 1024)              1090560   
                                                                 
 elu_3 (ELU)                 (None, 1024)              0         
                                                                 
 dropout_9 (Dropout)         (None, 1024)              0         
                                                                 
 dense_13 (Dense)            (None, 512)               524800    
                                                                 
 dropout_10 (Dropout)        (None, 512)               0         
                                                                 
 dense_14 (Dense)            (None, 512)               262656    
                                                                 
 dropout_11 (Dropout)        (None, 512)               0         
                                                                 
 dense_15 (Dense)            (None, 24)                12312     
                                                                 
=================================================================
Total params: 1,890,328
Trainable params: 1,890,328
Non-trainable params: 0
_________________________________________________________________
Epoch 1/200
222/222 [==============================] - 2s 8ms/step - loss: 0.5443 - val_loss: 0.1191
Epoch 2/200
222/222 [==============================] - 2s 7ms/step - loss: 0.2700 - val_loss: 0.1108
Epoch 3/200
222/222 [==============================] - 2s 7ms/step - loss: 0.2202 - val_loss: 0.1027
Epoch 4/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1956 - val_loss: 0.1022
Epoch 5/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1798 - val_loss: 0.0977
Epoch 6/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1681 - val_loss: 0.0980
Epoch 7/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1589 - val_loss: 0.0957
Epoch 8/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1513 - val_loss: 0.0957
Epoch 9/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1452 - val_loss: 0.0953
Epoch 10/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1401 - val_loss: 0.0937
Epoch 11/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1354 - val_loss: 0.0931
Epoch 12/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1319 - val_loss: 0.0930
Epoch 13/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1285 - val_loss: 0.0940
Epoch 14/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1251 - val_loss: 0.0937
Epoch 15/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1228 - val_loss: 0.0914
Epoch 16/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1200 - val_loss: 0.0919
Epoch 17/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1180 - val_loss: 0.0940
Epoch 18/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1161 - val_loss: 0.0894
Epoch 19/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1143 - val_loss: 0.0917
Epoch 20/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1128 - val_loss: 0.0903
Epoch 21/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1114 - val_loss: 0.0899
Epoch 22/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1099 - val_loss: 0.0900
Epoch 23/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1087 - val_loss: 0.0926
Epoch 24/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1076 - val_loss: 0.0909
Epoch 25/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1066 - val_loss: 0.0897
Epoch 26/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1054 - val_loss: 0.0904
Epoch 27/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1044 - val_loss: 0.0896
Epoch 28/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1034 - val_loss: 0.0904
Val_yp Shape is 
(75678, 24)
Results === Test == Validation ===== 152
=============================
step 1    -  0.987    0.986
step 2    -  0.980    0.978
step 3    -  0.971    0.969
step 4    -  0.960    0.959
step 5    -  0.950    0.948
step 6    -  0.939    0.937
step 7    -  0.928    0.927
step 8    -  0.917    0.916
step 9    -  0.906    0.906
step 10   -  0.897    0.898
step 11   -  0.888    0.889
step 12   -  0.881    0.882
step 13   -  0.875    0.877
step 14   -  0.869    0.871
step 15   -  0.862    0.865
step 16   -  0.857    0.860
step 17   -  0.852    0.856
step 18   -  0.848    0.852
step 19   -  0.843    0.848
step 20   -  0.838    0.844
step 21   -  0.834    0.841
step 22   -  0.830    0.838
step 23   -  0.826    0.834
step 24   -  0.822    0.831
=============================
Summary   -  21.359    21.410
V_y Shape is 
(75678, 1)
=========== SHAPE INPUT-OUTPUT MATRIXES =====================
shape x_training : (227120, 1078)  y_training: :  (227120, 24)
shape x_test     :  (75677, 1078)  y_test      :  (75677, 24)
shape x_val      :  (75677, 1078)  y_val       :  (75677, 24)
=============================================================
(1078,)
Model: "MLP"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_5 (InputLayer)        [(None, 1078)]            0         
                                                                 
 dense_16 (Dense)            (None, 1024)              1104896   
                                                                 
 elu_4 (ELU)                 (None, 1024)              0         
                                                                 
 dropout_12 (Dropout)        (None, 1024)              0         
                                                                 
 dense_17 (Dense)            (None, 512)               524800    
                                                                 
 dropout_13 (Dropout)        (None, 512)               0         
                                                                 
 dense_18 (Dense)            (None, 512)               262656    
                                                                 
 dropout_14 (Dropout)        (None, 512)               0         
                                                                 
 dense_19 (Dense)            (None, 24)                12312     
                                                                 
=================================================================
Total params: 1,904,664
Trainable params: 1,904,664
Non-trainable params: 0



=========== SHAPE INPUT-OUTPUT MATRIXES =====================
shape x_training : (227120, 1078)  y_training: :  (227120, 24)
shape x_test     :  (75677, 1078)  y_test      :  (75677, 24)
shape x_val      :  (75677, 1078)  y_val       :  (75677, 24)
=============================================================
(1078,)
Model: "MLP"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_1 (InputLayer)        [(None, 1078)]            0         
                                                                 
 dense (Dense)               (None, 1024)              1104896   
                                                                 
 elu (ELU)                   (None, 1024)              0         
                                                                 
 dropout (Dropout)           (None, 1024)              0         
                                                                 
 dense_1 (Dense)             (None, 512)               524800    
                                                                 
 dropout_1 (Dropout)         (None, 512)               0         
                                                                 
 dense_2 (Dense)             (None, 512)               262656    
                                                                 
 dropout_2 (Dropout)         (None, 512)               0         
                                                                 
 dense_3 (Dense)             (None, 24)                12312     
                                                                 
=================================================================
Total params: 1,904,664
Trainable params: 1,904,664
Non-trainable params: 0
_________________________________________________________________
Epoch 1/200
222/222 [==============================] - 3s 7ms/step - loss: 0.5531 - val_loss: 0.1197
Epoch 2/200
222/222 [==============================] - 1s 6ms/step - loss: 0.2721 - val_loss: 0.1110
Epoch 3/200
222/222 [==============================] - 1s 6ms/step - loss: 0.2215 - val_loss: 0.1021
Epoch 4/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1963 - val_loss: 0.1000
Epoch 5/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1805 - val_loss: 0.0993
Epoch 6/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1684 - val_loss: 0.0963
Epoch 7/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1591 - val_loss: 0.0958
Epoch 8/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1518 - val_loss: 0.0960
Epoch 9/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1455 - val_loss: 0.0928
Epoch 10/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1401 - val_loss: 0.0933
Epoch 11/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1356 - val_loss: 0.0935
Epoch 12/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1317 - val_loss: 0.0938
Epoch 13/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1281 - val_loss: 0.0973
Epoch 14/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1251 - val_loss: 0.0967
Epoch 15/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1226 - val_loss: 0.0924
Epoch 16/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1202 - val_loss: 0.0902
Epoch 17/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1180 - val_loss: 0.0929
Epoch 18/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1157 - val_loss: 0.0914
Epoch 19/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1142 - val_loss: 0.0902
Epoch 20/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1127 - val_loss: 0.0912
Epoch 21/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1108 - val_loss: 0.0905
Epoch 22/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1097 - val_loss: 0.0915
Epoch 23/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1084 - val_loss: 0.0907
Epoch 24/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1073 - val_loss: 0.0912
Epoch 25/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1059 - val_loss: 0.0895
Epoch 26/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1051 - val_loss: 0.0891
Epoch 27/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1043 - val_loss: 0.0902
Epoch 28/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1031 - val_loss: 0.0885
Epoch 29/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1023 - val_loss: 0.0919
Epoch 30/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1017 - val_loss: 0.0910
Epoch 31/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1004 - val_loss: 0.0905
Epoch 32/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0998 - val_loss: 0.0910
Epoch 33/200
222/222 [==============================] - 2s 7ms/step - loss: 0.0992 - val_loss: 0.0905
Epoch 34/200
222/222 [==============================] - 2s 7ms/step - loss: 0.0981 - val_loss: 0.0891
Epoch 35/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0975 - val_loss: 0.0919
Epoch 36/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0965 - val_loss: 0.0892
Epoch 37/200
222/222 [==============================] - 2s 7ms/step - loss: 0.0958 - val_loss: 0.0895
Epoch 38/200
222/222 [==============================] - 2s 7ms/step - loss: 0.0953 - val_loss: 0.0897
Val_yp Shape is 
(75677, 24)
Results === Test == Validation ===== 154
=============================
step 1    -  0.987    0.986
step 2    -  0.980    0.979
step 3    -  0.971    0.970
step 4    -  0.962    0.960
step 5    -  0.952    0.951
step 6    -  0.941    0.940
step 7    -  0.931    0.930
step 8    -  0.921    0.921
step 9    -  0.911    0.911
step 10   -  0.901    0.902
step 11   -  0.893    0.894
step 12   -  0.883    0.886
step 13   -  0.876    0.879
step 14   -  0.870    0.873
step 15   -  0.864    0.868
step 16   -  0.858    0.863
step 17   -  0.854    0.858
step 18   -  0.849    0.854
step 19   -  0.845    0.851
step 20   -  0.840    0.847
step 21   -  0.835    0.843
step 22   -  0.832    0.840
step 23   -  0.828    0.838
step 24   -  0.823    0.834
=============================
Summary   -  21.409    21.478
V_y Shape is 
(75677, 1)
=========== SHAPE INPUT-OUTPUT MATRIXES =====================
shape x_training : (227118, 1092)  y_training: :  (227118, 24)
shape x_test     :  (75676, 1092)  y_test      :  (75676, 24)
shape x_val      :  (75676, 1092)  y_val       :  (75676, 24)
=============================================================
(1092,)
Model: "MLP"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_2 (InputLayer)        [(None, 1092)]            0         
                                                                 
 dense_4 (Dense)             (None, 1024)              1119232   
                                                                 
 elu_1 (ELU)                 (None, 1024)              0         
                                                                 
 dropout_3 (Dropout)         (None, 1024)              0         
                                                                 
 dense_5 (Dense)             (None, 512)               524800    
                                                                 
 dropout_4 (Dropout)         (None, 512)               0         
                                                                 
 dense_6 (Dense)             (None, 512)               262656    
                                                                 
 dropout_5 (Dropout)         (None, 512)               0         
                                                                 
 dense_7 (Dense)             (None, 24)                12312     
                                                                 
=================================================================
Total params: 1,919,000
Trainable params: 1,919,000
Non-trainable params: 0
_________________________________________________________________
Epoch 1/200
222/222 [==============================] - 2s 8ms/step - loss: 0.5502 - val_loss: 0.1197
Epoch 2/200
222/222 [==============================] - 2s 7ms/step - loss: 0.2723 - val_loss: 0.1083
Epoch 3/200
222/222 [==============================] - 2s 7ms/step - loss: 0.2216 - val_loss: 0.1033
Epoch 4/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1968 - val_loss: 0.1011
Epoch 5/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1803 - val_loss: 0.0979
Epoch 6/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1680 - val_loss: 0.0967
Epoch 7/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1591 - val_loss: 0.0966
Epoch 8/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1518 - val_loss: 0.0923
Epoch 9/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1455 - val_loss: 0.0923
Epoch 10/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1401 - val_loss: 0.0921
Epoch 11/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1354 - val_loss: 0.0924
Epoch 12/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1317 - val_loss: 0.0910
Epoch 13/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1283 - val_loss: 0.0938
Epoch 14/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1251 - val_loss: 0.0915
Epoch 15/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1222 - val_loss: 0.0894
Epoch 16/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1197 - val_loss: 0.0905
Epoch 17/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1179 - val_loss: 0.0927
Epoch 18/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1157 - val_loss: 0.0911
Epoch 19/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1139 - val_loss: 0.0910
Epoch 20/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1122 - val_loss: 0.0911
Epoch 21/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1109 - val_loss: 0.0914
Epoch 22/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1095 - val_loss: 0.0900
Epoch 23/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1081 - val_loss: 0.0901
Epoch 24/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1072 - val_loss: 0.0929
Epoch 25/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1060 - val_loss: 0.0931
Val_yp Shape is 
(75676, 24)
Results === Test == Validation ===== 156
=============================
step 1    -  0.985    0.984
step 2    -  0.978    0.977
step 3    -  0.969    0.968
step 4    -  0.960    0.958
step 5    -  0.950    0.949
step 6    -  0.940    0.939
step 7    -  0.931    0.930
step 8    -  0.921    0.921
step 9    -  0.912    0.912
step 10   -  0.903    0.904
step 11   -  0.895    0.897
step 12   -  0.888    0.890
step 13   -  0.882    0.884
step 14   -  0.877    0.879
step 15   -  0.871    0.874
step 16   -  0.866    0.869
step 17   -  0.861    0.865
step 18   -  0.858    0.862
step 19   -  0.854    0.859
step 20   -  0.851    0.856
step 21   -  0.847    0.853
step 22   -  0.843    0.850
step 23   -  0.839    0.846
step 24   -  0.834    0.842
=============================
Summary   -  21.515    21.570
V_y Shape is 
(75676, 1)
=========== SHAPE INPUT-OUTPUT MATRIXES =====================
shape x_training : (227116, 1106)  y_training: :  (227116, 24)
shape x_test     :  (75675, 1106)  y_test      :  (75675, 24)
shape x_val      :  (75675, 1106)  y_val       :  (75675, 24)
=============================================================
(1106,)
Model: "MLP"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_3 (InputLayer)        [(None, 1106)]            0         
                                                                 
 dense_8 (Dense)             (None, 1024)              1133568   
                                                                 
 elu_2 (ELU)                 (None, 1024)              0         
                                                                 
 dropout_6 (Dropout)         (None, 1024)              0         
                                                                 
 dense_9 (Dense)             (None, 512)               524800    
                                                                 
 dropout_7 (Dropout)         (None, 512)               0         
                                                                 
 dense_10 (Dense)            (None, 512)               262656    
                                                                 
 dropout_8 (Dropout)         (None, 512)               0         
                                                                 
 dense_11 (Dense)            (None, 24)                12312     
                                                                 
=================================================================
Total params: 1,933,336
Trainable params: 1,933,336
Non-trainable params: 0
_________________________________________________________________
Epoch 1/200
222/222 [==============================] - 2s 7ms/step - loss: 0.5513 - val_loss: 0.1182
Epoch 2/200
222/222 [==============================] - 1s 6ms/step - loss: 0.2723 - val_loss: 0.1072
Epoch 3/200
222/222 [==============================] - 1s 6ms/step - loss: 0.2216 - val_loss: 0.1012
Epoch 4/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1961 - val_loss: 0.0982
Epoch 5/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1799 - val_loss: 0.0984
Epoch 6/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1680 - val_loss: 0.0995
Epoch 7/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1590 - val_loss: 0.0935
Epoch 8/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1513 - val_loss: 0.0952
Epoch 9/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1454 - val_loss: 0.0950
Epoch 10/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1399 - val_loss: 0.0922
Epoch 11/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1352 - val_loss: 0.0927
Epoch 12/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1312 - val_loss: 0.0917
Epoch 13/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1281 - val_loss: 0.0934
Epoch 14/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1252 - val_loss: 0.0910
Epoch 15/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1220 - val_loss: 0.0911
Epoch 16/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1197 - val_loss: 0.0893
Epoch 17/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1174 - val_loss: 0.0901
Epoch 18/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1154 - val_loss: 0.0906
Epoch 19/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1134 - val_loss: 0.0933
Epoch 20/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1120 - val_loss: 0.0901
Epoch 21/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1106 - val_loss: 0.0899
Epoch 22/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1092 - val_loss: 0.0900
Epoch 23/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1078 - val_loss: 0.0901
Epoch 24/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1063 - val_loss: 0.0920
Epoch 25/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1054 - val_loss: 0.0897
Epoch 26/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1046 - val_loss: 0.0909
Val_yp Shape is 
(75675, 24)
Results === Test == Validation ===== 158
=============================
step 1    -  0.987    0.986
step 2    -  0.980    0.978
step 3    -  0.970    0.968
step 4    -  0.961    0.958
step 5    -  0.950    0.948
step 6    -  0.940    0.937
step 7    -  0.930    0.927
step 8    -  0.920    0.918
step 9    -  0.911    0.909
step 10   -  0.902    0.901
step 11   -  0.894    0.894
step 12   -  0.888    0.888
step 13   -  0.881    0.882
step 14   -  0.875    0.876
step 15   -  0.869    0.871
step 16   -  0.864    0.865
step 17   -  0.859    0.861
step 18   -  0.853    0.856
step 19   -  0.849    0.852
step 20   -  0.845    0.848
step 21   -  0.840    0.845
step 22   -  0.836    0.841
step 23   -  0.831    0.837
step 24   -  0.826    0.833
=============================
Summary   -  21.461    21.479
V_y Shape is 
(75675, 1)
=========== SHAPE INPUT-OUTPUT MATRIXES =====================
shape x_training : (227114, 1120)  y_training: :  (227114, 24)
shape x_test     :  (75674, 1120)  y_test      :  (75674, 24)
shape x_val      :  (75674, 1120)  y_val       :  (75674, 24)
=============================================================
(1120,)
Model: "MLP"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_4 (InputLayer)        [(None, 1120)]            0         
                                                                 
 dense_12 (Dense)            (None, 1024)              1147904   
                                                                 
 elu_3 (ELU)                 (None, 1024)              0         
                                                                 
 dropout_9 (Dropout)         (None, 1024)              0         
                                                                 
 dense_13 (Dense)            (None, 512)               524800    
                                                                 
 dropout_10 (Dropout)        (None, 512)               0         
                                                                 
 dense_14 (Dense)            (None, 512)               262656    
                                                                 
 dropout_11 (Dropout)        (None, 512)               0         
                                                                 
 dense_15 (Dense)            (None, 24)                12312     
                                                                 
=================================================================
Total params: 1,947,672
Trainable params: 1,947,672
Non-trainable params: 0
_________________________________________________________________
Epoch 1/200
222/222 [==============================] - 2s 7ms/step - loss: 0.5482 - val_loss: 0.1188
Epoch 2/200
222/222 [==============================] - 1s 7ms/step - loss: 0.2724 - val_loss: 0.1089
Epoch 3/200
222/222 [==============================] - 1s 6ms/step - loss: 0.2211 - val_loss: 0.1044
Epoch 4/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1959 - val_loss: 0.0998
Epoch 5/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1799 - val_loss: 0.0996
Epoch 6/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1681 - val_loss: 0.0984
Epoch 7/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1587 - val_loss: 0.0964
Epoch 8/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1510 - val_loss: 0.0998
Epoch 9/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1450 - val_loss: 0.0948
Epoch 10/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1397 - val_loss: 0.0921
Epoch 11/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1349 - val_loss: 0.0912
Epoch 12/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1310 - val_loss: 0.0937
Epoch 13/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1273 - val_loss: 0.0922
Epoch 14/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1245 - val_loss: 0.0917
Epoch 15/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1217 - val_loss: 0.0947
Epoch 16/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1193 - val_loss: 0.0925
Epoch 17/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1167 - val_loss: 0.0909
Epoch 18/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1150 - val_loss: 0.0908
Epoch 19/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1131 - val_loss: 0.0906
Epoch 20/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1114 - val_loss: 0.0918
Epoch 21/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1101 - val_loss: 0.0911
Epoch 22/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1087 - val_loss: 0.0898
Epoch 23/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1072 - val_loss: 0.0933
Epoch 24/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1062 - val_loss: 0.0889
Epoch 25/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1050 - val_loss: 0.0904
Epoch 26/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1040 - val_loss: 0.0916
Epoch 27/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1033 - val_loss: 0.0913
Epoch 28/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1023 - val_loss: 0.0906
Epoch 29/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1013 - val_loss: 0.0897
Epoch 30/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1004 - val_loss: 0.0901
Epoch 31/200
222/222 [==============================] - 2s 7ms/step - loss: 0.0993 - val_loss: 0.0910
Epoch 32/200
222/222 [==============================] - 2s 7ms/step - loss: 0.0989 - val_loss: 0.0909
Epoch 33/200
222/222 [==============================] - 2s 7ms/step - loss: 0.0977 - val_loss: 0.0901
Epoch 34/200
222/222 [==============================] - 2s 7ms/step - loss: 0.0971 - val_loss: 0.0910
Val_yp Shape is 
(75674, 24)
Results === Test == Validation ===== 160
=============================
step 1    -  0.987    0.986
step 2    -  0.980    0.978
step 3    -  0.970    0.968
step 4    -  0.960    0.957
step 5    -  0.948    0.946
step 6    -  0.936    0.934
step 7    -  0.925    0.923
step 8    -  0.913    0.912
step 9    -  0.902    0.901
step 10   -  0.891    0.891
step 11   -  0.882    0.882
step 12   -  0.875    0.876
step 13   -  0.868    0.870
step 14   -  0.863    0.865
step 15   -  0.859    0.861
step 16   -  0.854    0.857
step 17   -  0.848    0.851
step 18   -  0.843    0.847
step 19   -  0.837    0.842
step 20   -  0.831    0.837
step 21   -  0.826    0.833
step 22   -  0.821    0.829
step 23   -  0.816    0.825
step 24   -  0.810    0.819
=============================
Summary   -  21.245    21.289
V_y Shape is 
(75674, 1)
=========== SHAPE INPUT-OUTPUT MATRIXES =====================
shape x_training : (227112, 1134)  y_training: :  (227112, 24)
shape x_test     :  (75673, 1134)  y_test      :  (75673, 24)
shape x_val      :  (75673, 1134)  y_val       :  (75673, 24)
=============================================================
(1134,)
Model: "MLP"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_5 (InputLayer)        [(None, 1134)]            0         
                                                                 
 dense_16 (Dense)            (None, 1024)              1162240   
                                                                 
 elu_4 (ELU)                 (None, 1024)              0         
                                                                 
 dropout_12 (Dropout)        (None, 1024)              0         
                                                                 
 dense_17 (Dense)            (None, 512)               524800    
                                                                 
 dropout_13 (Dropout)        (None, 512)               0         
                                                                 
 dense_18 (Dense)            (None, 512)               262656    
                                                                 
 dropout_14 (Dropout)        (None, 512)               0         
                                                                 
 dense_19 (Dense)            (None, 24)                12312     
                                                                 
=================================================================
Total params: 1,962,008
Trainable params: 1,962,008
Non-trainable params: 0
_________________________________________________________________
Epoch 1/200
222/222 [==============================] - 2s 8ms/step - loss: 0.5454 - val_loss: 0.1204
Epoch 2/200
222/222 [==============================] - 2s 7ms/step - loss: 0.2717 - val_loss: 0.1086
Epoch 3/200
222/222 [==============================] - 2s 7ms/step - loss: 0.2210 - val_loss: 0.1037
Epoch 4/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1964 - val_loss: 0.1002
Epoch 5/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1800 - val_loss: 0.0980
Epoch 6/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1680 - val_loss: 0.0959
Epoch 7/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1587 - val_loss: 0.0975
Epoch 8/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1510 - val_loss: 0.0960
Epoch 9/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1444 - val_loss: 0.0938
Epoch 10/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1394 - val_loss: 0.0924
Epoch 11/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1349 - val_loss: 0.0917
Epoch 12/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1309 - val_loss: 0.0939
Epoch 13/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1272 - val_loss: 0.0938
Epoch 14/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1239 - val_loss: 0.0922
Epoch 15/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1214 - val_loss: 0.0924
Epoch 16/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1188 - val_loss: 0.0921
Epoch 17/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1164 - val_loss: 0.0918
Epoch 18/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1145 - val_loss: 0.0912
Epoch 19/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1128 - val_loss: 0.0897
Epoch 20/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1109 - val_loss: 0.0903
Epoch 21/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1094 - val_loss: 0.0913
Epoch 22/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1082 - val_loss: 0.0909
Epoch 23/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1068 - val_loss: 0.0904
Epoch 24/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1058 - val_loss: 0.0913
Epoch 25/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1046 - val_loss: 0.0914
Epoch 26/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1038 - val_loss: 0.0927
Epoch 27/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1027 - val_loss: 0.0902
Epoch 28/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1016 - val_loss: 0.0909
Epoch 29/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1007 - val_loss: 0.0924
Val_yp Shape is 
(75673, 24)
Results === Test == Validation ===== 162
=============================
step 1    -  0.986    0.985
step 2    -  0.979    0.977
step 3    -  0.969    0.967
step 4    -  0.959    0.956
step 5    -  0.948    0.946
step 6    -  0.938    0.935
step 7    -  0.927    0.925
step 8    -  0.917    0.915
step 9    -  0.907    0.906
step 10   -  0.899    0.898
step 11   -  0.891    0.890
step 12   -  0.883    0.883
step 13   -  0.876    0.877
step 14   -  0.869    0.870
step 15   -  0.863    0.864
step 16   -  0.859    0.860
step 17   -  0.854    0.856
step 18   -  0.850    0.853
step 19   -  0.845    0.849
step 20   -  0.839    0.843
step 21   -  0.834    0.839
step 22   -  0.829    0.834
step 23   -  0.823    0.829
step 24   -  0.818    0.825
=============================
Summary   -  21.365    21.380
V_y Shape is 
(75673, 1)
=========== SHAPE INPUT-OUTPUT MATRIXES =====================
shape x_training : (227110, 1148)  y_training: :  (227110, 24)
shape x_test     :  (75672, 1148)  y_test      :  (75672, 24)
shape x_val      :  (75672, 1148)  y_val       :  (75672, 24)
=============================================================
(1148,)
Model: "MLP"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_6 (InputLayer)        [(None, 1148)]            0         
                                                                 
 dense_20 (Dense)            (None, 1024)              1176576   
                                                                 
 elu_5 (ELU)                 (None, 1024)              0         
                                                                 
 dropout_15 (Dropout)        (None, 1024)              0         
                                                                 
 dense_21 (Dense)            (None, 512)               524800    
                                                                 
 dropout_16 (Dropout)        (None, 512)               0         
                                                                 
 dense_22 (Dense)            (None, 512)               262656    
                                                                 
 dropout_17 (Dropout)        (None, 512)               0         
                                                                 
 dense_23 (Dense)            (None, 24)                12312     
                                                                 
=================================================================
Total params: 1,976,344
Trainable params: 1,976,344
Non-trainable params: 0
_________________________________________________________________


=========== SHAPE INPUT-OUTPUT MATRIXES =====================
shape x_training : (227104, 1190)  y_training: :  (227104, 24)
shape x_test     :  (75669, 1190)  y_test      :  (75669, 24)
shape x_val      :  (75669, 1190)  y_val       :  (75669, 24)
=============================================================
(1190,)
Model: "MLP"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_1 (InputLayer)        [(None, 1190)]            0         
                                                                 
 dense (Dense)               (None, 1024)              1219584   
                                                                 
 elu (ELU)                   (None, 1024)              0         
                                                                 
 dropout (Dropout)           (None, 1024)              0         
                                                                 
 dense_1 (Dense)             (None, 512)               524800    
                                                                 
 dropout_1 (Dropout)         (None, 512)               0         
                                                                 
 dense_2 (Dense)             (None, 512)               262656    
                                                                 
 dropout_2 (Dropout)         (None, 512)               0         
                                                                 
 dense_3 (Dense)             (None, 24)                12312     
                                                                 
=================================================================
Total params: 2,019,352
Trainable params: 2,019,352
Non-trainable params: 0
_________________________________________________________________
Epoch 1/200
222/222 [==============================] - 3s 8ms/step - loss: 0.5563 - val_loss: 0.1228
Epoch 2/200
222/222 [==============================] - 1s 6ms/step - loss: 0.2726 - val_loss: 0.1068
Epoch 3/200
222/222 [==============================] - 1s 6ms/step - loss: 0.2223 - val_loss: 0.1051
Epoch 4/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1968 - val_loss: 0.1017
Epoch 5/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1805 - val_loss: 0.1011
Epoch 6/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1683 - val_loss: 0.0977
Epoch 7/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1587 - val_loss: 0.0942
Epoch 8/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1511 - val_loss: 0.0960
Epoch 9/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1450 - val_loss: 0.0960
Epoch 10/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1393 - val_loss: 0.0953
Epoch 11/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1343 - val_loss: 0.0941
Epoch 12/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1305 - val_loss: 0.0955
Epoch 13/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1269 - val_loss: 0.0931
Epoch 14/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1239 - val_loss: 0.0919
Epoch 15/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1209 - val_loss: 0.0921
Epoch 16/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1183 - val_loss: 0.0904
Epoch 17/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1160 - val_loss: 0.0922
Epoch 18/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1139 - val_loss: 0.0911
Epoch 19/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1121 - val_loss: 0.0920
Epoch 20/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1103 - val_loss: 0.0910
Epoch 21/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1089 - val_loss: 0.0916
Epoch 22/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1075 - val_loss: 0.0909
Epoch 23/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1060 - val_loss: 0.0922
Epoch 24/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1048 - val_loss: 0.0907
Epoch 25/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1037 - val_loss: 0.0921
Epoch 26/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1026 - val_loss: 0.0926
Val_yp Shape is 
(75669, 24)
Results === Test == Validation ===== 170
=============================
step 1    -  0.987    0.986
step 2    -  0.979    0.978
step 3    -  0.970    0.968
step 4    -  0.960    0.957
step 5    -  0.949    0.946
step 6    -  0.938    0.936
step 7    -  0.928    0.926
step 8    -  0.918    0.916
step 9    -  0.908    0.907
step 10   -  0.899    0.898
step 11   -  0.891    0.890
step 12   -  0.884    0.884
step 13   -  0.877    0.877
step 14   -  0.871    0.871
step 15   -  0.865    0.866
step 16   -  0.858    0.859
step 17   -  0.853    0.854
step 18   -  0.847    0.849
step 19   -  0.842    0.845
step 20   -  0.837    0.840
step 21   -  0.832    0.836
step 22   -  0.828    0.832
step 23   -  0.824    0.829
step 24   -  0.818    0.824
=============================
Summary   -  21.364    21.373
V_y Shape is 
(75669, 1)
=========== SHAPE INPUT-OUTPUT MATRIXES =====================
shape x_training : (227102, 1204)  y_training: :  (227102, 24)
shape x_test     :  (75668, 1204)  y_test      :  (75668, 24)
shape x_val      :  (75668, 1204)  y_val       :  (75668, 24)
=============================================================
(1204,)
Model: "MLP"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_2 (InputLayer)        [(None, 1204)]            0         
                                                                 
 dense_4 (Dense)             (None, 1024)              1233920   
                                                                 
 elu_1 (ELU)                 (None, 1024)              0         
                                                                 
 dropout_3 (Dropout)         (None, 1024)              0         
                                                                 
 dense_5 (Dense)             (None, 512)               524800    
                                                                 
 dropout_4 (Dropout)         (None, 512)               0         
                                                                 
 dense_6 (Dense)             (None, 512)               262656    
                                                                 
 dropout_5 (Dropout)         (None, 512)               0         
                                                                 
 dense_7 (Dense)             (None, 24)                12312     
                                                                 
=================================================================
Total params: 2,033,688
Trainable params: 2,033,688
Non-trainable params: 0
_________________________________________________________________
Epoch 1/200
222/222 [==============================] - 3s 10ms/step - loss: 0.5614 - val_loss: 0.1230
Epoch 2/200
222/222 [==============================] - 2s 7ms/step - loss: 0.2754 - val_loss: 0.1103
Epoch 3/200
222/222 [==============================] - 2s 7ms/step - loss: 0.2234 - val_loss: 0.1066
Epoch 4/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1980 - val_loss: 0.1024
Epoch 5/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1811 - val_loss: 0.0999
Epoch 6/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1688 - val_loss: 0.0970
Epoch 7/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1593 - val_loss: 0.0963
Epoch 8/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1514 - val_loss: 0.0976
Epoch 9/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1449 - val_loss: 0.0963
Epoch 10/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1395 - val_loss: 0.0975
Epoch 11/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1347 - val_loss: 0.0932
Epoch 12/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1305 - val_loss: 0.0926
Epoch 13/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1267 - val_loss: 0.0932
Epoch 14/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1237 - val_loss: 0.0939
Epoch 15/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1209 - val_loss: 0.0916
Epoch 16/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1181 - val_loss: 0.0931
Epoch 17/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1160 - val_loss: 0.0925
Epoch 18/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1137 - val_loss: 0.0921
Epoch 19/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1117 - val_loss: 0.0955
Epoch 20/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1102 - val_loss: 0.0934
Epoch 21/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1086 - val_loss: 0.0920
Epoch 22/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1071 - val_loss: 0.0906
Epoch 23/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1058 - val_loss: 0.0930
Epoch 24/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1043 - val_loss: 0.0921
Epoch 25/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1034 - val_loss: 0.0912
Epoch 26/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1020 - val_loss: 0.0913
Epoch 27/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1013 - val_loss: 0.0925
Epoch 28/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1000 - val_loss: 0.0932
Epoch 29/200
222/222 [==============================] - 2s 7ms/step - loss: 0.0991 - val_loss: 0.0910
Epoch 30/200
222/222 [==============================] - 2s 7ms/step - loss: 0.0982 - val_loss: 0.0937
Epoch 31/200
222/222 [==============================] - 2s 7ms/step - loss: 0.0974 - val_loss: 0.0926
Epoch 32/200
222/222 [==============================] - 2s 7ms/step - loss: 0.0967 - val_loss: 0.0921
Val_yp Shape is 
(75668, 24)
Results === Test == Validation ===== 172
=============================
step 1    -  0.987    0.986
step 2    -  0.979    0.978
step 3    -  0.970    0.968
step 4    -  0.959    0.957
step 5    -  0.948    0.946
step 6    -  0.937    0.935
step 7    -  0.927    0.925
step 8    -  0.917    0.916
step 9    -  0.908    0.907
step 10   -  0.898    0.898
step 11   -  0.889    0.889
step 12   -  0.880    0.881
step 13   -  0.872    0.873
step 14   -  0.864    0.866
step 15   -  0.858    0.860
step 16   -  0.853    0.855
step 17   -  0.847    0.851
step 18   -  0.842    0.846
step 19   -  0.839    0.843
step 20   -  0.835    0.840
step 21   -  0.831    0.837
step 22   -  0.827    0.834
step 23   -  0.823    0.831
step 24   -  0.819    0.828
=============================
Summary   -  21.310    21.349
V_y Shape is 
(75668, 1)
=========== SHAPE INPUT-OUTPUT MATRIXES =====================
shape x_training : (227100, 1218)  y_training: :  (227100, 24)
shape x_test     :  (75667, 1218)  y_test      :  (75667, 24)
shape x_val      :  (75667, 1218)  y_val       :  (75667, 24)
=============================================================
(1218,)
Model: "MLP"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_3 (InputLayer)        [(None, 1218)]            0         
                                                                 
 dense_8 (Dense)             (None, 1024)              1248256   
                                                                 
 elu_2 (ELU)                 (None, 1024)              0         
                                                                 
 dropout_6 (Dropout)         (None, 1024)              0         
                                                                 
 dense_9 (Dense)             (None, 512)               524800    
                                                                 
 dropout_7 (Dropout)         (None, 512)               0         
                                                                 
 dense_10 (Dense)            (None, 512)               262656    
                                                                 
 dropout_8 (Dropout)         (None, 512)               0         
                                                                 
 dense_11 (Dense)            (None, 24)                12312     
                                                                 
=================================================================
Total params: 2,048,024
Trainable params: 2,048,024
Non-trainable params: 0
_________________________________________________________________
Epoch 1/200
222/222 [==============================] - 3s 9ms/step - loss: 0.5628 - val_loss: 0.1219
Epoch 2/200
222/222 [==============================] - 1s 7ms/step - loss: 0.2750 - val_loss: 0.1106
Epoch 3/200
222/222 [==============================] - 1s 6ms/step - loss: 0.2233 - val_loss: 0.1049
Epoch 4/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1975 - val_loss: 0.1023
Epoch 5/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1811 - val_loss: 0.0997
Epoch 6/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1683 - val_loss: 0.0987
Epoch 7/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1593 - val_loss: 0.0974
Epoch 8/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1510 - val_loss: 0.0951
Epoch 9/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1447 - val_loss: 0.0955
Epoch 10/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1390 - val_loss: 0.0953
Epoch 11/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1339 - val_loss: 0.0921
Epoch 12/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1301 - val_loss: 0.0968
Epoch 13/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1262 - val_loss: 0.0917
Epoch 14/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1231 - val_loss: 0.0923
Epoch 15/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1203 - val_loss: 0.0935
Epoch 16/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1178 - val_loss: 0.0915
Epoch 17/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1150 - val_loss: 0.0929
Epoch 18/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1132 - val_loss: 0.0935
Epoch 19/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1112 - val_loss: 0.0917
Epoch 20/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1096 - val_loss: 0.0940
Epoch 21/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1078 - val_loss: 0.0908
Epoch 22/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1065 - val_loss: 0.0923
Epoch 23/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1054 - val_loss: 0.0915
Epoch 24/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1037 - val_loss: 0.0904
Epoch 25/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1028 - val_loss: 0.0915
Epoch 26/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1020 - val_loss: 0.0922
Epoch 27/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1005 - val_loss: 0.0929
Epoch 28/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0997 - val_loss: 0.0900
Epoch 29/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0987 - val_loss: 0.0904
Epoch 30/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0979 - val_loss: 0.0925
Epoch 31/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0966 - val_loss: 0.0909
Epoch 32/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0957 - val_loss: 0.0983
Epoch 33/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0950 - val_loss: 0.0910
Epoch 34/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0942 - val_loss: 0.0942
Epoch 35/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0934 - val_loss: 0.0908
Epoch 36/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0925 - val_loss: 0.0907
Epoch 37/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0916 - val_loss: 0.0920
Epoch 38/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0911 - val_loss: 0.0922
Val_yp Shape is 
(75667, 24)
Results === Test == Validation ===== 174
=============================
step 1    -  0.987    0.985
step 2    -  0.979    0.977
step 3    -  0.969    0.966
step 4    -  0.958    0.956
step 5    -  0.947    0.945
step 6    -  0.937    0.935
step 7    -  0.926    0.925
step 8    -  0.915    0.915
step 9    -  0.905    0.905
step 10   -  0.896    0.896
step 11   -  0.887    0.889
step 12   -  0.879    0.881
step 13   -  0.870    0.873
step 14   -  0.863    0.866
step 15   -  0.857    0.861
step 16   -  0.851    0.855
step 17   -  0.844    0.849
step 18   -  0.836    0.841
step 19   -  0.831    0.837
step 20   -  0.827    0.833
step 21   -  0.822    0.829
step 22   -  0.818    0.826
step 23   -  0.814    0.823
step 24   -  0.810    0.819
=============================
Summary   -  21.228    21.286
V_y Shape is 
(75667, 1)

=========== SHAPE INPUT-OUTPUT MATRIXES =====================
shape x_training : (227098, 1232)  y_training: :  (227098, 24)
shape x_test     :  (75666, 1232)  y_test      :  (75666, 24)
shape x_val      :  (75666, 1232)  y_val       :  (75666, 24)
=============================================================
(1232,)
Model: "MLP"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_1 (InputLayer)        [(None, 1232)]            0         
                                                                 
 dense (Dense)               (None, 1024)              1262592   
                                                                 
 elu (ELU)                   (None, 1024)              0         
                                                                 
 dropout (Dropout)           (None, 1024)              0         
                                                                 
 dense_1 (Dense)             (None, 512)               524800    
                                                                 
 dropout_1 (Dropout)         (None, 512)               0         
                                                                 
 dense_2 (Dense)             (None, 512)               262656    
                                                                 
 dropout_2 (Dropout)         (None, 512)               0         
                                                                 
 dense_3 (Dense)             (None, 24)                12312     
                                                                 
=================================================================
Total params: 2,062,360
Trainable params: 2,062,360
Non-trainable params: 0
_________________________________________________________________
Epoch 1/200
222/222 [==============================] - 3s 8ms/step - loss: 0.5535 - val_loss: 0.1226
Epoch 2/200
222/222 [==============================] - 1s 6ms/step - loss: 0.2750 - val_loss: 0.1099
Epoch 3/200
222/222 [==============================] - 1s 7ms/step - loss: 0.2228 - val_loss: 0.1102
Epoch 4/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1973 - val_loss: 0.1036
Epoch 5/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1803 - val_loss: 0.1024
Epoch 6/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1683 - val_loss: 0.1015
Epoch 7/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1587 - val_loss: 0.0976
Epoch 8/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1512 - val_loss: 0.0972
Epoch 9/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1444 - val_loss: 0.0940
Epoch 10/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1387 - val_loss: 0.0963
Epoch 11/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1341 - val_loss: 0.0957
Epoch 12/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1301 - val_loss: 0.0957
Epoch 13/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1261 - val_loss: 0.0956
Epoch 14/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1228 - val_loss: 0.0947
Epoch 15/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1201 - val_loss: 0.0937
Epoch 16/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1171 - val_loss: 0.0927
Epoch 17/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1150 - val_loss: 0.0932
Epoch 18/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1126 - val_loss: 0.0912
Epoch 19/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1111 - val_loss: 0.0925
Epoch 20/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1093 - val_loss: 0.0924
Epoch 21/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1078 - val_loss: 0.0912
Epoch 22/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1063 - val_loss: 0.0941
Epoch 23/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1050 - val_loss: 0.0923
Epoch 24/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1039 - val_loss: 0.0924
Epoch 25/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1024 - val_loss: 0.0918
Epoch 26/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1013 - val_loss: 0.0939
Epoch 27/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1003 - val_loss: 0.0931
Epoch 28/200
222/222 [==============================] - 2s 7ms/step - loss: 0.0993 - val_loss: 0.0924
Epoch 29/200
222/222 [==============================] - 2s 7ms/step - loss: 0.0983 - val_loss: 0.0923
Epoch 30/200
222/222 [==============================] - 2s 7ms/step - loss: 0.0974 - val_loss: 0.0937
Epoch 31/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0964 - val_loss: 0.0920
Val_yp Shape is 
(75666, 24)
Results === Test == Validation ===== 176
=============================
step 1    -  0.987    0.985
step 2    -  0.980    0.978
step 3    -  0.970    0.968
step 4    -  0.960    0.957
step 5    -  0.949    0.946
step 6    -  0.938    0.936
step 7    -  0.928    0.926
step 8    -  0.918    0.916
step 9    -  0.907    0.905
step 10   -  0.899    0.897
step 11   -  0.890    0.889
step 12   -  0.884    0.883
step 13   -  0.878    0.878
step 14   -  0.871    0.871
step 15   -  0.866    0.866
step 16   -  0.859    0.860
step 17   -  0.855    0.856
step 18   -  0.850    0.852
step 19   -  0.844    0.847
step 20   -  0.839    0.842
step 21   -  0.833    0.837
step 22   -  0.828    0.833
step 23   -  0.825    0.831
step 24   -  0.820    0.827
=============================
Summary   -  21.378    21.389
V_y Shape is 
(75666, 1)
=========== SHAPE INPUT-OUTPUT MATRIXES =====================
shape x_training : (227096, 1246)  y_training: :  (227096, 24)
shape x_test     :  (75665, 1246)  y_test      :  (75665, 24)
shape x_val      :  (75665, 1246)  y_val       :  (75665, 24)
=============================================================
(1246,)
Model: "MLP"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_2 (InputLayer)        [(None, 1246)]            0         
                                                                 
 dense_4 (Dense)             (None, 1024)              1276928   
                                                                 
 elu_1 (ELU)                 (None, 1024)              0         
                                                                 
 dropout_3 (Dropout)         (None, 1024)              0         
                                                                 
 dense_5 (Dense)             (None, 512)               524800    
                                                                 
 dropout_4 (Dropout)         (None, 512)               0         
                                                                 
 dense_6 (Dense)             (None, 512)               262656    
                                                                 
 dropout_5 (Dropout)         (None, 512)               0         
                                                                 
 dense_7 (Dense)             (None, 24)                12312     
                                                                 
=================================================================
Total params: 2,076,696
Trainable params: 2,076,696
Non-trainable params: 0
_________________________________________________________________
Epoch 1/200
222/222 [==============================] - 2s 8ms/step - loss: 0.5644 - val_loss: 0.1221
Epoch 2/200
222/222 [==============================] - 2s 7ms/step - loss: 0.2759 - val_loss: 0.1109
Epoch 3/200
222/222 [==============================] - 2s 7ms/step - loss: 0.2235 - val_loss: 0.1066
Epoch 4/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1978 - val_loss: 0.1015
Epoch 5/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1810 - val_loss: 0.1009
Epoch 6/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1685 - val_loss: 0.0975
Epoch 7/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1589 - val_loss: 0.0952
Epoch 8/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1512 - val_loss: 0.0947
Epoch 9/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1444 - val_loss: 0.0942
Epoch 10/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1389 - val_loss: 0.0961
Epoch 11/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1341 - val_loss: 0.0951
Epoch 12/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1296 - val_loss: 0.0926
Epoch 13/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1260 - val_loss: 0.0937
Epoch 14/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1229 - val_loss: 0.0931
Epoch 15/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1198 - val_loss: 0.0922
Epoch 16/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1172 - val_loss: 0.0930
Epoch 17/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1147 - val_loss: 0.0914
Epoch 18/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1127 - val_loss: 0.0947
Epoch 19/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1110 - val_loss: 0.0933
Epoch 20/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1092 - val_loss: 0.0941
Epoch 21/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1077 - val_loss: 0.0919
Epoch 22/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1058 - val_loss: 0.0926
Epoch 23/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1048 - val_loss: 0.0923
Epoch 24/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1032 - val_loss: 0.0912
Epoch 25/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1022 - val_loss: 0.0921
Epoch 26/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1012 - val_loss: 0.0926
Epoch 27/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1002 - val_loss: 0.0922
Epoch 28/200
222/222 [==============================] - 2s 7ms/step - loss: 0.0991 - val_loss: 0.0912
Epoch 29/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0981 - val_loss: 0.0931
Epoch 30/200
222/222 [==============================] - 2s 7ms/step - loss: 0.0969 - val_loss: 0.0916
Epoch 31/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0962 - val_loss: 0.0942
Epoch 32/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0952 - val_loss: 0.0909
Epoch 33/200
222/222 [==============================] - 2s 7ms/step - loss: 0.0942 - val_loss: 0.0918
Epoch 34/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0934 - val_loss: 0.0934
Epoch 35/200
222/222 [==============================] - 2s 7ms/step - loss: 0.0927 - val_loss: 0.0937
Epoch 36/200
222/222 [==============================] - 2s 7ms/step - loss: 0.0919 - val_loss: 0.0928
Epoch 37/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0911 - val_loss: 0.0919
Epoch 38/200
222/222 [==============================] - 2s 7ms/step - loss: 0.0904 - val_loss: 0.0921
Epoch 39/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0894 - val_loss: 0.0931
Epoch 40/200
222/222 [==============================] - 2s 7ms/step - loss: 0.0889 - val_loss: 0.0921
Epoch 41/200
222/222 [==============================] - 2s 7ms/step - loss: 0.0880 - val_loss: 0.0942
Epoch 42/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0875 - val_loss: 0.0923
Val_yp Shape is 
(75665, 24)
Results === Test == Validation ===== 178
=============================
step 1    -  0.988    0.986
step 2    -  0.980    0.979
step 3    -  0.972    0.969
step 4    -  0.962    0.960
step 5    -  0.952    0.950
step 6    -  0.941    0.940
step 7    -  0.932    0.931
step 8    -  0.922    0.921
step 9    -  0.913    0.912
step 10   -  0.903    0.904
step 11   -  0.896    0.896
step 12   -  0.888    0.889
step 13   -  0.881    0.882
step 14   -  0.874    0.875
step 15   -  0.865    0.867
step 16   -  0.859    0.861
step 17   -  0.851    0.854
step 18   -  0.844    0.848
step 19   -  0.837    0.841
step 20   -  0.830    0.835
step 21   -  0.826    0.831
step 22   -  0.822    0.828
step 23   -  0.818    0.825
step 24   -  0.814    0.822
=============================
Summary   -  21.367    21.406
V_y Shape is 
(75665, 1)
=========== SHAPE INPUT-OUTPUT MATRIXES =====================
shape x_training : (227094, 1260)  y_training: :  (227094, 24)
shape x_test     :  (75664, 1260)  y_test      :  (75664, 24)
shape x_val      :  (75664, 1260)  y_val       :  (75664, 24)
=============================================================
(1260,)
Model: "MLP"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_3 (InputLayer)        [(None, 1260)]            0         
                                                                 
 dense_8 (Dense)             (None, 1024)              1291264   
                                                                 
 elu_2 (ELU)                 (None, 1024)              0         
                                                                 
 dropout_6 (Dropout)         (None, 1024)              0         
                                                                 
 dense_9 (Dense)             (None, 512)               524800    
                                                                 
 dropout_7 (Dropout)         (None, 512)               0         
                                                                 
 dense_10 (Dense)            (None, 512)               262656    
                                                                 
 dropout_8 (Dropout)         (None, 512)               0         
                                                                 
 dense_11 (Dense)            (None, 24)                12312     
                                                                 
=================================================================
Total params: 2,091,032
Trainable params: 2,091,032
Non-trainable params: 0
_________________________________________________________________
Epoch 1/200
222/222 [==============================] - 2s 8ms/step - loss: 0.5657 - val_loss: 0.1204
Epoch 2/200
222/222 [==============================] - 2s 7ms/step - loss: 0.2761 - val_loss: 0.1104
Epoch 3/200
222/222 [==============================] - 2s 7ms/step - loss: 0.2235 - val_loss: 0.1049
Epoch 4/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1978 - val_loss: 0.1001
Epoch 5/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1809 - val_loss: 0.0983
Epoch 6/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1684 - val_loss: 0.0990
Epoch 7/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1591 - val_loss: 0.0984
Epoch 8/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1512 - val_loss: 0.0966
Epoch 9/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1445 - val_loss: 0.0976
Epoch 10/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1390 - val_loss: 0.0970
Epoch 11/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1341 - val_loss: 0.0948
Epoch 12/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1297 - val_loss: 0.0957
Epoch 13/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1260 - val_loss: 0.0953
Epoch 14/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1226 - val_loss: 0.0932
Epoch 15/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1195 - val_loss: 0.0929
Epoch 16/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1170 - val_loss: 0.0938
Epoch 17/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1145 - val_loss: 0.0922
Epoch 18/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1123 - val_loss: 0.0930
Epoch 19/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1102 - val_loss: 0.0932
Epoch 20/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1086 - val_loss: 0.0926
Epoch 21/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1070 - val_loss: 0.0913
Epoch 22/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1057 - val_loss: 0.0928
Epoch 23/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1043 - val_loss: 0.0931
Epoch 24/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1030 - val_loss: 0.0926
Epoch 25/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1021 - val_loss: 0.0920
Epoch 26/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1008 - val_loss: 0.0928
Epoch 27/200
222/222 [==============================] - 2s 7ms/step - loss: 0.0998 - val_loss: 0.0918
Epoch 28/200
222/222 [==============================] - 2s 7ms/step - loss: 0.0986 - val_loss: 0.0927
Epoch 29/200
222/222 [==============================] - 2s 7ms/step - loss: 0.0978 - val_loss: 0.0917
Epoch 30/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0966 - val_loss: 0.0937
Epoch 31/200
222/222 [==============================] - 2s 7ms/step - loss: 0.0957 - val_loss: 0.0927
Val_yp Shape is 
(75664, 24)
Results === Test == Validation ===== 180
=============================
step 1    -  0.987    0.986
step 2    -  0.979    0.978
step 3    -  0.970    0.968
step 4    -  0.961    0.959
step 5    -  0.951    0.949
step 6    -  0.940    0.939
step 7    -  0.930    0.929
step 8    -  0.920    0.919
step 9    -  0.910    0.909
step 10   -  0.900    0.900
step 11   -  0.891    0.892
step 12   -  0.883    0.884
step 13   -  0.876    0.877
step 14   -  0.867    0.869
step 15   -  0.861    0.864
step 16   -  0.855    0.858
step 17   -  0.848    0.852
step 18   -  0.843    0.848
step 19   -  0.838    0.843
step 20   -  0.834    0.840
step 21   -  0.829    0.836
step 22   -  0.825    0.833
step 23   -  0.821    0.829
step 24   -  0.817    0.827
=============================
Summary   -  21.337    21.388
V_y Shape is 
(75664, 1)
=========== SHAPE INPUT-OUTPUT MATRIXES =====================
shape x_training : (227092, 1274)  y_training: :  (227092, 24)
shape x_test     :  (75663, 1274)  y_test      :  (75663, 24)
shape x_val      :  (75663, 1274)  y_val       :  (75663, 24)
=============================================================
(1274,)
Model: "MLP"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_4 (InputLayer)        [(None, 1274)]            0         
                                                                 
 dense_12 (Dense)            (None, 1024)              1305600   
                                                                 
 elu_3 (ELU)                 (None, 1024)              0         
                                                                 
 dropout_9 (Dropout)         (None, 1024)              0         
                                                                 
 dense_13 (Dense)            (None, 512)               524800    
                                                                 
 dropout_10 (Dropout)        (None, 512)               0         
                                                                 
 dense_14 (Dense)            (None, 512)               262656    
                                                                 
 dropout_11 (Dropout)        (None, 512)               0         
                                                                 
 dense_15 (Dense)            (None, 24)                12312     
                                                                 
=================================================================
Total params: 2,105,368
Trainable params: 2,105,368
Non-trainable params: 0
_________________________________________________________________
Epoch 1/200
222/222 [==============================] - 2s 8ms/step - loss: 0.5677 - val_loss: 0.1239
Epoch 2/200
222/222 [==============================] - 1s 7ms/step - loss: 0.2770 - val_loss: 0.1123
Epoch 3/200
222/222 [==============================] - 1s 7ms/step - loss: 0.2248 - val_loss: 0.1059
Epoch 4/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1983 - val_loss: 0.1020
Epoch 5/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1818 - val_loss: 0.0996
Epoch 6/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1690 - val_loss: 0.0994
Epoch 7/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1593 - val_loss: 0.0970
Epoch 8/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1514 - val_loss: 0.0981
Epoch 9/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1447 - val_loss: 0.0964
Epoch 10/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1394 - val_loss: 0.0943
Epoch 11/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1340 - val_loss: 0.0958
Epoch 12/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1299 - val_loss: 0.0948
Epoch 13/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1260 - val_loss: 0.0941
Epoch 14/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1226 - val_loss: 0.0941
Epoch 15/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1200 - val_loss: 0.0938
Epoch 16/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1167 - val_loss: 0.0944
Epoch 17/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1148 - val_loss: 0.0931
Epoch 18/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1126 - val_loss: 0.0934
Epoch 19/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1104 - val_loss: 0.0927
Epoch 20/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1085 - val_loss: 0.0923
Epoch 21/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1071 - val_loss: 0.0935
Epoch 22/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1056 - val_loss: 0.0950
Epoch 23/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1042 - val_loss: 0.0937
Epoch 24/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1029 - val_loss: 0.0926
Epoch 25/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1019 - val_loss: 0.0920
Epoch 26/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1007 - val_loss: 0.0923
Epoch 27/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0994 - val_loss: 0.0949
Epoch 28/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0984 - val_loss: 0.0936
Epoch 29/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0975 - val_loss: 0.0918
Epoch 30/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0965 - val_loss: 0.0940
Epoch 31/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0955 - val_loss: 0.0927
Epoch 32/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0946 - val_loss: 0.0923
Epoch 33/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0938 - val_loss: 0.0926
Epoch 34/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0929 - val_loss: 0.0929
Epoch 35/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0919 - val_loss: 0.0945
Epoch 36/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0912 - val_loss: 0.0956
Epoch 37/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0906 - val_loss: 0.0939
Epoch 38/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0899 - val_loss: 0.0924
Epoch 39/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0889 - val_loss: 0.0913
Epoch 40/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0880 - val_loss: 0.0931
Epoch 41/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0876 - val_loss: 0.0914
Epoch 42/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0869 - val_loss: 0.0935
Epoch 43/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0863 - val_loss: 0.0958
Epoch 44/200
222/222 [==============================] - 2s 7ms/step - loss: 0.0856 - val_loss: 0.0920
Epoch 45/200
222/222 [==============================] - 2s 7ms/step - loss: 0.0847 - val_loss: 0.0936
Epoch 46/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0842 - val_loss: 0.0928
Epoch 47/200
222/222 [==============================] - 2s 7ms/step - loss: 0.0836 - val_loss: 0.0922
Epoch 48/200
222/222 [==============================] - 2s 7ms/step - loss: 0.0829 - val_loss: 0.0931
Epoch 49/200
222/222 [==============================] - 2s 8ms/step - loss: 0.0823 - val_loss: 0.0932
Val_yp Shape is 
(75663, 24)
Results === Test == Validation ===== 182
=============================
step 1    -  0.987    0.985
step 2    -  0.979    0.976
step 3    -  0.969    0.966
step 4    -  0.959    0.956
step 5    -  0.949    0.946
step 6    -  0.939    0.936
step 7    -  0.928    0.926
step 8    -  0.917    0.916
step 9    -  0.906    0.905
step 10   -  0.897    0.896
step 11   -  0.886    0.886
step 12   -  0.877    0.878
step 13   -  0.867    0.868
step 14   -  0.859    0.861
step 15   -  0.852    0.855
step 16   -  0.845    0.848
step 17   -  0.837    0.840
step 18   -  0.831    0.835
step 19   -  0.826    0.831
step 20   -  0.820    0.825
step 21   -  0.816    0.822
step 22   -  0.813    0.820
step 23   -  0.809    0.817
step 24   -  0.806    0.815
=============================
Summary   -  21.176    21.208
V_y Shape is 
(75663, 1)
=========== SHAPE INPUT-OUTPUT MATRIXES =====================
shape x_training : (227090, 1288)  y_training: :  (227090, 24)
shape x_test     :  (75662, 1288)  y_test      :  (75662, 24)
shape x_val      :  (75662, 1288)  y_val       :  (75662, 24)
=============================================================
(1288,)
Model: "MLP"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_5 (InputLayer)        [(None, 1288)]            0         
                                                                 
 dense_16 (Dense)            (None, 1024)              1319936   
                                                                 
 elu_4 (ELU)                 (None, 1024)              0         
                                                                 
 dropout_12 (Dropout)        (None, 1024)              0         
                                                                 
 dense_17 (Dense)            (None, 512)               524800    
                                                                 
 dropout_13 (Dropout)        (None, 512)               0         
                                                                 
 dense_18 (Dense)            (None, 512)               262656    
                                                                 
 dropout_14 (Dropout)        (None, 512)               0         
                                                                 
 dense_19 (Dense)            (None, 24)                12312     
                                                                 
=================================================================
Total params: 2,119,704
Trainable params: 2,119,704
Non-trainable params: 0
_________________________________________________________________
Epoch 1/200
222/222 [==============================] - 2s 7ms/step - loss: 0.5659 - val_loss: 0.1222
Epoch 2/200
222/222 [==============================] - 1s 6ms/step - loss: 0.2781 - val_loss: 0.1098
Epoch 3/200
222/222 [==============================] - 1s 6ms/step - loss: 0.2245 - val_loss: 0.1064
Epoch 4/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1985 - val_loss: 0.1023
Epoch 5/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1816 - val_loss: 0.1000
Epoch 6/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1691 - val_loss: 0.0976
Epoch 7/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1592 - val_loss: 0.0962
Epoch 8/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1508 - val_loss: 0.0950
Epoch 9/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1444 - val_loss: 0.0955
Epoch 10/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1384 - val_loss: 0.0952
Epoch 11/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1338 - val_loss: 0.0956
Epoch 12/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1295 - val_loss: 0.0946
Epoch 13/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1257 - val_loss: 0.0934
Epoch 14/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1223 - val_loss: 0.0946
Epoch 15/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1194 - val_loss: 0.0941
Epoch 16/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1166 - val_loss: 0.0933
Epoch 17/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1140 - val_loss: 0.0915
Epoch 18/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1120 - val_loss: 0.0929
Epoch 19/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1099 - val_loss: 0.0931
Epoch 20/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1080 - val_loss: 0.0920
Epoch 21/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1063 - val_loss: 0.0932
Epoch 22/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1051 - val_loss: 0.0950
Epoch 23/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1037 - val_loss: 0.0941
Epoch 24/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1024 - val_loss: 0.0939
Epoch 25/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1012 - val_loss: 0.0951
Epoch 26/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1000 - val_loss: 0.0938
Epoch 27/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0990 - val_loss: 0.0937
Val_yp Shape is 
(75662, 24)
Results === Test == Validation ===== 184
=============================
step 1    -  0.986    0.984
step 2    -  0.978    0.976
step 3    -  0.968    0.965
step 4    -  0.957    0.954
step 5    -  0.945    0.942
step 6    -  0.933    0.930
step 7    -  0.922    0.919
step 8    -  0.911    0.909
step 9    -  0.901    0.899
step 10   -  0.891    0.890
step 11   -  0.882    0.882
step 12   -  0.875    0.875
step 13   -  0.868    0.869
step 14   -  0.862    0.863
step 15   -  0.856    0.858
step 16   -  0.851    0.854
step 17   -  0.846    0.849
step 18   -  0.842    0.846
step 19   -  0.838    0.842
step 20   -  0.833    0.837
step 21   -  0.828    0.833
step 22   -  0.823    0.829
step 23   -  0.818    0.823
step 24   -  0.813    0.819
=============================
Summary   -  21.225    21.247
V_y Shape is 
(75662, 1)

=========== SHAPE INPUT-OUTPUT MATRIXES =====================
shape x_training : (227088, 1302)  y_training: :  (227088, 24)
shape x_test     :  (75661, 1302)  y_test      :  (75661, 24)
shape x_val      :  (75661, 1302)  y_val       :  (75661, 24)
=============================================================
(1302,)
Model: "MLP"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_1 (InputLayer)        [(None, 1302)]            0         
                                                                 
 dense (Dense)               (None, 1024)              1334272   
                                                                 
 elu (ELU)                   (None, 1024)              0         
                                                                 
 dropout (Dropout)           (None, 1024)              0         
                                                                 
 dense_1 (Dense)             (None, 512)               524800    
                                                                 
 dropout_1 (Dropout)         (None, 512)               0         
                                                                 
 dense_2 (Dense)             (None, 512)               262656    
                                                                 
 dropout_2 (Dropout)         (None, 512)               0         
                                                                 
 dense_3 (Dense)             (None, 24)                12312     
                                                                 
=================================================================
Total params: 2,134,040
Trainable params: 2,134,040
Non-trainable params: 0
_________________________________________________________________
Epoch 1/200
222/222 [==============================] - 3s 8ms/step - loss: 0.5692 - val_loss: 0.1214
Epoch 2/200
222/222 [==============================] - 1s 6ms/step - loss: 0.2783 - val_loss: 0.1099
Epoch 3/200
222/222 [==============================] - 1s 7ms/step - loss: 0.2246 - val_loss: 0.1053
Epoch 4/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1985 - val_loss: 0.1007
Epoch 5/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1814 - val_loss: 0.0994
Epoch 6/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1686 - val_loss: 0.0987
Epoch 7/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1589 - val_loss: 0.0985
Epoch 8/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1509 - val_loss: 0.0968
Epoch 9/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1442 - val_loss: 0.0941
Epoch 10/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1386 - val_loss: 0.0940
Epoch 11/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1336 - val_loss: 0.0937
Epoch 12/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1293 - val_loss: 0.0942
Epoch 13/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1253 - val_loss: 0.0950
Epoch 14/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1222 - val_loss: 0.0933
Epoch 15/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1191 - val_loss: 0.0951
Epoch 16/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1162 - val_loss: 0.0927
Epoch 17/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1138 - val_loss: 0.0932
Epoch 18/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1115 - val_loss: 0.0936
Epoch 19/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1099 - val_loss: 0.0932
Epoch 20/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1079 - val_loss: 0.0941
Epoch 21/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1063 - val_loss: 0.0923
Epoch 22/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1049 - val_loss: 0.0911
Epoch 23/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1034 - val_loss: 0.0952
Epoch 24/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1021 - val_loss: 0.0929
Epoch 25/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1012 - val_loss: 0.0937
Epoch 26/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0998 - val_loss: 0.0920
Epoch 27/200
222/222 [==============================] - 2s 7ms/step - loss: 0.0988 - val_loss: 0.0936
Epoch 28/200
222/222 [==============================] - 2s 7ms/step - loss: 0.0978 - val_loss: 0.0918
Epoch 29/200
222/222 [==============================] - 2s 7ms/step - loss: 0.0967 - val_loss: 0.0936
Epoch 30/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0955 - val_loss: 0.0928
Epoch 31/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0948 - val_loss: 0.0926
Epoch 32/200
222/222 [==============================] - 2s 7ms/step - loss: 0.0939 - val_loss: 0.0921
Val_yp Shape is 
(75661, 24)
Results === Test == Validation ===== 186
=============================
step 1    -  0.987    0.986
step 2    -  0.980    0.978
step 3    -  0.970    0.968
step 4    -  0.960    0.958
step 5    -  0.949    0.947
step 6    -  0.938    0.936
step 7    -  0.927    0.925
step 8    -  0.916    0.915
step 9    -  0.905    0.904
step 10   -  0.896    0.895
step 11   -  0.887    0.887
step 12   -  0.879    0.879
step 13   -  0.871    0.872
step 14   -  0.864    0.866
step 15   -  0.859    0.861
step 16   -  0.853    0.856
step 17   -  0.849    0.852
step 18   -  0.843    0.847
step 19   -  0.840    0.844
step 20   -  0.836    0.841
step 21   -  0.831    0.836
step 22   -  0.827    0.833
step 23   -  0.824    0.831
step 24   -  0.818    0.826
=============================
Summary   -  21.312    21.342
V_y Shape is 
(75661, 1)
=========== SHAPE INPUT-OUTPUT MATRIXES =====================
shape x_training : (227086, 1316)  y_training: :  (227086, 24)
shape x_test     :  (75660, 1316)  y_test      :  (75660, 24)
shape x_val      :  (75660, 1316)  y_val       :  (75660, 24)
=============================================================
(1316,)
Model: "MLP"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_2 (InputLayer)        [(None, 1316)]            0         
                                                                 
 dense_4 (Dense)             (None, 1024)              1348608   
                                                                 
 elu_1 (ELU)                 (None, 1024)              0         
                                                                 
 dropout_3 (Dropout)         (None, 1024)              0         
                                                                 
 dense_5 (Dense)             (None, 512)               524800    
                                                                 
 dropout_4 (Dropout)         (None, 512)               0         
                                                                 
 dense_6 (Dense)             (None, 512)               262656    
                                                                 
 dropout_5 (Dropout)         (None, 512)               0         
                                                                 
 dense_7 (Dense)             (None, 24)                12312     
                                                                 
=================================================================
Total params: 2,148,376
Trainable params: 2,148,376
Non-trainable params: 0
_________________________________________________________________
Epoch 1/200
222/222 [==============================] - 3s 10ms/step - loss: 0.5644 - val_loss: 0.1221
Epoch 2/200
222/222 [==============================] - 2s 8ms/step - loss: 0.2751 - val_loss: 0.1086
Epoch 3/200
222/222 [==============================] - 2s 8ms/step - loss: 0.2236 - val_loss: 0.1050
Epoch 4/200
222/222 [==============================] - 2s 8ms/step - loss: 0.1977 - val_loss: 0.1045
Epoch 5/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1807 - val_loss: 0.1011
Epoch 6/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1683 - val_loss: 0.1001
Epoch 7/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1585 - val_loss: 0.0975
Epoch 8/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1507 - val_loss: 0.0960
Epoch 9/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1436 - val_loss: 0.0958
Epoch 10/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1382 - val_loss: 0.0993
Epoch 11/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1333 - val_loss: 0.0953
Epoch 12/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1288 - val_loss: 0.0949
Epoch 13/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1252 - val_loss: 0.0950
Epoch 14/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1214 - val_loss: 0.0944
Epoch 15/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1187 - val_loss: 0.0953
Epoch 16/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1159 - val_loss: 0.0956
Epoch 17/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1135 - val_loss: 0.0945
Epoch 18/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1113 - val_loss: 0.0954
Epoch 19/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1093 - val_loss: 0.0938
Epoch 20/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1074 - val_loss: 0.0951
Epoch 21/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1058 - val_loss: 0.0928
Epoch 22/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1044 - val_loss: 0.0942
Epoch 23/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1031 - val_loss: 0.0942
Epoch 24/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1018 - val_loss: 0.0951
Epoch 25/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1005 - val_loss: 0.0940
Epoch 26/200
222/222 [==============================] - 2s 7ms/step - loss: 0.0994 - val_loss: 0.0968
Epoch 27/200
222/222 [==============================] - 2s 7ms/step - loss: 0.0986 - val_loss: 0.0942
Epoch 28/200
222/222 [==============================] - 2s 7ms/step - loss: 0.0974 - val_loss: 0.0928
Epoch 29/200
222/222 [==============================] - 2s 7ms/step - loss: 0.0963 - val_loss: 0.0930
Epoch 30/200
222/222 [==============================] - 2s 7ms/step - loss: 0.0952 - val_loss: 0.0921
Epoch 31/200
222/222 [==============================] - 2s 7ms/step - loss: 0.0942 - val_loss: 0.0938
Epoch 32/200
222/222 [==============================] - 2s 7ms/step - loss: 0.0933 - val_loss: 0.0939
Epoch 33/200
222/222 [==============================] - 2s 7ms/step - loss: 0.0924 - val_loss: 0.0950
Epoch 34/200
222/222 [==============================] - 2s 7ms/step - loss: 0.0918 - val_loss: 0.0924
Epoch 35/200
222/222 [==============================] - 2s 7ms/step - loss: 0.0908 - val_loss: 0.0923
Epoch 36/200
222/222 [==============================] - 2s 7ms/step - loss: 0.0902 - val_loss: 0.0934
Epoch 37/200
222/222 [==============================] - 2s 7ms/step - loss: 0.0892 - val_loss: 0.0931
Epoch 38/200
222/222 [==============================] - 2s 7ms/step - loss: 0.0883 - val_loss: 0.0928
Epoch 39/200
222/222 [==============================] - 2s 7ms/step - loss: 0.0877 - val_loss: 0.0933
Epoch 40/200
222/222 [==============================] - 2s 7ms/step - loss: 0.0873 - val_loss: 0.0932
Val_yp Shape is 
(75660, 24)
Results === Test == Validation ===== 188
=============================
step 1    -  0.986    0.984
step 2    -  0.978    0.976
step 3    -  0.969    0.966
step 4    -  0.959    0.955
step 5    -  0.948    0.945
step 6    -  0.937    0.934
step 7    -  0.926    0.923
step 8    -  0.915    0.913
step 9    -  0.905    0.904
step 10   -  0.897    0.897
step 11   -  0.887    0.888
step 12   -  0.879    0.880
step 13   -  0.871    0.873
step 14   -  0.864    0.866
step 15   -  0.857    0.860
step 16   -  0.851    0.854
step 17   -  0.846    0.850
step 18   -  0.841    0.846
step 19   -  0.837    0.842
step 20   -  0.834    0.840
step 21   -  0.830    0.836
step 22   -  0.828    0.835
step 23   -  0.824    0.832
step 24   -  0.819    0.828
=============================
Summary   -  21.287    21.328
V_y Shape is 
(75660, 1)
=========== SHAPE INPUT-OUTPUT MATRIXES =====================
shape x_training : (227084, 1330)  y_training: :  (227084, 24)
shape x_test     :  (75659, 1330)  y_test      :  (75659, 24)
shape x_val      :  (75659, 1330)  y_val       :  (75659, 24)
=============================================================
(1330,)
Model: "MLP"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_3 (InputLayer)        [(None, 1330)]            0         
                                                                 
 dense_8 (Dense)             (None, 1024)              1362944   
                                                                 
 elu_2 (ELU)                 (None, 1024)              0         
                                                                 
 dropout_6 (Dropout)         (None, 1024)              0         
                                                                 
 dense_9 (Dense)             (None, 512)               524800    
                                                                 
 dropout_7 (Dropout)         (None, 512)               0         
                                                                 
 dense_10 (Dense)            (None, 512)               262656    
                                                                 
 dropout_8 (Dropout)         (None, 512)               0         
                                                                 
 dense_11 (Dense)            (None, 24)                12312     
                                                                 
=================================================================
Total params: 2,162,712
Trainable params: 2,162,712
Non-trainable params: 0
_________________________________________________________________
Epoch 1/200
222/222 [==============================] - 2s 8ms/step - loss: 0.5642 - val_loss: 0.1222
Epoch 2/200
222/222 [==============================] - 1s 7ms/step - loss: 0.2770 - val_loss: 0.1115
Epoch 3/200
222/222 [==============================] - 1s 7ms/step - loss: 0.2246 - val_loss: 0.1065
Epoch 4/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1981 - val_loss: 0.1035
Epoch 5/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1810 - val_loss: 0.1044
Epoch 6/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1687 - val_loss: 0.1011
Epoch 7/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1585 - val_loss: 0.0992
Epoch 8/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1505 - val_loss: 0.0994
Epoch 9/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1433 - val_loss: 0.1002
Epoch 10/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1383 - val_loss: 0.0959
Epoch 11/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1334 - val_loss: 0.0942
Epoch 12/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1288 - val_loss: 0.0944
Epoch 13/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1250 - val_loss: 0.0924
Epoch 14/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1213 - val_loss: 0.0944
Epoch 15/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1185 - val_loss: 0.0953
Epoch 16/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1159 - val_loss: 0.0926
Epoch 17/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1134 - val_loss: 0.0953
Epoch 18/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1112 - val_loss: 0.0928
Epoch 19/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1091 - val_loss: 0.0944
Epoch 20/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1072 - val_loss: 0.0937
Epoch 21/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1056 - val_loss: 0.0932
Epoch 22/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1042 - val_loss: 0.0924
Epoch 23/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1027 - val_loss: 0.0954
Epoch 24/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1016 - val_loss: 0.0949
Epoch 25/200
222/222 [==============================] - 1s 6ms/step - loss: 0.1002 - val_loss: 0.0926
Epoch 26/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0992 - val_loss: 0.0933
Epoch 27/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0981 - val_loss: 0.0937
Epoch 28/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0973 - val_loss: 0.0935
Epoch 29/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0958 - val_loss: 0.0957
Epoch 30/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0949 - val_loss: 0.0923
Epoch 31/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0940 - val_loss: 0.0943
Epoch 32/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0929 - val_loss: 0.0943
Epoch 33/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0922 - val_loss: 0.0936
Epoch 34/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0913 - val_loss: 0.0935
Epoch 35/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0903 - val_loss: 0.0942
Epoch 36/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0895 - val_loss: 0.0939
Epoch 37/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0889 - val_loss: 0.0955
Epoch 38/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0880 - val_loss: 0.0946
Epoch 39/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0870 - val_loss: 0.0928
Epoch 40/200
222/222 [==============================] - 1s 6ms/step - loss: 0.0864 - val_loss: 0.0940
Val_yp Shape is 
(75659, 24)
Results === Test == Validation ===== 190
=============================
step 1    -  0.986    0.985
step 2    -  0.979    0.977
step 3    -  0.969    0.967
step 4    -  0.958    0.956
step 5    -  0.947    0.945
step 6    -  0.936    0.935
step 7    -  0.925    0.924
step 8    -  0.915    0.915
step 9    -  0.905    0.905
step 10   -  0.895    0.896
step 11   -  0.886    0.887
step 12   -  0.877    0.878
step 13   -  0.869    0.871
step 14   -  0.860    0.863
step 15   -  0.853    0.857
step 16   -  0.847    0.851
step 17   -  0.841    0.846
step 18   -  0.836    0.841
step 19   -  0.832    0.837
step 20   -  0.826    0.832
step 21   -  0.822    0.829
step 22   -  0.816    0.824
step 23   -  0.814    0.822
step 24   -  0.811    0.820
=============================
Summary   -  21.204    21.261
V_y Shape is 
(75659, 1)
=========== SHAPE INPUT-OUTPUT MATRIXES =====================
shape x_training : (227082, 1344)  y_training: :  (227082, 24)
shape x_test     :  (75658, 1344)  y_test      :  (75658, 24)
shape x_val      :  (75658, 1344)  y_val       :  (75658, 24)
=============================================================
(1344,)
Model: "MLP"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_4 (InputLayer)        [(None, 1344)]            0         
                                                                 
 dense_12 (Dense)            (None, 1024)              1377280   
                                                                 
 elu_3 (ELU)                 (None, 1024)              0         
                                                                 
 dropout_9 (Dropout)         (None, 1024)              0         
                                                                 
 dense_13 (Dense)            (None, 512)               524800    
                                                                 
 dropout_10 (Dropout)        (None, 512)               0         
                                                                 
 dense_14 (Dense)            (None, 512)               262656    
                                                                 
 dropout_11 (Dropout)        (None, 512)               0         
                                                                 
 dense_15 (Dense)            (None, 24)                12312     
                                                                 
=================================================================
Total params: 2,177,048
Trainable params: 2,177,048
Non-trainable params: 0

=========== SHAPE INPUT-OUTPUT MATRIXES =====================
shape x_training : (227082, 1344)  y_training: :  (227082, 24)
shape x_test     :  (75658, 1344)  y_test      :  (75658, 24)
shape x_val      :  (75658, 1344)  y_val       :  (75658, 24)
=============================================================
(1344,)
Model: "MLP"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_1 (InputLayer)        [(None, 1344)]            0         
                                                                 
 dense (Dense)               (None, 1024)              1377280   
                                                                 
 elu (ELU)                   (None, 1024)              0         
                                                                 
 dropout (Dropout)           (None, 1024)              0         
                                                                 
 dense_1 (Dense)             (None, 512)               524800    
                                                                 
 dropout_1 (Dropout)         (None, 512)               0         
                                                                 
 dense_2 (Dense)             (None, 512)               262656    
                                                                 
 dropout_2 (Dropout)         (None, 512)               0         
                                                                 
 dense_3 (Dense)             (None, 24)                12312     
                                                                 
=================================================================
Total params: 2,177,048
Trainable params: 2,177,048
Non-trainable params: 0
_________________________________________________________________
Epoch 1/200
222/222 [==============================] - 3s 8ms/step - loss: 0.5655 - val_loss: 0.1227
Epoch 2/200
222/222 [==============================] - 2s 7ms/step - loss: 0.2755 - val_loss: 0.1101
Epoch 3/200
222/222 [==============================] - 2s 7ms/step - loss: 0.2232 - val_loss: 0.1063
Epoch 4/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1971 - val_loss: 0.1035
Epoch 5/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1802 - val_loss: 0.1013
Epoch 6/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1678 - val_loss: 0.1000
Epoch 7/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1579 - val_loss: 0.0981
Epoch 8/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1498 - val_loss: 0.0970
Epoch 9/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1433 - val_loss: 0.0960
Epoch 10/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1373 - val_loss: 0.0978
Epoch 11/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1326 - val_loss: 0.0952
Epoch 12/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1280 - val_loss: 0.0960
Epoch 13/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1244 - val_loss: 0.0946
Epoch 14/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1209 - val_loss: 0.0943
Epoch 15/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1179 - val_loss: 0.0933
Epoch 16/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1151 - val_loss: 0.0935
Epoch 17/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1129 - val_loss: 0.0956
Epoch 18/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1106 - val_loss: 0.0933
Epoch 19/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1088 - val_loss: 0.0933
Epoch 20/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1069 - val_loss: 0.0950
Epoch 21/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1051 - val_loss: 0.0936
Epoch 22/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1037 - val_loss: 0.0953
Epoch 23/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1024 - val_loss: 0.0954
Epoch 24/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1009 - val_loss: 0.0950
Epoch 25/200
222/222 [==============================] - 2s 7ms/step - loss: 0.0998 - val_loss: 0.0945
Val_yp Shape is 
(75658, 24)
Results === Test == Validation ===== 192
=============================
step 1    -  0.986    0.984
step 2    -  0.978    0.976
step 3    -  0.969    0.967
step 4    -  0.959    0.957
step 5    -  0.949    0.946
step 6    -  0.939    0.936
step 7    -  0.928    0.925
step 8    -  0.917    0.915
step 9    -  0.907    0.906
step 10   -  0.897    0.896
step 11   -  0.888    0.887
step 12   -  0.879    0.878
step 13   -  0.872    0.871
step 14   -  0.866    0.866
step 15   -  0.859    0.859
step 16   -  0.853    0.854
step 17   -  0.848    0.848
step 18   -  0.843    0.844
step 19   -  0.839    0.840
step 20   -  0.835    0.837
step 21   -  0.832    0.834
step 22   -  0.827    0.831
step 23   -  0.823    0.827
step 24   -  0.818    0.824
=============================
Summary   -  21.311    21.306
V_y Shape is 
(75658, 1)
=========== SHAPE INPUT-OUTPUT MATRIXES =====================
shape x_training : (227080, 1358)  y_training: :  (227080, 24)
shape x_test     :  (75657, 1358)  y_test      :  (75657, 24)
shape x_val      :  (75657, 1358)  y_val       :  (75657, 24)
=============================================================
(1358,)
Model: "MLP"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_2 (InputLayer)        [(None, 1358)]            0         
                                                                 
 dense_4 (Dense)             (None, 1024)              1391616   
                                                                 
 elu_1 (ELU)                 (None, 1024)              0         
                                                                 
 dropout_3 (Dropout)         (None, 1024)              0         
                                                                 
 dense_5 (Dense)             (None, 512)               524800    
                                                                 
 dropout_4 (Dropout)         (None, 512)               0         
                                                                 
 dense_6 (Dense)             (None, 512)               262656    
                                                                 
 dropout_5 (Dropout)         (None, 512)               0         
                                                                 
 dense_7 (Dense)             (None, 24)                12312     
                                                                 
=================================================================
Total params: 2,191,384
Trainable params: 2,191,384
Non-trainable params: 0
_________________________________________________________________
Epoch 1/200
222/222 [==============================] - 3s 10ms/step - loss: 0.5794 - val_loss: 0.1192
Epoch 2/200
222/222 [==============================] - 2s 8ms/step - loss: 0.2800 - val_loss: 0.1091
Epoch 3/200
222/222 [==============================] - 2s 8ms/step - loss: 0.2260 - val_loss: 0.1058
Epoch 4/200
222/222 [==============================] - 2s 8ms/step - loss: 0.1988 - val_loss: 0.1009
Epoch 5/200
222/222 [==============================] - 2s 8ms/step - loss: 0.1820 - val_loss: 0.1016
Epoch 6/200
222/222 [==============================] - 2s 8ms/step - loss: 0.1693 - val_loss: 0.0976
Epoch 7/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1589 - val_loss: 0.0964
Epoch 8/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1511 - val_loss: 0.1000
Epoch 9/200
222/222 [==============================] - 2s 8ms/step - loss: 0.1440 - val_loss: 0.0946
Epoch 10/200
222/222 [==============================] - 2s 8ms/step - loss: 0.1386 - val_loss: 0.0949
Epoch 11/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1332 - val_loss: 0.0932
Epoch 12/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1288 - val_loss: 0.0951
Epoch 13/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1249 - val_loss: 0.0946
Epoch 14/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1215 - val_loss: 0.0944
Epoch 15/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1185 - val_loss: 0.0957
Epoch 16/200
222/222 [==============================] - 2s 8ms/step - loss: 0.1157 - val_loss: 0.0931
Epoch 17/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1130 - val_loss: 0.0947
Epoch 18/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1110 - val_loss: 0.0923
Epoch 19/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1090 - val_loss: 0.0925
Epoch 20/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1072 - val_loss: 0.0973
Epoch 21/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1053 - val_loss: 0.0927
Epoch 22/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1036 - val_loss: 0.0922
Epoch 23/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1028 - val_loss: 0.0955
Epoch 24/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1009 - val_loss: 0.0945
Epoch 25/200
222/222 [==============================] - 2s 7ms/step - loss: 0.0998 - val_loss: 0.0940
Epoch 26/200
222/222 [==============================] - 2s 7ms/step - loss: 0.0986 - val_loss: 0.0933
Epoch 27/200
222/222 [==============================] - 2s 7ms/step - loss: 0.0978 - val_loss: 0.0946
Epoch 28/200
222/222 [==============================] - 2s 7ms/step - loss: 0.0966 - val_loss: 0.0932
Epoch 29/200
222/222 [==============================] - 2s 7ms/step - loss: 0.0953 - val_loss: 0.0943
Epoch 30/200
222/222 [==============================] - 2s 7ms/step - loss: 0.0943 - val_loss: 0.0956
Epoch 31/200
222/222 [==============================] - 2s 7ms/step - loss: 0.0934 - val_loss: 0.0956
Epoch 32/200
222/222 [==============================] - 2s 7ms/step - loss: 0.0927 - val_loss: 0.0932
Val_yp Shape is 
(75657, 24)
Results === Test == Validation ===== 194
=============================
step 1    -  0.987    0.985
step 2    -  0.979    0.977
step 3    -  0.970    0.968
step 4    -  0.960    0.957
step 5    -  0.950    0.947
step 6    -  0.939    0.936
step 7    -  0.929    0.926
step 8    -  0.919    0.916
step 9    -  0.909    0.906
step 10   -  0.899    0.896
step 11   -  0.889    0.887
step 12   -  0.881    0.879
step 13   -  0.872    0.872
step 14   -  0.865    0.865
step 15   -  0.857    0.858
step 16   -  0.851    0.852
step 17   -  0.844    0.846
step 18   -  0.838    0.840
step 19   -  0.833    0.836
step 20   -  0.826    0.830
step 21   -  0.820    0.824
step 22   -  0.817    0.822
step 23   -  0.813    0.820
step 24   -  0.807    0.814
=============================
Summary   -  21.253    21.261
V_y Shape is 
(75657, 1)
=========== SHAPE INPUT-OUTPUT MATRIXES =====================
shape x_training : (227078, 1372)  y_training: :  (227078, 24)
shape x_test     :  (75656, 1372)  y_test      :  (75656, 24)
shape x_val      :  (75656, 1372)  y_val       :  (75656, 24)
=============================================================
(1372,)
Model: "MLP"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_3 (InputLayer)        [(None, 1372)]            0         
                                                                 
 dense_8 (Dense)             (None, 1024)              1405952   
                                                                 
 elu_2 (ELU)                 (None, 1024)              0         
                                                                 
 dropout_6 (Dropout)         (None, 1024)              0         
                                                                 
 dense_9 (Dense)             (None, 512)               524800    
                                                                 
 dropout_7 (Dropout)         (None, 512)               0         
                                                                 
 dense_10 (Dense)            (None, 512)               262656    
                                                                 
 dropout_8 (Dropout)         (None, 512)               0         
                                                                 
 dense_11 (Dense)            (None, 24)                12312     
                                                                 
=================================================================
Total params: 2,205,720
Trainable params: 2,205,720
Non-trainable params: 0
_________________________________________________________________
Epoch 1/200
222/222 [==============================] - 3s 10ms/step - loss: 0.5687 - val_loss: 0.1238
Epoch 2/200
222/222 [==============================] - 2s 8ms/step - loss: 0.2790 - val_loss: 0.1135
Epoch 3/200
222/222 [==============================] - 2s 8ms/step - loss: 0.2257 - val_loss: 0.1064
Epoch 4/200
222/222 [==============================] - 2s 8ms/step - loss: 0.1993 - val_loss: 0.1048
Epoch 5/200
222/222 [==============================] - 2s 8ms/step - loss: 0.1820 - val_loss: 0.1023
Epoch 6/200
222/222 [==============================] - 2s 8ms/step - loss: 0.1692 - val_loss: 0.0984
Epoch 7/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1593 - val_loss: 0.0980
Epoch 8/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1509 - val_loss: 0.1000
Epoch 9/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1443 - val_loss: 0.0972
Epoch 10/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1385 - val_loss: 0.0974
Epoch 11/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1333 - val_loss: 0.0954
Epoch 12/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1289 - val_loss: 0.0962
Epoch 13/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1247 - val_loss: 0.0961
Epoch 14/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1213 - val_loss: 0.0944
Epoch 15/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1182 - val_loss: 0.0956
Epoch 16/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1156 - val_loss: 0.0974
Epoch 17/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1130 - val_loss: 0.0926
Epoch 18/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1106 - val_loss: 0.0940
Epoch 19/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1084 - val_loss: 0.0929
Epoch 20/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1067 - val_loss: 0.0951
Epoch 21/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1051 - val_loss: 0.0959
Epoch 22/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1034 - val_loss: 0.0937
Epoch 23/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1021 - val_loss: 0.0927
Epoch 24/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1005 - val_loss: 0.0947
Epoch 25/200
222/222 [==============================] - 2s 7ms/step - loss: 0.0994 - val_loss: 0.0942
Epoch 26/200
222/222 [==============================] - 2s 7ms/step - loss: 0.0984 - val_loss: 0.0926
Epoch 27/200
222/222 [==============================] - 2s 7ms/step - loss: 0.0970 - val_loss: 0.0949
Epoch 28/200
222/222 [==============================] - 2s 7ms/step - loss: 0.0962 - val_loss: 0.0956
Epoch 29/200
222/222 [==============================] - 2s 7ms/step - loss: 0.0953 - val_loss: 0.0927
Epoch 30/200
222/222 [==============================] - 2s 7ms/step - loss: 0.0940 - val_loss: 0.0961
Epoch 31/200
222/222 [==============================] - 2s 7ms/step - loss: 0.0933 - val_loss: 0.0942
Epoch 32/200
222/222 [==============================] - 2s 7ms/step - loss: 0.0924 - val_loss: 0.0943
Epoch 33/200
222/222 [==============================] - 2s 7ms/step - loss: 0.0910 - val_loss: 0.0941
Epoch 34/200
222/222 [==============================] - 2s 7ms/step - loss: 0.0902 - val_loss: 0.0939
Epoch 35/200
222/222 [==============================] - 2s 7ms/step - loss: 0.0895 - val_loss: 0.0948
Epoch 36/200
222/222 [==============================] - 2s 7ms/step - loss: 0.0887 - val_loss: 0.0960
Val_yp Shape is 
(75656, 24)
Results === Test == Validation ===== 196
=============================
step 1    -  0.987    0.986
step 2    -  0.979    0.978
step 3    -  0.970    0.968
step 4    -  0.961    0.959
step 5    -  0.951    0.949
step 6    -  0.941    0.939
step 7    -  0.931    0.930
step 8    -  0.921    0.921
step 9    -  0.912    0.912
step 10   -  0.904    0.904
step 11   -  0.895    0.896
step 12   -  0.888    0.890
step 13   -  0.881    0.883
step 14   -  0.875    0.878
step 15   -  0.869    0.872
step 16   -  0.864    0.867
step 17   -  0.858    0.861
step 18   -  0.852    0.856
step 19   -  0.848    0.852
step 20   -  0.844    0.849
step 21   -  0.840    0.846
step 22   -  0.836    0.844
step 23   -  0.833    0.841
step 24   -  0.829    0.839
=============================
Summary   -  21.469    21.519
V_y Shape is 
(75656, 1)

shape x_training : (227076, 1386)  y_training: :  (227076, 24)
shape x_test     :  (75655, 1386)  y_test      :  (75655, 24)
shape x_val      :  (75655, 1386)  y_val       :  (75655, 24)
=============================================================
(1386,)
Model: "MLP"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_1 (InputLayer)        [(None, 1386)]            0         
                                                                 
 dense (Dense)               (None, 1024)              1420288   
                                                                 
 elu (ELU)                   (None, 1024)              0         
                                                                 
 dropout (Dropout)           (None, 1024)              0         
                                                                 
 dense_1 (Dense)             (None, 512)               524800    
                                                                 
 dropout_1 (Dropout)         (None, 512)               0         
                                                                 
 dense_2 (Dense)             (None, 512)               262656    
                                                                 
 dropout_2 (Dropout)         (None, 512)               0         
                                                                 
 dense_3 (Dense)             (None, 24)                12312     
                                                                 
=================================================================
Total params: 2,220,056
Trainable params: 2,220,056
Non-trainable params: 0
_________________________________________________________________
Epoch 1/200
222/222 [==============================] - 4s 9ms/step - loss: 0.5677 - val_loss: 0.1216
Epoch 2/200
222/222 [==============================] - 2s 8ms/step - loss: 0.2791 - val_loss: 0.1105
Epoch 3/200
222/222 [==============================] - 2s 7ms/step - loss: 0.2254 - val_loss: 0.1055
Epoch 4/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1989 - val_loss: 0.1032
Epoch 5/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1817 - val_loss: 0.1011
Epoch 6/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1685 - val_loss: 0.1000
Epoch 7/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1585 - val_loss: 0.0978
Epoch 8/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1505 - val_loss: 0.0972
Epoch 9/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1437 - val_loss: 0.0948
Epoch 10/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1374 - val_loss: 0.0958
Epoch 11/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1327 - val_loss: 0.0949
Epoch 12/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1281 - val_loss: 0.0932
Epoch 13/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1240 - val_loss: 0.0939
Epoch 14/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1206 - val_loss: 0.0930
Epoch 15/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1175 - val_loss: 0.0933
Epoch 16/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1146 - val_loss: 0.0951
Epoch 17/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1122 - val_loss: 0.0930
Epoch 18/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1099 - val_loss: 0.0938
Epoch 19/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1081 - val_loss: 0.0922
Epoch 20/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1061 - val_loss: 0.0955
Epoch 21/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1041 - val_loss: 0.0940
Epoch 22/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1028 - val_loss: 0.0983
Epoch 23/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1013 - val_loss: 0.0922
Epoch 24/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1001 - val_loss: 0.0936
Epoch 25/200
222/222 [==============================] - 2s 7ms/step - loss: 0.0988 - val_loss: 0.0935
Epoch 26/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0973 - val_loss: 0.0948
Epoch 27/200
222/222 [==============================] - 2s 7ms/step - loss: 0.0964 - val_loss: 0.0934
Epoch 28/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0954 - val_loss: 0.0941
Epoch 29/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0942 - val_loss: 0.0933
Epoch 30/200
222/222 [==============================] - 2s 7ms/step - loss: 0.0933 - val_loss: 0.0929
Epoch 31/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0923 - val_loss: 0.0926
Epoch 32/200
222/222 [==============================] - 2s 7ms/step - loss: 0.0913 - val_loss: 0.0962
Epoch 33/200
222/222 [==============================] - 1s 7ms/step - loss: 0.0904 - val_loss: 0.0937
Val_yp Shape is 
(75655, 24)
Results === Test == Validation ===== 198
=============================
step 1    -  0.986    0.985
step 2    -  0.978    0.976
step 3    -  0.968    0.966
step 4    -  0.957    0.954
step 5    -  0.945    0.942
step 6    -  0.934    0.931
step 7    -  0.924    0.921
step 8    -  0.913    0.911
step 9    -  0.902    0.900
step 10   -  0.892    0.891
step 11   -  0.883    0.883
step 12   -  0.875    0.875
step 13   -  0.868    0.869
step 14   -  0.861    0.863
step 15   -  0.856    0.858
step 16   -  0.851    0.854
step 17   -  0.847    0.850
step 18   -  0.843    0.847
step 19   -  0.839    0.844
step 20   -  0.835    0.840
step 21   -  0.832    0.838
step 22   -  0.828    0.835
step 23   -  0.825    0.833
step 24   -  0.820    0.830
=============================
Summary   -  21.262    21.296
V_y Shape is 
(75655, 1)

=========== SHAPE INPUT-OUTPUT MATRIXES =====================
shape x_training : (227074, 1400)  y_training: :  (227074, 24)
shape x_test     :  (75654, 1400)  y_test      :  (75654, 24)
shape x_val      :  (75654, 1400)  y_val       :  (75654, 24)
=============================================================
(1400,)
Model: "MLP"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_1 (InputLayer)        [(None, 1400)]            0         
                                                                 
 dense (Dense)               (None, 1024)              1434624   
                                                                 
 elu (ELU)                   (None, 1024)              0         
                                                                 
 dropout (Dropout)           (None, 1024)              0         
                                                                 
 dense_1 (Dense)             (None, 512)               524800    
                                                                 
 dropout_1 (Dropout)         (None, 512)               0         
                                                                 
 dense_2 (Dense)             (None, 512)               262656    
                                                                 
 dropout_2 (Dropout)         (None, 512)               0         
                                                                 
 dense_3 (Dense)             (None, 24)                12312     
                                                                 
=================================================================
Total params: 2,234,392
Trainable params: 2,234,392
Non-trainable params: 0
_________________________________________________________________
Epoch 1/200
222/222 [==============================] - 4s 9ms/step - loss: 0.5856 - val_loss: 0.1221
Epoch 2/200
222/222 [==============================] - 2s 8ms/step - loss: 0.2820 - val_loss: 0.1100
Epoch 3/200
222/222 [==============================] - 2s 7ms/step - loss: 0.2272 - val_loss: 0.1062
Epoch 4/200
222/222 [==============================] - 2s 7ms/step - loss: 0.2005 - val_loss: 0.1057
Epoch 5/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1828 - val_loss: 0.1021
Epoch 6/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1702 - val_loss: 0.1028
Epoch 7/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1596 - val_loss: 0.0986
Epoch 8/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1516 - val_loss: 0.0958
Epoch 9/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1444 - val_loss: 0.0984
Epoch 10/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1386 - val_loss: 0.0952
Epoch 11/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1333 - val_loss: 0.0984
Epoch 12/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1291 - val_loss: 0.1001
Epoch 13/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1250 - val_loss: 0.0985
Epoch 14/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1212 - val_loss: 0.0960
Epoch 15/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1180 - val_loss: 0.0970
Epoch 16/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1151 - val_loss: 0.0960
Epoch 17/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1124 - val_loss: 0.0960
Epoch 18/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1105 - val_loss: 0.0936
Epoch 19/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1081 - val_loss: 0.0947
Epoch 20/200
222/222 [==============================] - 1s 7ms/step - loss: 0.1062 - val_loss: 0.0936
Epoch 21/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1045 - val_loss: 0.0928
Epoch 22/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1030 - val_loss: 0.0953
Epoch 23/200
222/222 [==============================] - 2s 7ms/step - loss: 0.1013 - val_loss: 0.0927
Epoch 24/200
222/222 [==============================] - 2s 7ms/step - loss: 0.0999 - val_loss: 0.0946
Epoch 25/200
222/222 [==============================] - 2s 7ms/step - loss: 0.0988 - val_loss: 0.0946
Epoch 26/200
222/222 [==============================] - 2s 7ms/step - loss: 0.0976 - val_loss: 0.0952
Epoch 27/200
222/222 [==============================] - 2s 7ms/step - loss: 0.0963 - val_loss: 0.0968
Epoch 28/200
222/222 [==============================] - 2s 7ms/step - loss: 0.0955 - val_loss: 0.0950
Epoch 29/200
222/222 [==============================] - 2s 7ms/step - loss: 0.0941 - val_loss: 0.0941
Epoch 30/200
222/222 [==============================] - 2s 7ms/step - loss: 0.0931 - val_loss: 0.0942
Epoch 31/200
222/222 [==============================] - 2s 7ms/step - loss: 0.0922 - val_loss: 0.0938
Epoch 32/200
222/222 [==============================] - 2s 7ms/step - loss: 0.0912 - val_loss: 0.0943
Epoch 33/200
222/222 [==============================] - 2s 7ms/step - loss: 0.0904 - val_loss: 0.0960
Val_yp Shape is 
(75654, 24)
Results === Test == Validation ===== 200
=============================
step 1    -  0.986    0.984
step 2    -  0.978    0.975
step 3    -  0.968    0.964
step 4    -  0.956    0.953
step 5    -  0.945    0.941
step 6    -  0.934    0.930
step 7    -  0.923    0.919
step 8    -  0.913    0.910
step 9    -  0.902    0.900
step 10   -  0.894    0.891
step 11   -  0.884    0.883
step 12   -  0.876    0.875
step 13   -  0.868    0.867
step 14   -  0.862    0.861
step 15   -  0.855    0.854
step 16   -  0.850    0.850
step 17   -  0.845    0.846
step 18   -  0.840    0.841
step 19   -  0.835    0.836
step 20   -  0.830    0.832
step 21   -  0.826    0.828
step 22   -  0.820    0.824
step 23   -  0.817    0.822
step 24   -  0.812    0.818
=============================
Summary   -  21.218    21.203
V_y Shape is 
(75654, 1)